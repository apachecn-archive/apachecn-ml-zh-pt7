<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<title>Chapter 7: Working with Model Explainability </title>

</head>
<body>
<div><div><h1 class="chapter-number" id="_idParaDest-110"><a id="_idTextAnchor143"/> 7</h1>
<h1 id="_idParaDest-111"><a id="_idTextAnchor144"/>使用模型可解释性</h1>
<p>模型选择和性能的合理性与模型训练同样重要。你可以让<em class="italic"> N </em>个使用不同算法训练的模型，所有这些模型都能够对现实世界的问题做出足够好的预测。那么，您如何选择其中一个用于您的生产服务，以及您如何向您的利益相关者证明您选择的模型比其他模型更好，即使所有其他模型也能够在某种程度上做出准确的预测？一个答案是性能指标，但正如我们在上一章中看到的，有许多性能指标，它们都衡量不同类型的性能。选择正确的性能指标可以归结为您的ML问题的背景。我们还可以使用什么来帮助我们选择正确的模型，并进一步帮助我们证明这一选择的合理性？</p>
<p>这个问题的答案是可视化图表。人类是视觉动物，因此，一张图片胜过千言万语。一个好的图表可以比任何度量数字更好地解释一个模型。图形的多样性对于解释模型的行为以及它如何适合作为我们的ML问题的解决方案是非常有用的。</p>
<p>H2O的可解释性界面是一个独特的功能，它涵盖了H2O为一个模型或模型列表自动计算的各种可解释性功能和视觉效果，包括H2O AutoML对象。</p>
<p>在这一章中，我们将探索H2O可解释性接口以及它如何与H2O AutoML对象一起工作。我们还将实现一个实际的例子来理解如何在Python和r中使用explainability接口。最后，我们将浏览并理解我们作为输出得到的各种explainability特性。</p>
<p>在本章中，我们将讨论以下主要话题:</p>
<ul>
<li><a id="_idTextAnchor145"/> <a id="_idTextAnchor146"/>使用模型可解释性接口</li>
<li>探索各种可解释的特性</li>
</ul>
<p>在本章结束时，通过查看模型可解释性接口描述的各种性能指标，您应该对如何解释模型性能有了一个很好的想法。</p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor147"/>技术要求</h1>
<p>对于本章，您将需要以下内容:</p>
<ul>
<li>您首选的web浏览器的最新版本。</li>
<li>您选择的<strong class="bold">集成开发环境</strong> ( <strong class="bold"> IDE </strong>)或终端。</li>
<li>本章进行的所有实验都是在终端上进行的。您可以自由地使用相同的设置进行操作，也可以使用您选择的任何IDE来执行相同的实验。</li>
</ul>
<p>本章的所有代码示例都可以在GitHub上的<a href="https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207">https://GitHub . com/packt publishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter % 207</a>找到。</p>
<p>因此，让我们从理解模型可解释性接口如何工作开始。</p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor148"/>使用模型可解释性接口</h1>
<p><strong class="bold">模型可解释性接口</strong>是一个简单的函数，它包含了关于模型及其工作的各种图表和<a id="_idIndexMarker748"/>信息。在H2O中，模型可解释性有两个主要功能:</p>
<ul>
<li><code>h2o.explain()</code>函数，其<a id="_idIndexMarker749"/>用于解释模型在整个测试数据集上的行为<a id="_idIndexMarker750"/>。这也叫做<strong class="bold">全局解释</strong>。</li>
<li><code>h2o.explain_row()</code>函数，<a id="_idIndexMarker751"/>用于解释模型在测试数据集中的单个行上的行为。这也是<a id="_idIndexMarker752"/>称为<strong class="bold">的地方解释</strong>。</li>
</ul>
<p>这两个函数既可以作用于单个H2O模型对象，也可以作用于一系列H2O模型对象或H2O AutoML对象。这些函数生成由各种<a id="_idIndexMarker753"/>图形图组成的结果列表，如<a id="_idIndexMarker754"/>一个<strong class="bold">变量重要性图</strong>、<strong class="bold">部分依赖图</strong>和一个<strong class="bold">排行榜</strong>(如果<a id="_idIndexMarker755"/>用于多个模型)。</p>
<p>对于图形<a id="_idIndexMarker756"/>和其他可视结果，<code>explain</code>对象依赖于可视化引擎来呈现图形:</p>
<ul>
<li>对于R接口，H2O使用<code>ggplot2</code>包进行渲染。</li>
<li>对于Python接口，H2O使用<code>matplotlib</code>包进行渲染。</li>
</ul>
<p>考虑到这一点，我们需要确保每当我们使用explainability接口来获得可视化图形时，我们都在支持图形渲染的环境中运行它。这个界面在终端和其他非图形命令行界面中不会有太大用处。本章中的例子已经在<strong class="bold"> Jupyter笔记本</strong>上运行过，但是任何支持绘图渲染的<a id="_idIndexMarker757"/>环境都应该工作良好。</p>
<p>可解释性函数具有以下参数:</p>
<ul>
<li><code>newdata</code> / <code>frame</code>:该<a id="_idIndexMarker758"/>参数用于指定计算<a id="_idIndexMarker759"/>某些可解释特性(如<code>newdata</code>)所需的H2O测试数据帧，而在Python可解释接口中相同的是<code>frame</code>。</li>
<li><code>columns</code>:该<a id="_idIndexMarker761"/>参数用于<a id="_idIndexMarker762"/>指定基于列的解释<a id="_idIndexMarker763"/>中要考虑的列，如<strong class="bold">个别条件期望图</strong>或<strong class="bold">部分依赖图</strong>。</li>
<li><code>top_n_features</code>:该<a id="_idIndexMarker764"/>参数用于为基于列的解释指定基于特征重要性等级考虑的列数。默认值为<code>5</code>。</li>
</ul>
<p>可解释性函数将考虑<code>columns</code>参数或<code>top_n_features</code>参数。优先选择<code>columns</code>参数，所以如果两个参数都传递了值，那么<code>top_n_features</code>将被忽略。</p>
<ul>
<li><code>include_explanations</code>:该<a id="_idIndexMarker765"/>参数用于指定您想要从可解释函数的输出中得到的解释。</li>
<li><code>exclude_explanations</code>:该<a id="_idIndexMarker766"/>参数用于指定你不希望从可解释函数的输出中得到的解释。<code>include_explanations</code>和<code>exclude_explanations</code>是互斥的参数。这两个参数的可用值如下:<ul><li><code>leaderboard</code>:该值仅对模型列表或AutoML对象有效。</li><li><code>residual_analysis</code>:该值仅对回归模型有效。</li><li><code>confusion_matrix</code>:该值仅对分类模型有效。</li><li><code>varimp</code>:该值代表可变重要性，仅对基础模型有效，对堆叠集合模型无效。</li><li><code>varimp_heatmap</code>:该值代表重要性可变的热图。</li><li><code>model_correlation_heatmap</code>:该值代表模型关联的热图。</li><li><code>shap_summary</code>:该值代表Shapley加法解释。</li><li><code>pdp</code>:该值代表部分依赖图。</li><li><code>ice</code>:该值<a id="_idIndexMarker767"/>代表单个条件期望图。</li></ul></li>
<li><code>plot_overrides</code>:该<a id="_idIndexMarker768"/>参数用于覆盖各个解释图的值。如果您希望在一个图中考虑前10个特性，而在另一个图中考虑特定的列，此参数很有用:<pre>list(pdp = list(top_n_features = 8))</pre></li>
<li><code>object</code>:该<a id="_idIndexMarker769"/>参数用于指定H2O模型或H2O AutoML对象，我们将很快介绍。这个参数是特定于R explainability接口的。</li>
</ul>
<p>现在我们已经知道了explainability接口是如何工作的，以及它的各种参数是什么，让我们通过一个实现例子来更好地理解它。</p>
<p>我们将使用<strong class="bold"> Fisher的Iris flower数据集</strong>，它是我们在<a href="B17298_01.xhtml#_idTextAnchor017"> <em class="italic">第1章</em> </a>、<em class="italic">了解H2O AutoML基础知识</em>中使用的，来训练使用AutoML的模型。然后，我们将使用AutoML对象上的explaibility接口来显示它必须提供的所有explaibility特性。</p>
<p>所以，让我们从用Python实现它开始。</p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor149"/>用Python实现模型可解释性接口</h2>
<p>要在Python中实现模型可解释性<a id="_idIndexMarker772"/>功能，请遵循以下步骤:</p>
<ol>
<li>导入<code>h2o</code>库并运行<a id="_idTextAnchor150"/>本地H2O服务器:<pre>library(h2o) h2o.init(max_mem_size = "12g")</pre></li>
</ol>
<p>explainability接口在幕后执行繁重的计算，以计算绘制图表所需的数据。为了加快处理速度，建议使用尽可能多的内存来初始化H2O服务器。</p>
<ol>
<li value="2">使用<code>h2o.importFile(“Dataset/iris.data”)</code> : <pre>data = h2o.import_file("Dataset/iris.data")</pre>导入数据集</li>
<li>设置哪些列是特征，哪些列是标签:<pre>features = data.columns label = "C5"</pre></li>
<li>从特性中移除标签:<pre>features.remove(label)</pre></li>
<li>将数据帧分为训练和测试数据帧:<pre>train_dataframe, test_dataframe = data.split_frame([0.8])</pre></li>
<li>初始化H2O AutoML对象:<pre>aml = h2o.automl.H2OAutoML(max_models=10, seed = 1)</pre></li>
<li>触发<a id="_idIndexMarker773"/>H2O AutoML<a id="_idIndexMarker774"/>对象，使其开始自动训练模型:<pre>aml.train(x = features, y = label, training_frame = train_dataframe)</pre></li>
<li>一旦训练完成，我们就可以在现在训练过的<code>aml</code>对象上使用H2O可解释接口<code>h2o.explain()</code></li>
</ol>
<p><code>explain</code>函数需要一些时间来完成计算。一旦完成，您应该会看到一个列出所有可解释特性的输出。输出应该如下所示:</p>
<div><div><img alt="Figure 7.1 – Model explainability interface output  " height="386" src="img/B17298_07_001.jpg" width="1160"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.1–模型可解释性界面输出</p>
<ol>
<li value="9">您还可以使用<code>h2o.explain_row()</code>界面来显示数据集中单行的模型可解释性特征:<pre>aml.explain_row(test_dataframe, row_index=0)</pre></li>
</ol>
<p>这样的输出应该会给你一个在数据集的第一行做出预测的模型的排行榜。</p>
<ol>
<li value="10">为了从可解释性的角度获得关于模型的附加信息，您可以通过在leader模型上使用<code>explain_row()</code>函数来进一步扩展可解释性接口，如下:<pre>aml.leader.explain_row(test_dataframe, row_index=0)</pre></li>
</ol>
<p>此操作的<a id="_idIndexMarker775"/>输出应该<a id="_idIndexMarker776"/>根据该行上的预测，为您提供该模型所有适用的图形模型可解释性特性。</p>
<p>既然我们知道了如何在Python中使用model explainability接口，那么让我们看看如何在R语言中使用这个接口。</p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor151"/>在R中实现模型可解释性接口</h2>
<p>类似于我们如何在Python中实现了<a id="_idIndexMarker778"/>可解释性接口，H2O也规定了在R编程语言中使用可解释性接口。</p>
<p>要在R中实现模型可解释性功能，请遵循以下步骤:</p>
<ol>
<li value="1">导入<code>h2o</code>库并启动本地H2O服务器:<pre>library(h2o) h2o.init(max_mem_size = "12g")</pre></li>
<li>使用<code>h2o.importFile(“Dataset/iris.data”)</code> : <pre>data = h2o.import_file("Dataset/iris.data")</pre>导入数据集</li>
<li>将<code>C5</code>列设置为标签:<pre>label &lt;- "C5"</pre></li>
<li>将数据帧分为训练和测试数据帧，并将其分配给适当的变量:<pre>splits &lt;- h2o.splitFrame(data, ratios = 0.8, seed = 7) train_dataframe &lt;- splits[[1]] test_dataframe &lt;- splits[[2]]</pre></li>
<li>运行H2O汽车培训:<pre>aml &lt;- h2o.automl(y = label, training_frame = train_dataframe, max_models = 10)</pre></li>
<li>在现在训练的<code>aml</code>对象上使用<a id="_idIndexMarker779"/> H2O可解释性<a id="_idIndexMarker780"/>接口:<pre>explanability_object &lt;- h2o.explain(aml, test_dataframe)</pre></li>
</ol>
<p>一旦explaibility对象完成了它的计算，您应该会看到一个列出了所有explaibility特性的大输出。</p>
<ol>
<li value="7">就像Python一样，您也可以扩展模型可解释性接口函数，这样它就可以使用<code>h2o.explain_row()</code>函数在一行上运行，如下:<pre>h2o.explain_row(aml, test, row_index = 1)</pre></li>
</ol>
<p>这将为您提供在数据集的第一行做出预测的模型排行榜。</p>
<ol>
<li value="8">类似地，您可以通过使用leader模型上的<code>h2o.explain_row()</code>函数来扩展这个可解释性接口，以获得关于leader模型的更多高级信息:<pre>h2o.explain_row(aml@leader, test, row_index = 1)</pre></li>
</ol>
<p>在这些例子中，我们使用Iris flower数据集来解决多项式分类问题。类似地，我们可以在训练好的回归模型上使用解释能力接口。一些可解释性特征仅取决于训练模型<a id="_idIndexMarker782"/>是回归模型还是分类模型<a id="_idIndexMarker781"/>才可用。</p>
<p>现在我们知道了如何用Python和R实现模型可解释性接口，让我们更深入地看看接口的输出，并尝试理解H2O计算的各种可解释性特征。</p>
<h1 id="_idParaDest-116">探索各种可解释的特征</h1>
<p><a id="_idIndexMarker783"/>可解释性接口的输出是一个<code>H2OExplanation</code>对象。对象只不过是一个简单的字典，用可解释特性的名字作为键。您可以通过使用一个特性的键名作为可解释性对象上的<code>dict</code>键来检索单个可解释性特性。</p>
<p>如果你向下滚动H2O AutoML对象的explainability接口的输出，你会注意到有很多带有解释的标题。在这些标题下面，有一个关于可解释特性的简短描述。有些有图解，而其他的可能有表格。</p>
<p>各种可解释特性如下:</p>
<ul>
<li><strong class="bold">排行榜</strong>:该功能是一个排行榜<a id="_idIndexMarker785"/>，由所有<a id="_idIndexMarker786"/>训练过的模型及其基本指标组成，从表现最好到最差排列。仅当可解释性接口在H2O AutoML对象或H2O模型列表上运行时，才计算此功能。</li>
<li><strong class="bold">混淆矩阵</strong>:这个特性是一个性能度量，它生成一个矩阵来跟踪分类模型的正确和不正确的预测。仅<a id="_idIndexMarker787"/>可用于分类模型。对于多个模型，混淆矩阵<a id="_idIndexMarker788"/>只针对leader模型计算。</li>
<li><strong class="bold">残差分析</strong>:该特性在可解释性接口中使用的测试数据集上绘制预测值与残差<a id="_idIndexMarker789"/>的关系。它只根据排行榜上的模型排名来分析领导者模型。它仅适用于回归模型。对于多个模型，残差分析<a id="_idIndexMarker790"/>在先导模型上进行。</li>
<li><strong class="bold">变量重要性</strong>:该特性描绘了变量<a id="_idIndexMarker791"/>在数据集中的重要性。它适用于除堆叠模型之外的所有模型。对于<a id="_idIndexMarker792"/>多个模型，只在leader模型上执行，leader模型不是堆叠模型。</li>
<li><strong class="bold">可变重要性热图</strong>:该功能绘制了所有<a id="_idIndexMarker793"/>型号的可变重要性热图。<a id="_idIndexMarker794"/>可用于比较除堆叠模型以外的所有模型。</li>
<li><strong class="bold">模型相关性热图</strong>:该<a id="_idIndexMarker795"/>功能绘制不同模型的预测值<a id="_idIndexMarker796"/>之间的相关性。这有助于将具有相似性能的模型组合在一起。它仅适用于多模型解释。</li>
<li><strong class="bold">基于顶层树的模型</strong>的SHAP摘要:该功能描绘了<a id="_idIndexMarker797"/>变量在对复杂的基于树的模型(如随机森林和神经网络)所做的<a id="_idIndexMarker798"/>决策中的重要性。此功能为排行榜中排名靠前的基于树的模型计算此图。</li>
<li><strong class="bold">部分依赖多图</strong>:这个<a id="_idIndexMarker799"/>特征<a id="_idIndexMarker800"/>绘制目标特征和数据集中我们认为重要的某组特征之间的依赖关系。</li>
<li><strong class="bold">个体条件期望</strong> ( <strong class="bold"> ICE </strong> ) <strong class="bold">绘制</strong>:该特性绘制目标<a id="_idIndexMarker802"/>特性和数据集中某一组特性之间的依赖关系<a id="_idIndexMarker801"/>，我们分别认为这些特性对每个实例都很重要。</li>
</ul>
<p>将它与我们在<em class="italic">使用模型可解释性接口</em>部分执行的实验中从模型可解释性接口获得的输出进行比较，您会注意到输出中缺少了一些可解释性特性。这是因为这些特征中的一些仅可用于被训练的模型类型。例如，残差分析仅可用于回归模型，而在使用模型可解释性接口部分的<em class="italic">中进行的实验是训练分类模型的分类问题。因此，你不会在模型的可解释性输出中找到残差分析。</em></p>
<p>你可以用一个回归问题进行同样的实验；模型可解释性接口将输出支持回归的可解释性特征。</p>
<p>现在我们知道了解释接口中可用的不同可解释特性，让我们一个一个地深入了解它们，深入理解它们的含义。我们将仔细检查用Python和r实现explainability接口得到的输出。</p>
<p>在前面的章节中，我们了解了什么是排行榜和困惑矩阵。那么，我们从下一个解释特征开始:残差分析。</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor154"/>了解残差分析</h2>
<p><strong class="bold">残差分析</strong>是<a id="_idIndexMarker803"/>对<strong class="bold">回归模型</strong>执行<a id="_idIndexMarker804"/>。如<a href="B17298_05.xhtml#_idTextAnchor109"> <em class="italic">第五章</em> </a>、<em class="italic">理解AutoML算法</em>所述，在<em class="italic">理解广义线性模型</em>和<em class="italic">线性回归简介</em>章节中，<strong class="bold">残差</strong>是<a id="_idIndexMarker805"/>回归模型预测值与同一行数据实际值之间的<a id="_idIndexMarker806"/>差值。分析这些残差值是诊断模型中任何问题的好方法。</p>
<p>残差分析图<a id="_idIndexMarker807"/>是一个图表<a id="_idIndexMarker808"/>，其中绘制了<strong class="bold">残差值</strong>与<strong class="bold">预测值</strong>。另一件我们在<a href="B17298_05.xhtml#_idTextAnchor109"> <em class="italic">第五章</em> </a>、<em class="italic">了解AutoML算法</em>，在<em class="italic">了解广义线性模型</em>和<em class="italic">了解线性回归</em>的假设章节中了解到的<a id="_idIndexMarker809"/>事情是，<strong class="bold">线性回归</strong>中的<a id="_idIndexMarker810"/>主要<a id="_idIndexMarker811"/>假设之一是残差的分布是<strong class="bold">正态分布</strong>。</p>
<p>因此，相应地，我们期望我们的剩余图是一个无定形的点的集合。残差值和预测值之间不应有任何模式。</p>
<p>残差分析可以突出训练模型中<strong class="bold">异方差</strong>的<a id="_idIndexMarker812"/>存在。如果预测值的标准偏差在不同的特征值上发生变化，则称异方差已经发生。</p>
<p>请考虑下图:</p>
<div><div><img alt="Figure 7.2 – Regression graph for a homoscedastic dataset  " height="532" src="img/B17298_07_002.jpg" width="1091"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.2-同质数据集的回归图</p>
<p>前面的<a id="_idIndexMarker813"/>图显示了一个回归图，其中我们有一些样本数据来映射<em class="italic"> X </em>和<em class="italic"> Y </em>之间的关系。让我们通过这些数据拟合一条直线，它代表我们的线性模型。如果我们在<em class="italic"> X </em>轴上从左到右计算每个点的残差，我们会注意到误差率在<em class="italic"> X </em>的所有值中保持相当恒定。这意味着所有的误差值都位于平行的蓝色<a id="_idIndexMarker814"/>线之间。误差或残差的分布在整个独立变量中保持不变的情况称为<a id="_idIndexMarker815"/>同方差。</p>
<p>与<a id="_idIndexMarker817"/>同异方差相对的<a id="_idIndexMarker816"/>是<strong class="bold">异方差</strong>。这是误差率随<em class="italic"> X </em>值的变化而变化的地方。请参考下图:</p>
<div><div><img alt="Figure 7.3 – Regression graph for a heteroscedastic dataset  " height="499" src="img/B17298_07_003.jpg" width="1017"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.3-异方差数据集的回归图</p>
<p>如您所见，线性模型产生的误差幅度随着<em class="italic"> X </em>的增加而增加。如果你画出包含所有误差的蓝色误差线，你会注意到它们逐渐散开并且不平行。这种误差或残差的分布在整个自变量中不恒定的情况称为异方差。</p>
<p><a id="_idIndexMarker818"/>异方差<a id="_idIndexMarker819"/>告诉我们的是,<a id="_idIndexMarker820"/>有一些模型无法捕捉和学习的信息。异方差也违反了线性回归的基本假设。因此，它可以帮助您确定您可能需要将缺失的信息添加到数据集中以正确定型您的线性模型，或者您可能需要实现某种非线性回归算法以获得性能更好的模型。</p>
<p>由于残差分析是特定于回归的模型可解释性特征，我们不能使用在<em class="italic">使用模型可解释性接口</em>部分中执行的Iris数据集分类实验。相反，我们需要训练一个回归模型，然后在该模型上使用模型可解释性接口来获得剩余分析输出。因此，让我们来看一个使用红酒质量数据集的回归问题。你可以在<a href="https://archive.ics.uci.edu/ml/datasets/wine+quality">https://archive.ics.uci.edu/ml/datasets/wine+quality</a>找到这个<a id="_idIndexMarker821"/>数据集。</p>
<p>该数据集由以下要素组成:</p>
<ul>
<li><strong class="bold">固定酸度</strong>:这一<a id="_idIndexMarker822"/>特征解释了不挥发酸度的数量，这意味着它不会在一段时间内蒸发。</li>
<li><strong class="bold">挥发性酸度</strong>:这个<a id="_idIndexMarker823"/>特性解释了挥发性酸度的数量，这意味着它将在一段时间内蒸发。</li>
<li><strong class="bold">柠檬酸</strong>:这个<a id="_idIndexMarker824"/>特征解释了葡萄酒中柠檬酸的含量。</li>
<li><strong class="bold">残糖</strong>:这个<a id="_idIndexMarker825"/>特征解释了葡萄酒中存在的残糖量。</li>
<li><strong class="bold">氯化物</strong>:这个<a id="_idIndexMarker826"/>特征解释了葡萄酒中存在的氯化物数量。</li>
<li><strong class="bold">游离二氧化硫</strong>:这个<a id="_idIndexMarker827"/>特征解释了葡萄酒中游离二氧化硫的含量。</li>
<li><strong class="bold">总二氧化硫</strong>:这个<a id="_idIndexMarker828"/>特征解释了葡萄酒中总二氧化硫的含量。</li>
<li><strong class="bold">密度</strong>:这个<a id="_idIndexMarker829"/>特征解释了葡萄酒的密度。</li>
<li><strong class="bold"> pH </strong>:这个<a id="_idIndexMarker830"/>特征说明了葡萄酒的pH值，0为酸性最强，14为碱性最强。</li>
<li><strong class="bold">硫酸盐</strong>:这个<a id="_idIndexMarker831"/>特征解释了葡萄酒中硫酸盐的数量。</li>
<li><strong class="bold">酒精</strong>:这个<a id="_idIndexMarker832"/>特征解释了葡萄酒中的酒精含量。</li>
<li><strong class="bold">质量</strong>:这是<a id="_idIndexMarker833"/>响应栏，记录了葡萄酒的质量。0表示酒很差，10表示酒很好。</li>
</ul>
<p>我们将运行我们训练模型的基本H2O AutoML过程，然后在被训练的AutoML对象上使用模型可解释性接口来获得残差分析图。</p>
<p>现在，让我们观察从这个实现中得到的<a id="_idIndexMarker834"/>残差分析图，然后看看我们如何从图中检索所需的信息。请参考下图:</p>
<div><div><img alt="Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset  " height="657" src="img/B17298_07_004.jpg" width="1319"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.4-红酒质量数据集的残差分析曲线图</p>
<p>在这里，您可以看到堆叠集合模型的残差分析，它是AutoML训练模型的领导者。在<em class="italic"> X </em>轴上，您有<strong class="bold">拟合</strong>，也称为预测值，而在<em class="italic"> Y </em>轴上，您有<strong class="bold">残差</strong>。</p>
<p>在<em class="italic"> Y </em>轴的左边界和<em class="italic"> X </em>轴的下方，您将分别看到一个<strong class="bold">灰度</strong>列和行。这有助于您观察这些残差在<em class="italic"> X </em>和<em class="italic"> Y </em>轴上的分布。</p>
<p>为了确保残差的<a id="_idIndexMarker835"/>分布是正态的，并且数据不是异方差的，您需要观察<em class="italic"> Y </em>轴上的灰度。理想情况下，正态分布给出的灰度是中间最暗，随着远离而变亮。</p>
<p>既然您已经理解了如何解释残差分析图，那么让我们学习更多关于下一个可解释特性:变量重要性。</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor155"/>了解变量重要性</h2>
<p><strong class="bold">变量重要性</strong>，也叫<strong class="bold">特征重要性</strong>，顾名思义，解释数据集中不同<a id="_idIndexMarker838"/>变量/特征在进行预测时的重要性<a id="_idIndexMarker837"/>。在任何ML问题中，您的数据集通常会有多个变量对您的预测列的特征产生影响。但是，在大多数情况下，您通常会有一些功能比其他功能贡献更大。</p>
<p>这种理解可以帮助科学家和工程师从数据集中移除任何引入噪声的不需要的要素。这可以进一步提高模型的质量。</p>
<p>对于不同类型的算法，H2O计算变量重要性的方式不同。首先，让我们了解一下<strong class="bold">基于树的算法</strong>是如何计算变量重要性的。</p>
<p>基于树的算法中的变量重要性基于两个标准来计算:</p>
<ul>
<li>决策树决策变量的选择</li>
<li>由于选择，整个树的平方误差有所改善</li>
</ul>
<p>每当H2O构建决策树作为训练基于树的模型的一部分时，它将使用其中一个特征作为节点来进一步分裂树。正如我们在<a href="B17298_05.xhtml#_idTextAnchor109"> <em class="italic">第5章</em> </a>、<em class="italic">了解AutoML算法</em>中所学的，在<em class="italic">了解分布式随机森林算法</em>一节中，我们知道决策树中的每个节点分裂都旨在减少总体平方误差。这个扣除的值只不过是父节点与子节点的平方误差之差。</p>
<p>H2O在计算特征重要性时考虑了这种平方误差的减少。基于树的模型中每个节点的平方误差导致该节点响应值的方差降低。基于树的模型中每个节点的平方误差导致该节点响应值的方差降低。</p>
<p>因此，相应地，用于计算树的平方<a id="_idIndexMarker840"/>误差的<a id="_idIndexMarker839"/>等式如下:</p>
<div><div><img alt="" height="71" src="img/Formula_B17298_07_001.jpg" width="1650"/>
</div>
</div>
<p>在这里，我们有以下内容:</p>
<ul>
<li><em class="italic"> MSE </em>表示均方误差</li>
<li><em class="italic"> N </em>表示观察的总数</li>
<li><em class="italic"> VAR </em>表示方差</li>
</ul>
<p>计算方差的公式如下:</p>
<div><div><img alt="" height="136" src="img/Formula_B17298_07_002.jpg" width="1650"/>
</div>
</div>
<p>在这里，我们有以下内容:</p>
<ul>
<li><img alt="" height="46" src="img/Formula_B17298_07_003.png" width="38"/>表示观察值</li>
<li><img alt="" height="38" src="img/Formula_B17298_07_004.png" width="36"/>表示所有观察值的平均值</li>
<li><em class="italic"> N </em>表示观察的总数</li>
</ul>
<p>对于基于树的集成<a id="_idIndexMarker841"/>算法，例如<strong class="bold">梯度推进算法</strong> ( <strong class="bold"> GBM </strong>)，决策树被顺序训练。每一棵树都建立在前一棵树的错误之上。因此，特征重要性的计算与我们对单个决策树中的单个节点的计算是一样的。</p>
<p>对于<strong class="bold">分布式随机森林</strong> ( <strong class="bold"> DRF </strong>)，判决<a id="_idIndexMarker842"/>树是并行训练的，所以H2O只是平均结果来计算特征重要性。</p>
<p>对于<strong class="bold"> XGBoost </strong>，当构建树时，H2O根据个体特征的损失函数的增益来计算<a id="_idIndexMarker843"/>特征重要性。</p>
<p>对于<strong class="bold">深度学习</strong>，H2O <a id="_idIndexMarker844"/>使用一种叫做<a id="_idIndexMarker845"/>的特殊方法<strong class="bold"> Gedeon方法</strong>计算特征重要性。</p>
<p>对于<strong class="bold">广义线性模型</strong> ( <strong class="bold"> GLMs </strong>)，变量<a id="_idIndexMarker846"/>重要性与预测器权重相同，也称为系数幅度。如果在训练期间，您决定对数据进行标准化，则会返回标准化系数。</p>
<p>下图<a id="_idIndexMarker847"/>显示了我们在Iris flower数据集上的实验中计算的<a id="_idIndexMarker848"/>的特征重要性:</p>
<div><div><img alt="Figure 7.5 – Variable importance graph for the Iris flower dataset  " height="742" src="img/B17298_07_005.jpg" width="1308"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.5–鸢尾花数据集的可变重要性图</p>
<p>上图显示了深度学习模型的变量重要性图。如果您将其与您的排行榜进行比较，您会看到可变重要性图是为最领先的模型绘制的，而不是堆叠整体模型。</p>
<p>在图的<em class="italic">Y</em>-轴<a id="_idIndexMarker849"/>上，您有特征名称——在<a id="_idIndexMarker850"/>我们的例子中，鸢尾花数据集的<strong class="bold"> C1 </strong>、<strong class="bold"> C2 </strong>、<strong class="bold"> C3 </strong>和<strong class="bold"> C4 </strong>列。在<em class="italic"> X </em>轴上，你有这些变量的重要性。可以获得特征重要性的原始度量值，但是H2O通过在<strong class="bold"> 0 </strong>和<strong class="bold"> 1 </strong>之间缩小来显示重要性值，其中<strong class="bold"> 1 </strong>表示最重要的变量，而<strong class="bold"> 0 </strong>表示最不重要的变量。</p>
<p>由于变量重要性对于分类和回归模型都是可用的，所以你也将得到一个变量重要性图，作为红酒质量回归模型的一个可解释特性。该图应该如下所示:</p>
<div><div><img alt="Figure 7.6 – Variable importance graph for the Red Wine Quality dataset   " height="661" src="img/B17298_07_006.jpg" width="1187"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.6-红酒质量数据集的可变重要性图</p>
<p>既然您已经知道了如何解释特性重要性图，那么让我们来理解特性重要性热图。</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor156"/>了解功能重要性热图</h2>
<p>当显示特定模型的特性<a id="_idIndexMarker851"/>重要性时，很容易用柱状图或条形图来表示。然而，我们经常需要比较各种模型的特性<a id="_idIndexMarker852"/>重要性，以便我们能够理解哪个模型认为哪个特性是重要的，以及我们如何使用这些信息来比较模型性能。H2O汽车公司将固有地用不同的ML算法训练多个模型。因此，对模型性能的比较研究是必须的，并且特征重要性的图形表示可以对科学家和工程师有很大的帮助。</p>
<p>为了在单个图形中表示H2O汽车公司训练的所有模型的特征重要性，H2O生成了特征重要性热图。</p>
<p>热图是一种数据可视化图形，其中图形的颜色受特定值的密度或大小的影响。</p>
<p>一些H2O模型计算分类列的编码版本的变量重要性。不同的模型也有不同的分类值编码方式。因此，在所有模型中比较这些分类列的可变重要性可能会很棘手。H2O通过汇总所有特征的变量重要性进行比较，并返回代表原始分类特征的单个变量重要性值。</p>
<p>以下是鸢尾花数据集<a id="_idIndexMarker854"/>实验的<a id="_idIndexMarker853"/>特征重要性热图:</p>
<div><div><img alt="Figure 7.7 – Variable importance heatmap  " height="555" src="img/B17298_07_007.jpg" width="904"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.7–可变重要性热图</p>
<p>在这里，我们可以看到排行榜上排名前10的车型。</p>
<p>热图在<em class="italic"> Y </em>轴上有<strong class="bold"> C1 </strong>、<strong class="bold"> C2 </strong>、<strong class="bold"> C3 </strong>和<strong class="bold"> C4 </strong>特征，在<em class="italic"> X </em>轴上有型号id。图的颜色表示模型在预测过程中考虑要素的重要性。越重要就意味着越有价值，这反过来会使相应的<a id="_idIndexMarker855"/>图变红。重要性越低，特征的重要性<a id="_idIndexMarker856"/>值越低；颜色会变冷，变成蓝色。</p>
<p>现在，您已经知道如何解释功能重要性热图，让我们了解一下模型相关性热图。</p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor157"/>了解模型关联热图</h2>
<p>多个模型之间的另一个重要的<a id="_idIndexMarker857"/>比较是<strong class="bold">模型相关性</strong>。模型相关性可以解释为当您<a id="_idIndexMarker859"/>比较它们的预测值时，模型<a id="_idIndexMarker858"/>在性能方面有多相似。</p>
<p>如果一个模型做出的预测与另一个模型做出的预测相同或相似，则使用相同或不同的ML算法训练的不同模型被称为高度相关。</p>
<p>在模型关联热图中，H2O比较它训练的所有模型的预测值，并将它们相互比较。</p>
<p>以下是我们在Iris flower数据集上进行实验得到的模型相关性热图:</p>
<div><div><img alt="Figure 7.8 – Model correlation heatmap for the Iris flower dataset  " height="710" src="img/B17298_07_008.jpg" width="963"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.8–鸢尾花数据集的模型关联热图</p>
<p class="callout-heading">小费</p>
<p class="callout">为了理解这个可解释特性图，请参考您在代码中执行<code>explain()</code>函数后得到的输出中的<em class="italic">模型相关性</em>部分。</p>
<p>在<em class="italic"> X </em>和<em class="italic"> Y </em>轴上，我们<a id="_idIndexMarker860"/>有型号id。它们在图上的横截面表示它们之间的相关值。您会注意到<a id="_idIndexMarker861"/>图中位于<em class="italic"> X </em>和<em class="italic"> Y </em>轴内的热点具有相同的型号ID，始终为1；所以，剧情永远是红色的。这是正确的，因为从技术上来说，这是同一个模型，当你将一个模型的预测值与其自身进行比较时，会有100%的相关性。</p>
<p>为了更好地了解不同模型之间的相关性，您可以参考这些热量值。暗红色的点表示高相关性，而具有冷蓝色值的点表示低相关性。用红色突出显示的模型是可解释的模型，如GLMs。</p>
<p>您可能<a id="_idIndexMarker862"/>注意到，由于模型关联热图支持<a id="_idIndexMarker863"/>堆叠集成模型，而特性重要性热图不支持，如果您忽略模型关联热图中的堆叠集成模型(<em class="italic">图7.8 </em>，其余模型与特性重要性热图中的模型相同(<em class="italic">图7.7 </em>)。</p>
<p>现在您已经知道了如何解释模型相关性热图，让我们了解更多关于部分依赖图的信息。</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor158"/>了解部分依赖图</h2>
<p><strong class="bold">部分相关性图</strong> ( <strong class="bold"> PDP </strong>)是一个<a id="_idIndexMarker864"/>图表，向您显示预测值<a id="_idIndexMarker865"/>和我们感兴趣的输入特征<a id="_idIndexMarker866"/>集之间的相关性，同时忽略我们不感兴趣的特征值。</p>
<p>理解PDP的另一种方式是，它表示我们感兴趣的输入要素的函数，该函数将预期的预测值作为输出提供给我们。</p>
<p>PDP是一个非常有趣的图表，对于向数据科学领域不太熟练的组织成员展示和解释模型培训结果非常有用。</p>
<p>首先，在学习如何计算DPD之前，让我们先了解如何解释它。下图显示了我们在使用Iris flower数据集进行实验时获得的PDP:</p>
<div><div><img alt="Figure 7.9 – PDP for the C1 column with Iris-setosa as the target  " height="661" src="img/B17298_07_009.jpg" width="1229"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.9–以Iris-setosa为目标的C <a id="_idTextAnchor159"/> 1列的PDP</p>
<p class="callout-heading">小费</p>
<p class="callout">为了理解这个可解释性特征图，请参考您在代码中执行<code>explain()</code>函数后得到的输出中的<em class="italic">部分依赖图</em>部分。</p>
<p>PDP图<a id="_idIndexMarker867"/>是显示特性<a id="_idIndexMarker868"/>对响应值的边际影响的图表。在图表的<em class="italic"> X </em>轴上，您可以看到所选的特性及其值的范围。在<em class="italic"> Y </em>轴上，有目标值的平均响应值。PDP图旨在告诉查看者，对于所选特性的给定值，模型预测的平均响应值是多少。</p>
<p>在<em class="italic">图7.9 </em>中，为目标值的<strong class="bold"> C1 </strong>列绘制PDP图形，目标值为<strong class="bold"> Iris-setosa </strong>。在<em class="italic"> X </em>轴上，我们有<strong class="bold"> C1 </strong>列，它代表花的萼片长度，以厘米为单位。这些值的范围从数据集中的最小值到最大值。在<em class="italic"> Y </em>轴上，我们有平均响应值。对于这个实验，平均响应值是花是鸢尾的概率，这是该图的选定目标值。图表上的彩色线条表示H2O汽车公司训练的不同模型在<strong class="bold"> C1 </strong>值范围内预测的平均响应值。</p>
<p>查看此图，我们可以很好地了解响应值如何依赖于每个单独模型的单一特征<strong class="bold"> C1 </strong>。我们可以看到，只要萼片长度在4.5到6.5厘米之间，大多数模型显示大约有35%的可能性该花属于鸢尾属。</p>
<p>同样，在<a id="_idIndexMarker869"/>下图中，我们绘制了<strong class="bold"> C1 </strong>列的PDP图，只是<a id="_idIndexMarker870"/>这次目标响应列是<strong class="bold">鸢尾-杂色</strong>:</p>
<div><div><img alt="Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target  " height="663" src="img/B17298_07_010.jpg" width="1246"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.10–以Iris-versicolor为目标的C1色谱柱的PDP</p>
<p class="callout-heading">小费</p>
<p class="callout">为了理解这个可解释性特征图，请参考您在代码中执行<code>explain()</code>函数后得到的输出中的<em class="italic">部分依赖图</em>部分。</p>
<p>在这里，我们可以看到，只要<strong class="bold"> C1 </strong>的值在4.5到6.5之间，该花大约有27%到40%的机会是鸢尾-杂色类的。现在，让我们来看看第三目标值<strong class="bold"> Iris-virginica </strong>的<strong class="bold"> C1 </strong>的以下PDP图:</p>
<div><div><img alt="Figure 7.11 – PDP for the C1 column with Iris-virginica as the target  " height="651" src="img/B17298_07_011.jpg" width="1117"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.11–以Iris-virginica为目标的C1列的PDP</p>
<p class="callout-heading">小费</p>
<p class="callout">为了更好地理解这个可解释特性图，请参考您在代码中执行<code>explain()</code>函数后得到的输出中的<em class="italic">部分依赖图</em>部分。</p>
<p>你会<a id="_idIndexMarker871"/>注意到，对于<strong class="bold"> Iris-virginica </strong>，所有的模型对<a id="_idIndexMarker872"/>C1<strong class="bold">的相同值都有不同的预测。这可能意味着<strong class="bold">鸢尾-海滨鸢尾</strong>类并不强烈依赖于花的萼片长度——即<strong class="bold"> C1 </strong>值。</strong></p>
<p>PDP可能有用的另一种情况是在模型选择中。假设您确定数据集中的某个特定要素将对响应值产生很大影响，并且您对其训练了多个模型。然后，您可以选择最适合这种关系的模型，因为该模型将做出最真实准确的预测。</p>
<p>现在，让我们试着<a id="_idIndexMarker873"/>了解PDP图是如何生成的，以及<a id="_idIndexMarker874"/> H2O是如何计算这些图值的。</p>
<p>PDP绘图数据可以计算如下:</p>
<ol>
<li value="1">选择一个特征和目标值来绘制依赖关系。</li>
<li>从验证数据集中引导数据集，其中所选要素的值被设置为所有行的验证数据集中的最小值。</li>
<li>将此引导数据集传递给由H2O AutoML训练的模型之一，并计算它为所有行获得的预测值的平均值。</li>
<li>在该型号的PDP图上绘制该值。</li>
<li>对其余型号重复<em class="italic">步骤3 </em>和<em class="italic"> 4 </em>。</li>
<li>重复<em class="italic">步骤2 </em>，但这一次，将所选特性的值增加到验证数据集中的下一个值。然后，重复剩余的步骤。</li>
</ol>
<p>您将对验证数据集中出现的所有特征值执行此操作，并将它们绘制在同一PDP图上所有模型的结果上。</p>
<p>完成后，您将对特征和目标响应值的不同组合重复相同的过程。</p>
<p>H2O将为特性和响应值的所有组合绘制多个PDP图。以下是一个PDP图，其中选择的特征是<strong class="bold"> C2 </strong>，选择的目标值是<strong class="bold">光圈-setosa </strong>:</p>
<div><div><img alt="Figure 7.12 – PDP for the C2 column with Iris-setosa as the target  " height="677" src="img/B17298_07_012.jpg" width="1253"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.12–以Iris-setosa为目标的C2色谱柱的PDP</p>
<p class="callout-heading">小费</p>
<p class="callout">为了更好地理解这个可解释特性图，请参考您在代码中执行<code>explain()</code>函数后得到的输出中的<em class="italic">部分依赖图</em>部分。</p>
<p>类似地，它<a id="_idIndexMarker875"/>为<strong class="bold"> C3 </strong>和<strong class="bold"> C4 </strong>特征创建了不同的PDP情节组合。下面的<a id="_idIndexMarker876"/>是一个PDP图，其中选择的特征是<strong class="bold"> C3 </strong>，选择的目标值是<strong class="bold">虹膜变色</strong>:</p>
<div><div><img alt="Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target  " height="660" src="img/B17298_07_013.jpg" width="1139"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.13–C3色谱柱的PDP，以Iris-versicolor为目标</p>
<p class="callout-heading">小费</p>
<p class="callout">为了更好地理解这个可解释特性图，请参考您在代码中执行<code>explain()</code>函数后得到的输出中的<em class="italic">部分依赖图</em>部分。</p>
<p>现在您已经了解了如何解释要素重要性热图，让我们了解一下SHAP汇总图。</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor160"/>了解SHAP的概要情节</h2>
<p>对于复杂的问题，基于树的模型可能变得难以理解。复杂的树模型可能非常大，理解起来非常复杂。<strong class="bold"> SHAP汇总图</strong>是基于树的模型的<a id="_idIndexMarker877"/>简化<a id="_idIndexMarker878"/>图，为您提供模型复杂性及其行为的汇总视图。</p>
<p><strong class="bold"> SHAP </strong>代表<strong class="bold">沙普利添加剂说明</strong>。SHAP是一个模型可解释性特征，它采用博弈论的方法<a id="_idIndexMarker879"/>来解释ML模型的输出。与PDP类似，SHAP汇总图显示了要素对预测值的贡献。</p>
<p>让我们试着从一个例子中解读一个<a id="_idIndexMarker880"/> SHAP值。以下是我们从红酒质量数据集中得到的SHAP <a id="_idIndexMarker881"/>总结:</p>
<div><div><img alt="Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset  " height="658" src="img/B17298_07_014.jpg" width="1078"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.14-红酒质量数据集的SHAP汇总图</p>
<p class="callout-heading">小费</p>
<p class="callout">为了更好地理解这个可解释特性图，请参考在您的回归模型上执行<code>explain()</code>函数后得到的输出中的<em class="italic"> SHAP总结</em>部分。</p>
<p>在右手边，你可以看到一个蓝红条。这个条形用颜色表示葡萄酒质量的标准化值。颜色越红，质量越好；颜色越蓝，酒质越差。在二项式问题中，颜色将是红色和蓝色之间的鲜明对比。然而，在回归问题中，就像在我们的例子中，我们可以有一个完整的色谱，表明可能的数值范围。</p>
<p>在<em class="italic"> Y </em>轴上，您可以看到数据集中的要素。根据功能的重要性，它们从上到下按降序排列。在我们的例子中，酒精含量是数据集中最重要的特征；它对最终预测值的贡献更大。</p>
<p>在<em class="italic"> X </em>轴上，你<a id="_idIndexMarker882"/>有<strong class="bold"> SHAP值</strong>。SHAP值表示特性<a id="_idIndexMarker883"/>如何帮助模型达到预期的结果。SHAP值越大，特征对结果的贡献就越大。</p>
<p>让我们以SHAP总结中的酒精为例。基于此，我们可以看到酒精在其余特征中具有最高的SHAP值。因此，酒精对模型的预测贡献很大。此外，图表上SHAP值最高的酒精点用红色表示。这也表明高酒精含量有助于积极的结果。记住这一点，我们可以从这个图表中提取的是，酒精含量的特征在预测葡萄酒的质量中起着重要的作用，酒精含量越高，葡萄酒的质量越好。</p>
<p>同样，您可以从其他特征中解读相同的知识。这可以帮助您比较和了解哪些特征是重要的，以及它们如何影响模型的最终预测。</p>
<p>关于SHAP摘要和PDP的一个有趣问题<a id="_idIndexMarker884"/>是，它们之间有什么区别？这两者的主要区别在于，PDP解释了一次仅替换一个要素对输出的影响，而SHAP汇总考虑了该要素与数据集中其他要素的整体交互。因此，PDP的工作假设您的特征是相互独立的，而SHAP考虑了不同特征的综合贡献及其对整体预测的综合影响。</p>
<p>计算<strong class="bold"> SHAP值</strong>是一个源自博弈论的复杂过程。如果你对扩展你的博弈论知识和SHAP值是如何计算的感兴趣，请按照你自己的进度随意探索。理解SHAP的一个很好的起点是遵循https://shap.readthedocs.io/en/latest/index.xhtml.的解释<a id="_idIndexMarker885"/>在撰写本文时，H2O充当SHAP库的包装器，并在内部使用这个库来计算SHAP值。</p>
<p>既然我们知道了如何解释SHAP摘要情节，让我们来学习可解释性特征，<strong class="bold">个体条件期望</strong> ( <strong class="bold"> ICE </strong> ) <a id="_idTextAnchor161"/>情节。</p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor162"/>理解单个条件期望图</h2>
<p>一个<strong class="bold"> ICE </strong> plot <a id="_idIndexMarker886"/>是一个为每个<a id="_idIndexMarker887"/>实例显示一条线的图形，一个观察显示当一个特性的值改变时给定观察的预测如何改变。</p>
<p>ICE图类似于PDP图。PDP侧重于某个特征的变化对预测结果的总体平均影响，而ICE图侧重于结果对特征值的单个实例的依赖性。如果你平均冰图值，你应该得到一个PDP。</p>
<p>计算ICE图的方法非常简单，如下图所示:</p>
<div><div><img alt="Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation 1  " height="225" src="img/B17298_07_015.jpg" width="1026"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.15–ICE图表的样本数据集，突出了观察结果1</p>
<p>一旦您的模型经过训练，您必须执行以下步骤来计算ICE图:</p>
<ol>
<li value="1">考虑第一个观察值——在我们的示例中为<strong class="bold">观察值1</strong>——并绘制<strong class="bold">特征1 </strong>和相应的<strong class="bold">目标值</strong>之间的关系。</li>
<li>保持<strong class="bold">特征1 </strong>中的值不变，创建一个引导数据集，同时用原始数据集中的<strong class="bold">观察1 </strong>中的值替换所有其他特征值；将所有其他观察标记为<strong class="bold">观察1 </strong>。</li>
<li>使用您训练好的模型计算<a id="_idIndexMarker889"/>观察值的<a id="_idIndexMarker888"/>目标值<strong class="bold">。</strong></li>
</ol>
<p>请参考以下自举数据集的屏幕截图:</p>
<div><div><img alt="Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1  " height="218" src="img/B17298_07_016.jpg" width="1110"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.16–要素1的观测值1的引导数据集</p>
<ol>
<li value="4">对下一次观察重复同样的步骤。考虑第二个观察值——在我们的示例中为<strong class="bold">观察值2</strong>——并绘制<strong class="bold">特征1 </strong>和相应的<strong class="bold">目标值</strong>之间的关系:</li>
</ol>
<div><div><img alt="Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2  " height="222" src="img/B17298_07_017.jpg" width="1159"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.17–突出观察值2的冰图的样本数据集</p>
<ol>
<li value="5">保持<strong class="bold">功能1 </strong>中的值不变，并创建一个引导数据集；然后，使用训练好的模型计算<strong class="bold">目标</strong>值。请参考以下生成的引导数据集:</li>
</ol>
<div><div><img alt="Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1  " height="220" src="img/B17298_07_018.jpg" width="1001"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.18–特征1的观测值2的引导数据集</p>
<ol>
<li value="6">我们<a id="_idIndexMarker890"/>对所有特征的所有观察值<a id="_idIndexMarker891"/>重复这个过程。</li>
<li>从这些引导数据集观察到的结果绘制在每个要素的单个ICE图上。</li>
</ol>
<p>让我们看看如何解释ICE图，并从图表中提取可观察到的信息。参考下面的屏幕截图，它显示了我们在AutoML对象上运行模型可解释性接口后得到的ICE图，该对象是在红酒质量数据集上训练的:</p>
<div><div><img alt="Figure 7.19 – ICE plot for the Red Wine Quality dataset  " height="653" src="img/B17298_07_019.jpg" width="1147"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.19-红酒质量数据集的冰图</p>
<p>正如标题<a id="_idIndexMarker892"/>所述，这是H2O汽车公司训练的堆叠集合模型数据集的酒精<a id="_idIndexMarker893"/>特征列上的冰图。请记住，这个模型是AutoML训练的模型列表中的领导者。ICE图仅针对数据集的前导绘制。您还可以通过使用模型id提取模型，然后对其运行<code>ice_plot()</code>功能，来观察其他模型的ICE图。请参考下面的代码示例:</p>
<pre class="source-code">

model = h2o.get_model("XRT_1_AutoML_2_20220516_64004")

model.ice_plot(test_dataframe, "alcohol")</pre>
<p>在图表的<em class="italic"> X </em>轴上，您可以看到酒精特性的数值范围。在Y轴上，你有预测结果的数值范围，也就是葡萄酒的质量。</p>
<p>在图表的左侧，您可以看到说明不同类型线条及其百分位数的图例。ICE图描绘了每个十分位数的影响。所以，从技术上讲，在绘制冰图时，你要为每个观察值计算一条线。但是，在包含数千行(如果不是数百万行的话)数据的数据集中，您最终会在图上看到相同数量的线条。这样会把冰剧情搞得乱七八糟。这就是为什么要更好地观察这些数据，您必须将这些线聚集到最接近的十分位数，并为每个百分位划分绘制一条线。</p>
<p>虚线<a id="_idIndexMarker894"/>是所有其他<a id="_idIndexMarker895"/>百分位线的平均值，是该特性的PDP线。</p>
<p>现在你知道了如何解读冰图，让我们来看看学习曲线图。</p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor163"/>了解学习曲线图</h2>
<p><strong class="bold">学习曲线图</strong>是<a id="_idIndexMarker896"/>数据科学家用来观察模型学习率的最常用图之一。<strong class="bold">学习曲线</strong>展示了你的模型如何从<a id="_idIndexMarker897"/>数据集学习，以及用<a id="_idIndexMarker898"/>进行学习的效率。</p>
<p>在处理一个ML问题时，经常需要回答的一个重要问题是，<em class="italic">我们需要多少数据来训练最准确的模型？</em>学习曲线图可以帮助您了解增加数据集如何影响您的整体模型性能。</p>
<p>使用这些信息，您可以决定增加数据集的大小是否可以提高模型性能，或者是否需要对模型进行定型以提高模型的性能。</p>
<p>让我们观察一下学习曲线图，它是我们在由AutoML训练的XRT模型的红酒质量数据集上的实验中得到的:</p>
<div><div><img alt="Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality dataset  " height="572" src="img/B17298_07_020.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.20-红酒质量数据集上XRT模型的学习曲线图</p>
<p>在图的<em class="italic">X</em>-轴上，你有由XRT <a id="_idIndexMarker900"/>算法创建的树的数量。如你所见，该算法总共创建了大约40到50棵树。在<em class="italic"> Y </em>轴上，您可以看到性能指标RMSE，它是在算法创建树的模型训练的每个阶段计算的。</p>
<p>如前面的屏幕截图所示，随着算法创建更多的树，RMSE度量会降低。最终，在一定数量的树木被创造出来后，RMSE下降的速度会降低。任何超过这个数量的树都不会对模型性能的整体改善有所贡献。因此，学习率最终随着几棵树的增加而降低。</p>
<p>图表上的线条描绘了算法在训练期间使用的各种数据集，以及在创建树的每个实例期间各自的RMSE。</p>
<p>在撰写本文时，从H2O版本<em class="italic"> 3.36.1 </em>开始，学习曲线绘图不是默认模型可解释性界面的一部分。要绘制学习曲线，必须在相应的模型对象上使用以下函数进行绘制:</p>
<pre class="source-code">

model = h2o.get_model("GLM_1_AutoML_2_20220516_64004")

model.learning_curve_plot()</pre>
<p>不同算法的学习<a id="_idIndexMarker901"/>曲线图不同。下面的<a id="_idIndexMarker902"/>截图显示了AutoML在相同数据集上训练的GLM模型的学习图:</p>
<div><div><img alt="Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality dataset  " height="571" src="img/B17298_07_021.jpg" width="997"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">图7.21-红酒质量数据集上GLM模型的学习曲线图</p>
<p>如您所见，我们现在有迭代，而不是在<em class="italic"> X </em>轴上有多少棵树。树的数量与基于树的算法(如XRT和DRF)有关，但线性模型(如在线性算法上运行的GLM)对于帮助学习更有意义。在<em class="italic"> Y </em>轴上，你有偏差而不是RMSE，因为偏差更适合于测量线性模型的性能。</p>
<p>学习曲线<a id="_idIndexMarker903"/>对于不同类型的算法是不同的，包括堆叠集成模型。随意探索不同算法的学习曲线的不同变化。H2O已经根据算法选择了合适的性能指标和学习步骤，因此您不必担心是否选择了正确的指标来衡量学习速度。</p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor164"/>总结</h1>
<p>在这一章中，我们重点理解了H2O提供的模型可解释性接口。首先，我们理解了explainability接口如何提供不同的explainability特性，来帮助用户获得关于被训练模型的详细信息。然后，我们学习了如何在由H2O的AutoML训练的模型上用Python和r实现这个功能</p>
<p>一旦我们熟悉了它的实现，我们就开始探索和理解由可解释性接口的输出显示的各种可解释性图表，从残差分析开始。我们观察了残差分析如何帮助突出数据集中的异方差性，以及它如何帮助您识别数据集中是否有任何缺失信息。</p>
<p>然后，我们探讨了可变重要性以及它如何帮助您识别数据集中的重要要素。在此基础上，我们了解了特征重要性热图如何帮助您观察AutoML训练的所有模型中的特征重要性。</p>
<p>然后，我们发现了如何解释模型关联热图，以及它们如何帮助我们从模型列表中识别具有相似预测行为的模型。</p>
<p>后来，我们学习了PDP图，以及它们如何表达整体结果对数据集单个特征的依赖性。了解了这一点后，我们研究了SHAP总结和ICE图，在那里我们理解了这两个图表，以及每个图表如何关注结果依赖于个体特征的不同方面。</p>
<p>最后，我们探讨了什么是学习图，以及它如何帮助我们理解模型如何在观察、迭代或树的数量上提高性能，也称为学习，这取决于用于训练模型的算法类型。</p>
<p>在下一章中，我们将使用我们在前几章中学到的所有知识，并探索在使用H2O的AutoML功能时可用的其他高级参数。</p>
</div>
</div></body>
</html></body></html>