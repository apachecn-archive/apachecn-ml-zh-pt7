

# 八、超参数优化

Kaggle 解决方案的表现不仅仅取决于您选择的学习算法的类型。除了您使用的数据和特征之外，它还在很大程度上由算法的**超参数**决定，这些算法参数必须在训练之前固定*，并且不能在训练过程中学习。选择正确的变量/数据/特征在表格数据竞赛中最为有效；然而，超参数优化在任何种类的*所有*比赛中都有效。事实上，在给定固定数据和算法的情况下，超参数优化是增强算法预测性能和攀登排行榜的唯一可靠方法。这也有助于集合，因为经过调整的模型集合总是比未经调整的模型集合表现得更好。*

如果您知道并理解您的选择对算法的影响，您可能会听说手动调整超参数是可能的。许多 Kaggle 大师和大师都宣称，他们经常在比赛中直接调整他们的模型。它们以一分为二的操作方式选择性地操作最重要的超参数，探索越来越小的参数值区间，直到找到产生最佳结果的值。然后，他们转向另一个参数。如果每个参数都有一个最小值，并且这些参数是相互独立的，那么这就非常好了。在这种情况下，搜索主要由经验和学习算法的知识驱动。然而，根据我们的经验，你在 Kaggle 上遇到的大多数任务并不是这样的。问题的复杂性和使用的算法需要一个系统的方法，只有搜索算法可以提供。因此，我们决定写这一章。

在这一章中，我们将探索如何扩展您的交叉验证方法，以找到可以推广到您的测试集的最佳超参数。这个想法是为了处理你在比赛中经历的压力和时间、资源的稀缺。出于这个原因，我们将专注于**贝叶斯优化方法**，这是一种基于您可用资源优化复杂模型和数据问题的成熟方法。我们不会局限于搜索预定义超参数的最佳值；我们还将深入研究神经网络架构的问题。

我们将讨论以下主题:

*   基本优化技术
*   关键参数及其使用方法
*   贝叶斯优化

开始吧！

# 基本优化技术

Scikit-learn 包中的超参数优化核心算法是**网格搜索**和**随机搜索**。最近，Scikit-learn 的贡献者还在中添加了**减半算法**，以提高网格搜索和随机搜索策略的性能。

在本节中，我们将讨论所有这些基本技术。通过掌握它们，你不仅将拥有针对某些特定问题的有效优化工具(例如，支持向量机通常通过网格搜索进行优化)，而且你还将熟悉超参数优化的基本原理。

首先，找出必要的成分至关重要:

*   超参数需要优化的模型
*   一个搜索空间，包含要在每个超参数之间搜索的值的边界
*   交叉验证方案
*   一种评价指标及其得分函数

所有这些因素在搜索方法中结合在一起，以确定您要寻找的解决方案。让我们看看它是如何工作的。

## 网格搜索

**网格搜索**是穷举搜索超参数的方法，在高维空间不可行。对于每个参数，您选择一组想要测试的值。然后测试这个集合中所有可能的组合。这就是为什么它是详尽无遗的:你尝试一切。这是一个非常简单的算法，它遭受了维数灾难，但是，从积极的方面来看，它是令人尴尬的并行*(参见[https://www . cs . iusb . edu/~ danav/teach/b424/b424 _ 23 _ emb par . html](https://www.cs.iusb.edu/~danav/teach/b424/b424_23_embpar.html)了解这个计算机科学术语的定义)。这意味着，如果您有足够的处理器来运行搜索，您可以非常快地获得最佳调优。*

 *举个例子，我们拿一个分类问题和**支持向量机分类** ( **SVC** )。**针对分类和回归问题的支持向量机** ( **SVMs** )可能是你会使用网格搜索最多的机器学习算法。使用 Scikit-learn 中的`make_classification`函数，我们可以快速生成分类数据集:

```
from sklearn.datasets import make_classification

from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=300, n_features=50,

                           n_informative=10,

                           n_redundant=25, n_repeated=15,

                           n_clusters_per_class=5,

                           flip_y=0.05, class_sep=0.5,

                           random_state=0) 
```

对于我们的下一步，我们定义一个基本的 SVC 算法，并设置搜索空间。由于 SVC 的**内核** **函数**(在 SVM 中转换输入数据的内部函数)决定了要设置的不同超参数，我们提供了一个包含两个不同搜索空间字典的列表，用于根据所选内核类型使用的参数。我们还设置了评估指标(在这种情况下，我们使用准确性，因为目标是完全平衡的):

```
from sklearn import svm

svc = svm.SVC()

svc = svm.SVC(probability=True, random_state=1)

from sklearn import model_selection

search_grid = [

               {'C': [1, 10, 100, 1000], 'kernel': ['linear']},

               {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],

               'kernel': ['rbf']}

               ]

scorer = 'accuracy' 
```

在我们的例子中，线性核不需要调整`gamma`参数，尽管这对径向基函数核非常重要。因此，我们提供了两个字典:第一个包含线性核的参数，第二个包含径向基函数核的参数。每个字典只包含一个与其相关的内核的引用，以及与该内核相关的参数范围。

重要的是要注意，评估指标可以不同于算法优化的成本函数。事实上，正如第五章、*竞赛任务和指标*中所讨论的，您可能会遇到竞赛的评估指标不同的情况，但是您无法修改您的算法的成本函数。在这种情况下，根据您的评估度量调整超参数仍然有助于获得性能良好的模型。尽管是围绕算法的成本函数构建的，但找到的最佳超参数集将是在这种约束下返回最佳评估度量的超参数集。这可能不是理论上你能得到的解决问题的最好结果，但是它可能经常离它不远。

所有的成分(模型、搜索空间、评估标准、交叉验证方案)都被组合到`GridSearchCV`实例中，然后模型与数据相匹配:

```
search_func = model_selection.GridSearchCV(estimator=svc, 

                                           param_grid=search_grid,

                                           scoring=scorer, 

                                           n_jobs=-1,

                                           cv=5)

search_func.fit(X, y)

print (search_func.best_params_)

print (search_func.best_score_) 
```

一段时间后，根据运行优化的机器，您将获得基于交叉验证结果的最佳组合。

总之，网格搜索是一种非常简单的优化算法，可以利用多核计算机的可用性。它可以很好地与不需要许多调整的机器学习算法一起工作(如 SVM 和山脊和拉索回归)，但在所有其他情况下，它的适用性非常有限。首先，它仅限于通过离散选择优化超参数(您需要一组有限的值来循环)。此外，你不能指望它在需要调整多个超参数的算法上有效工作。这是因为搜索空间的爆炸式复杂性，并且因为大多数计算的低效率是由于搜索是盲目地尝试参数值的事实，其中大多数对问题不起作用。

## 随机搜索

**随机搜索**，即简单地对搜索空间进行随机采样，在高维空间中是可行的，在实践中被广泛使用。然而，随机搜索的缺点是，它不使用先前实验的信息来选择下一个设置(我们应该注意，网格搜索也有这个问题)。此外，为了尽快找到最佳解决方案，除了希望幸运地找到正确的超参数之外，您什么也做不了。

随机搜索的效果非常好，而且很容易理解。尽管事实上它依赖于随机性，但它不仅仅是基于盲目的运气，尽管它可能最初看起来是这样。事实上，它的工作原理类似于统计学中的随机抽样:该技术的要点是，如果您进行了足够多的随机测试，您就很有可能找到正确的参数，而不必浪费精力来测试类似执行组合的稍微不同的组合。

当有太多参数需要设置时，许多 AutoML 系统依赖于随机搜索(参见 Golovin，d .等人 *Google Vizier:一种用于黑盒优化的服务*，2017)。根据经验，当您的超参数优化问题的维度足够高(例如，超过 16)时，可以考虑使用随机搜索。

下面，我们使用随机搜索运行前面的示例:

```
import scipy.stats as stats

from sklearn.utils.fixes import loguniform

search_dict = {'kernel': ['linear', 'rbf'], 

               'C': loguniform(1, 1000),

               'gamma': loguniform(0.0001, 0.1)

               }

scorer = 'accuracy'

search_func = model_selection.RandomizedSearchCV

              (estimator=svc,param_distributions=search_dict, n_iter=6,

              scoring=scorer, n_jobs=-1, cv=5)

search_func.fit(X, y)

print (search_func.best_params_)

print (search_func.best_score_) 
```

注意，现在，我们不关心在不同的空间中为不同的内核运行搜索。与网格搜索相反，在网格搜索中，系统地测试每个参数，甚至是无效的参数，这需要计算时间，在网格搜索中，搜索的效率不受测试的超参数集的影响。搜索不依赖于无关的参数，而是由机会引导；任何尝试都是有用的，即使您只测试所选内核的许多参数中的一个有效参数。

## 减半搜索

正如我们提到的,网格搜索和随机搜索都是在不知情的情况下工作的:如果一些测试发现某些超参数不影响结果或者某些值区间无效，则信息不会传播到后续搜索。

出于这个原因，Scikit-learn 最近推出了`HalvingGridSearchCV`和`HalvingRandomSearchCV`估计器，可以使用应用于网格搜索和随机搜索调整策略的**连续减半**来搜索参数空间。

在减半中，在第一轮测试中评估大量超参数组合，但是使用少量计算资源。这是通过对来自您的训练数据的几个案例的子样本运行测试来实现的。较小的训练集需要测试较少的计算，因此使用较少的资源(即时间),代价是更不精确的性能估计。当训练集大小增加时，该初始回合允许选择在问题上表现更好的候选超参数值的子集，用于第二轮。

随后几轮以类似的方式进行，随着测试值的范围受到限制，分配越来越大的训练集子集进行搜索(测试现在需要更多的时间来执行，但会返回更精确的性能估计)，同时候选人的数量继续减半。

下面是一个应用于前一个问题的例子:

```
from sklearn.experimental import enable_halving_search_cv

from sklearn.model_selection import HalvingRandomSearchCV

search_func = HalvingRandomSearchCV(estimator=svc,

                                    param_distributions=search_dict,

                                    resource='n_samples',

                                    max_resources=100,

                                    aggressive_elimination=True,

                                    scoring=scorer,

                                    n_jobs=-1,

                                    cv=5,

                                    random_state=0)

search_func.fit(X, y)

print (search_func.best_params_)

print (search_func.best_score_) 
```

以这种方式，减半通过候选的选择向连续的优化步骤提供信息。在接下来的部分中，我们将讨论通过超参数空间实现更精确和有效搜索的更智能的方法。

![](img/Kazuki_Onodera.png)

友川·小野寺

[https://www.kaggle.com/onodera](https://www.kaggle.com/onodera)

让我们停下来采访另一位卡格勒。友川·小野寺是一名竞赛大师和讨论大师，拥有大约 7 年的竞赛经验。他还是 NVIDIA 的高级深度学习数据科学家，也是 NVIDIA kg mon(NVIDIA 的 Kaggle Grandmasters)团队的成员。

你最喜欢哪种比赛，为什么？从技术和解决途径来说，你在 Kaggle 上的特长是什么？

Instacart 市场篮分析。*事实证明，这场竞赛对 Kaggle 社区来说极具挑战性，因为它使用了与客户订单相关的匿名数据来预测用户的下一个订单中会出现哪些之前购买的产品。我喜欢它的原因是我喜欢功能工程，我可以想出一堆别人做不到的又好又有趣的功能，这让我在比赛中获得了第二名。*

你是如何对待一场 Kaggle 比赛的？这种方法与你在日常工作中的做法有什么不同？

*我试图想象一个模型是如何工作的，并深入研究假阴性和假阳性。和我的日常工作一样。*

告诉我们你参加的一个特别有挑战性的比赛，以及你用什么样的洞察力来完成这个任务。

人类蛋白质图谱-单细胞分类。*该竞赛是一种实例分割竞赛，但不提供遮罩。因此，它变成了一个弱监督多标签分类问题。我创建了一个消除标签噪声的两级管道。*

Kaggle 对你的职业生涯有帮助吗？如果有，如何实现？

*是的。我现在在 NVIDIA kg mon(NVIDIA 的 Kaggle 特级大师)团队工作。Kaggle 发起了许多不同的机器学习竞赛，这些竞赛在数据类型、表格、图像、自然语言和信号方面不同，在行业和领域方面也不同:工业、金融、天文学、病理学、体育、零售等等。我敢打赌，除了卡格勒之外，没有人能接触到所有这些类型的数据，也没有人有这方面的经验。*

以你的经验来看，没有经验的 Kagglers 经常会忽略什么？你现在知道了什么，你希望在你刚开始的时候就知道？

*目标分析。此外，种子平均法很容易被忽视:它总是简单而强大。*

你在过去的比赛中犯过什么错误？

*目标分析。顶尖团队总是比其他团队更好地分析目标，所以如果我不能在比赛中获得更好的位置，我就会去阅读顶级解决方案，因为他们总是向我描述我在比赛中错过的数据知识。*

对于数据分析或机器学习，你有什么特别推荐的工具或库吗？

*只是 Python 和 Jupyter**的笔记本。*

当一个人参加比赛时，他应该记住或做的最重要的事情是什么？

如果你能从失败中吸取教训，你就没有真正失败。

你使用其他比赛平台吗？他们和 Kaggle 相比如何？

*KDD 杯和 RecSys。两者都符合有趣和挑战性的最低要求。*

# 关键参数及其使用方法

下一个问题是为您使用的每种模型使用正确的超参数集。特别是，为了提高优化效率，您需要知道每个超参数的值，这对于测试每个不同的算法是有意义的。

在本节中，我们将研究 Kaggle 竞赛中最常用的模型，尤其是表格模型，并讨论为了获得最佳结果需要调整的超参数。对于一般的表格数据问题，我们将区分经典的机器学习模型和梯度推进模型(在参数空间方面要求更高)。

至于神经网络，当我们展示标准模型时，我们可以让您了解要调整的特定参数(例如，TabNet 神经模型需要设置一些特定的参数，以便它能够正常工作)。然而，Kaggle 竞赛中对深度神经网络的大多数优化不是在标准模型上进行的，而是在*定制*模型上进行的。因此，除了基本的学习参数(如学习速率和批量大小)之外，神经网络的优化还基于模型的神经结构的具体特征。你必须临时处理这个问题。在本章接近尾声时，我们将讨论一个使用 kera stuner([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))的**神经架构搜索** ( **NAS** )的例子。

## 线性模型

需要调整的线性模型通常是线性回归或带有正则化的逻辑回归；

*   `C`:你要搜索的范围是`np.logspace(-4, 4, 10)`；较小的值指定较强的正则化。
*   `alpha`:你要搜索范围`np.logspace(-2, 2, 10)`；较小的值指定较强的正则化，较大的值指定较强的正则化。还要注意，当使用套索时，较高的值需要更多的时间来处理。
*   `l1_ratio`:你应该从列表中挑选`[.1, .5, .7, .9, .95, .99, 1]`；它只适用于弹性网。

在 Scikit-learn 中，根据算法的不同，你会找到超参数`C`(逻辑回归)或`alpha`(套索、脊、弹性网)。

## 支持向量机

**支持向量机**是一个系列强大而先进的监督学习技术，用于分类和回归，可以自动拟合线性和非线性模型。Scikit-learn 基于`LIBSVM`和`LIBLINEAR`提供了一个实现，前者是 SVM 分类和回归实现的完整库，后者是线性分类的可扩展库，非常适合大型数据集，尤其是基于稀疏文本的数据集。在它们的优化中，支持向量机努力使用以类别之间最大可能间隔为特征的决策边界来分离分类问题中的目标类别。

尽管支持向量机在默认参数下工作良好，但它们通常不是最佳的，您需要使用交叉验证来测试各种值组合，以找到最佳值。根据它们列出的重要性，你必须设置以下参数:

*   `C`:惩罚值。减小它会使类之间的差值变大，从而忽略更多的噪声，但也会使模型更具概化能力。通常可以在范围`np.logspace(-3, 3, 7)`内找到最佳值。
*   `kernel`:该参数将决定如何在 SVM 中实现非线性，可设置为`'linear'`、`'poly'`、`'`、`rbf'`、`'sigmoid'`或自定义内核。最常用的数值当然是`rbf`。
*   `degree`:与`kernel='poly'`一起使用，表示多项式展开的维数。它会被其他内核忽略。通常，将其值设置在`2`和`5`之间效果最好。
*   `gamma`:用于`'rbf'`、`'`、`poly'`和`'sigmoid'`的系数。高值倾向于以更好的方式拟合数据，但会导致一些过度拟合。直观地说，我们可以把`gamma`想象成一个例子对模型的影响。低值让每个例子的影响力达到更远。由于必须考虑许多点，SVM 曲线将倾向于采取受局部点影响较小的形状，并且结果将是更平滑的决策轮廓曲线。相反，较高的`gamma`值意味着曲线更多地考虑了点在局部的排列方式，因此，您会得到一条更不规则、更不规则的决策曲线。该超参数的建议网格搜索范围为`np.logspace(-3, 3, 7)`。
*   `nu`:对于使用 nuSVR 和 nuSVC 的回归和分类，该参数为接近边缘且分类不正确的训练点设置一个公差。它有助于忽略边缘附近或边缘上的错误分类点，因此它可以使分类决策曲线更加平滑。它应该在范围`[0,1]`内，因为它是相对于你的训练集的一个比例。最终，它的行为类似于`C`，高比例扩大了利润。
*   `epsilon`:该参数通过定义一个`epsilon`大范围来指定 SVR 将接受多少误差，在该大范围内，在算法的训练过程中，没有与示例的不正确预测相关联的惩罚。建议的搜索范围是`np.logspace(-4, 2, 7)`。
*   `penalty`、`loss`和`dual`:对于 LinearSVC，这些参数接受`('l1', 'squared_hinge', False)`、`('l2', 'hinge', True)`、`('l2', 'squared_hinge', True)`和`('l2', 'squared_hinge', False)`的组合。`('l2', 'hinge', True)`组合类似于`SVC(kernel='linear')`学习器。

一个 SVM 似乎有许多超参数需要设置，但许多设置只针对实现或内核，所以您只需选择相关的参数。

## 随机森林和极度随机的树

利奥·布雷曼和 T42【阿黛尔·卡特勒最初设计了随机森林算法的核心思想，该算法的名称今天仍然是他们的商标(尽管该算法是开源的)。随机森林在 Scikit-learn 中实现为`RandomForestClassifier`或`RandomForestRegressor`。

随机森林的工作方式与 bagging类似，bagging 也是由 Leo Breiman 设计的，但它只使用二叉分裂决策树进行操作，让其发展到极致。此外，它使用**引导**对每个模型中使用的案例进行采样。随着树的生长，在一个分支的每一个分叉处，为分叉考虑的变量集也被随机抽取。

这是算法核心的秘密:它集成了树，由于分割时考虑的不同样本和变量，这些树彼此非常不同。因为它们是不同的，所以它们也是不相关的。这是有益的，因为当结果被集合时，许多方差被排除，因为分布两边的极值趋于平衡。换句话说，bagging 算法保证了预测中一定程度的多样性，允许它们开发单个学习者(如决策树)可能不会遇到的规则。所有这些多样性都是有用的，因为它有助于建立一个分布，其平均值比集合中的任何一棵树都更好地预测。

**额外树**(也称为**极度随机化树**)，在 Scikit-learn 中由`ExtraTreesClassifier` / `ExtraTreesRegressor`类表示，是一种更加随机化的随机森林，它以估计器的更大偏差为代价，在估计中产生更低的方差。然而，就 CPU 效率而言，与随机树相比，额外的树可以提供相当大的速度提升，因此当您在示例和功能方面处理大型数据集时，它们可能是理想的。产生更高偏差但更快速度的原因是分裂是在一个额外的树中构建的。随机森林，在绘制了一组随机的要被考虑用于分割一棵树的分支的特征之后，仔细地在它们之中搜索分配给每个分支的最佳值。相比之下，在额外的树中，用于分割的候选特征集和实际分割值都是完全随机决定的。因此，不需要太多的计算，尽管随机选择的分割可能不是最有效的(因此有偏差)。

对于这两种算法，应该设置的关键超参数如下:

*   `max_features`:这是每次分割时出现的采样特征的数量，可以决定算法的性能。数字越小，速度越快，但偏差越大。
*   `min_samples_leaf`:这可以让你决定树的深度。大的数字会减少方差，增加偏差。
*   `bootstrap`:这是一个允许引导的布尔值。
*   `n_estimators`:这是树的数量。请记住，树越多越好，虽然有一个阈值，超过这个阈值，我们会根据数据问题得到递减的回报。此外，这需要计算成本，您必须根据可用的资源来考虑这一点。

额外的树是随机森林的一个很好的替代，尤其是当你拥有的数据特别嘈杂的时候。由于他们在随机选择分裂时会用一些方差减少来换取更多的偏差，因此他们倾向于减少对重要但有噪声的特征的过度拟合，否则这些特征会在随机森林中的分裂中占主导地位。

## 梯度树提升

梯度树提升或**梯度提升决策树** ( **GBDT** )是提升的改进版本(提升通过将一系列弱学习者拟合到数据的重新加权版本上来工作)。像 AdaBoost 一样，GBDT 也是基于梯度下降函数。该算法已被证明是基于系综的模型家族中最熟练的算法之一，尽管它的特点是估计方差增加，对数据中的噪声更敏感(这两个问题都可以通过使用子采样来缓解)，以及由于非并行操作而导致的大量计算成本。

除了深度学习，梯度提升是最发达的机器学习算法。自从 AdaBoost 和最初的梯度增强实现(由 *Jerome Friedman* 开发)以来，出现了各种其他算法实现，最近的是 XGBoost、LightGBM 和 CatBoost。

### LightGBM

高性能的 LightGBM 算法(【https://github.com/Microsoft/LightGBM】T21)能够分布在多台计算机上，快速处理大量数据。它是由微软的一个团队作为 GitHub 上的开源项目开发的(还有一篇学术论文:[https://papers . nips . cc/paper/2017/hash/6449 F4 4a 102 FDE 848669 BDD 9 EB 6b 76 fa-abstract . html](https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html))。

与 XGBoost 一样，LightGBM 也是基于决策树的，但是它遵循不同的策略。XGBoost 使用决策树对一个变量进行分割，并在该变量上探索不同的树分割(层次式**树生长策略)，而 LightGBM 专注于一个分割，并从那里继续分割，以实现更好的拟合(叶式**树生长策略)。这使得 LightGBM 能够快速地获得数据的良好拟合，并生成与 XGBoost 相比的替代解决方案(如果您希望将两种解决方案混合在一起以减少估计的方差，这是一个好方法)。从算法上来说，如果我们把决策树操作的分裂结构看作一个图，XGBoost 追求的是*广度优先*搜索(BFS ), light GBM 追求的是*深度优先*搜索(DFS)。

调优 LightGBM 可能会让人望而生畏；它有一百多个参数可以调整，你可以在这个页面上探索:[https://github . com/Microsoft/light GBM/blob/master/docs/parameters . rst](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)(也在这里:[https://lightgbm.readthedocs.io/en/latest/Parameters.html](https://lightgbm.readthedocs.io/en/latest/Parameters.html))。

根据经验，您应该关注以下超参数，它们通常对结果影响最大:

*   `n_estimators`:10 到 10000 之间的整数，设置迭代次数。
*   `learning_rate`:0.01 到 1.0 之间的实数，通常从对数均匀分布中采样。它表示梯度下降过程的步长，该过程计算到目前为止算法的所有迭代的总和的权重。
*   `max_depth`:1 到 16 之间的整数，代表特征的最大分割数。将其设置为小于 0 的数字会允许最大可能的拆分次数，通常会有过度适应数据的风险。
*   `num_leaves`:介于 2 和 2^ `max_depth`之间的整数，代表每棵树最终最多拥有的叶子数。
*   `min_data_in_leaf`:0-300 之间的整数，确定一个叶中的最小数据点数。
*   `min_gain_to_split`:0 到 15 之间的浮点数；它设置了树分割算法的最小增益。通过设置这个参数，可以避免不必要的树分裂，从而减少过度拟合(它对应于 XGBoost 中的`gamma`参数)。
*   `max_bin`:一个介于 32 和 512 之间的整数，用于设置特征值将被存储到的最大箱数。此参数大于默认值 255 意味着产生过度拟合结果的风险更大。
*   `subsample`:介于 0.01 和 1.0 之间的实数，代表用于训练的样本部分。
*   `subsample_freq`:介于 0 和 10 之间的整数，指定算法对示例进行二次采样的迭代频率。

注意，如果设置为零，算法将忽略任何给定给`subsample`参数的值。此外，默认设置为零，因此仅设置`subsample`参数不起作用。

*   `feature_fraction`:一个介于 0.1 和 1.0 之间的实数，允许您指定要进行二次抽样的特征部分。对要素进行二次采样是允许更多随机化在训练中发挥作用的另一种方式，可消除要素中存在的噪声和多重共线性。
*   `subsample_for_bin`:介于 30 和例数之间的整数。这设置了为构建直方图仓而采样的样本数。
*   `reg_lambda`:设置 L2 正则化的 0 到 100.0 之间的实数。由于它对尺度比对参数的确切数目更敏感，因此通常从对数均匀分布中取样。
*   `reg_alpha`:0 到 100.0 之间的实数，通常从对数均匀分布中采样，设置 L1 正则化。
*   `scale_pos_weight`:1e-6 到 500 之间的实数，最好从对数均匀分布中取样。该参数对正例(从而有效地进行上采样或下采样)和负例进行加权，负例的值保持为1。

尽管在使用 LightGBM 时需要优化的超参数数量可能令人望而生畏，但实际上只有其中的几个非常重要。给定固定的迭代次数和学习率，只有几个是最有影响力的(`feature_fraction`、`num_leaves`、`subsample`、`reg_lambda`、`reg_alpha`、`min_data_in_leaf`)，正如 Kaggle 大师*科黑小崎*在这篇博客文章中解释的那样:[https://medium . com/optuna/light GBM-tuner-new-optuna-integration-for-hyperparameter-optimization-8b 7095 e 99258](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258)。科黑·小崎利用这一事实为 Optuna 创建了一个快速调优程序(在本章末尾你会找到更多关于 Optuna 优化器的内容)。

### XGBoost

XGBoost([https://github.com/dmlc/XGBoost](https://github.com/dmlc/XGBoost))代表**极限梯度提升**。它是一个开源项目，不属于 Scikit-learn 的一部分，尽管它最近通过 Scikit-learn 包装器接口进行了扩展，使得将 XGBoost 合并到 Scikit-learn 风格的数据管道中变得更加容易。

XGBoost 算法在 2015 年的数据科学竞赛中获得了动力和人气，例如 Kaggle 和 2015 年 KDD 杯。正如创作者( *Tianqui Chen* 、 *Tong He* 和 *Carlos Guestrin* )在他们撰写的关于该算法的论文中所报告的那样，在 2015 年 Kaggle 上举行的 29 场挑战中，有 17 场获胜的解决方案将 XGBoost 作为一个独立的解决方案或作为多个不同模型的一部分。从那以后，该算法在数据科学家社区中一直保持着强大的吸引力，尽管它很难跟上 LightGBM 和 CatBoost 等其他 GBM 实现带来的创新。

除了在准确性和计算效率方面的良好性能，XGBoost 还是一个*可扩展的*解决方案，最多使用多核处理器和分布式机器。

由于对最初的树提升 GBM 算法进行了重要的调整，XGBoost 代表了新一代的 GBM 算法:

*   稀疏意识；它可以利用稀疏矩阵，节省内存(不需要密集矩阵)和计算时间(零值以特殊方式处理)。
*   近似树学习(加权分位数草图)，它产生类似的结果，但与可能的分支切割的经典完整探索相比，时间要少得多。
*   单台机器上的并行计算(在搜索最佳分割时使用多线程)以及类似的多台机器上的分布式计算。
*   在单台机器上进行核外计算，利用名为**列块**的数据存储解决方案。这个按列在磁盘上排列数据，因此通过以优化算法(对列向量起作用)期望的方式从磁盘中提取数据来节省时间。

XGBoost 还可以有效地处理缺失数据。基于标准决策树的其他树集成需要首先使用非标度值(例如负数)来估算缺失数据，以便开发树的适当分支来处理缺失值。

至于 XGBoost 的参数([https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html))，我们已经决定突出几个你会在竞赛和项目中发现的关键参数:

*   `n_estimators`:通常是 10 到 5000 之间的整数。
*   `learning_rate`:从 0.01 到 1.0 的实数，最好从对数均匀分布中采样。
*   `min_child_weight`:通常是 1 到 10 之间的整数。
*   `max_depth`:通常是 1 到 50 之间的整数。
*   `max_delta_step`:通常是一个介于 0 和 20 之间的整数，代表我们允许每个叶输出的最大增量步长。
*   `subsample`:从 0.1 到 1.0 的实数，表示要进行二次抽样的样本比例。
*   `colsample_bytree`:从 0.1 到 1.0 的实数，表示按树的列的子抽样比率。
*   `colsample_bylevel`:从 0.1 到 1.0 的实数，表示树中各层的子抽样比率。
*   `reg_lambda`:1e-9 到 100.0 之间的实数，最好从对数均匀分布中采样。该参数控制 L2 正则化。
*   `reg_alpha`:1e-9 到 100.0 之间的实数，最好从对数均匀分布中采样。该参数控制 L1 正则化。
*   `gamma`:指定树划分的最小损失减少，该参数需要一个介于 1e-9 和 0.5 之间的实数，最好从对数均匀分布中采样。
*   `scale_pos_weight`:1e-6 到 500.0 之间的一个实数，最好从对数均匀分布中采样，代表正类的权重。

与 LightGBM 一样，XGBoost 也有许多类似的超参数需要优化，因此之前为 LightGBM 考虑的所有因素也适用于 XGBoost。

### CatBoost

2017 年 7 月，俄罗斯搜索引擎 Yandex 公开了另一个有趣的 GBM 算法 CatBoost([https://catboost.ai/](https://catboost.ai/))，其名称来自于将“类别”和“助推”两个词放在一起事实上，它的优点是能够处理分类变量，这些变量构成了大多数关系数据库中的大部分信息，这是通过采用一次性编码和目标编码的混合策略实现的。目标编码是一种表达分类水平的方法，通过为手头的问题分配一个适当的数值；有关这方面的更多信息，请参见第 7 章、【表格竞赛的 建模。

CatBoost 使用编码分类变量的思想并不新鲜，但它是一种以前使用过的特征工程，大多在数据科学竞赛中使用。目标编码，也称为似然编码、影响编码或均值编码，是一种基于标签与目标变量的关联将标签转换为数字的简单方法。如果您有一个回归，您可以根据该级别的典型平均目标值转换标签；如果它是一个分类，它只是给定标签的你的目标的分类概率(你的目标的概率取决于每个类别值)。这可能看起来是一个简单而聪明的特征工程技巧，但它有副作用，主要是在过度拟合方面，因为你将来自目标的信息带入了预测器。

CatBoost 有相当少的参数(参见[https://catboost.ai/en/docs/references/training-parameters/](https://catboost.ai/en/docs/references/training-parameters/))。我们将讨论限制在八个最重要的方面:

*   `iterations`:通常是 10 到 1000 之间的整数，但可以根据问题增加。
*   `depth`:1 到 8 之间的整数；通常，较高的值需要较长的拟合时间，并且不会产生较好的结果。
*   `learning_rate`:介于 0.01 和 1.0 之间的真实值，最好从对数均匀分布中采样。
*   `random_strength`:从 1e-9 到 10.0 范围内对数线性采样的实数，它指定了得分分割的随机性水平。
*   `bagging_temperature`:设置贝叶斯自举的 0.0 到 1.0 之间的实数值。
*   `border_count`:1 到 255 之间的整数，表示数字特征的拆分。
*   `l2_leaf_reg`:2 到 30 之间的整数；L2 正则化的值。
*   `scale_pos_weight`:一个介于 0.01 和 10.0 之间的实数，代表正类的权重。

即使 CatBoost 可能看起来只是另一个 GBM 实现，但它有相当多的差异(使用的不同参数也突出了这一点)，这可能会在竞赛中提供很大的帮助，无论是作为单一模型解决方案还是作为集成到更大集合中的模型。

### histgradientsboosting

最近，Scikit-learn 推出了一个受 LightGBM 的分箱数据和直方图启发的新版本的梯度增强(参见在 EuroPython 上由*Olivier Grisel*:[https://www.youtube.com/watch?v=urVUlKbQfQ4](https://www.youtube.com/watch?v=urVUlKbQfQ4)所做的这个演示)。无论是作为分类器(`HistGradientBoostingClassifier`)还是回归器(`HistGradientBoostingRegressor`)，它都可以用于丰富不同模型的集合，并且它提供了一个更短的需要调整的基本超参数范围:

*   `learning_rate`:0.01 到 1.0 之间的实数，通常从对数均匀分布中采样。
*   `max_iter`:10 到 10000 之间的整数。
*   `max_leaf_nodes`:2 到 500 之间的整数。它与`max_depth`交互；建议只设置两个中的一个，将另一个设置为`None`。
*   `max_depth`:介于 2 和 12 之间的整数。
*   `min_samples_leaf`:2 到 300 之间的整数。
*   `l2_regularization`:0.0 到 100.0 之间的浮动。
*   `max_bins`:32 到 512 之间的整数。

即使 Scikit-learn 的`HistGradientBoosting`与 LightGBM 或 XGBoost 没有什么太大的不同，但它确实提供了一种在竞赛中实现 GBM 的不同方式，并且`HistGradientBoosting`构建的模型可能会在集成多个预测时做出贡献，例如在混合和堆叠中。

学完本节的后，你应该更熟悉最常见的机器学习算法(只有深度学习解决方案没有讨论)和它们最重要的要调整的超参数，这将帮助你在 Kaggle 竞赛中建立一个出色的解决方案。了解基本的优化策略、可用的算法及其关键的超参数只是一个起点。在下一节中，我们将开始深入讨论如何使用贝叶斯优化更好地调优它们。

![](img/Alberto_Danese.png)

阿尔贝托·达内塞

[https://www.kaggle.com/albedan](https://www.kaggle.com/albedan)

我们对本章的第二次采访是意大利信用卡和数字支付公司 Nexi 的数据科学主管 Alberto Danese。作为 2015 年加入该平台的比赛大师，他作为一名单独的竞赛对手获得了大部分金牌。

你最喜欢哪种比赛，为什么？从技术和解决途径来说，你在 Kaggle 上的特长是什么？

*我一直在金融服务行业工作，主要处理结构化数据，我确实更喜欢属于这一类别的比赛。我喜欢能够实际掌握数据是关于什么的，并做一些智能特征工程，以便从数据中挤出每一点信息。*

从技术上讲，我对经典的 ML 库有很好的经验，尤其是梯度推进决策树:最常见的库(XGBoost，LightGBM，CatBoost)总是我的首选。

你是如何对待一场 Kaggle 比赛的？这种方法与你在日常工作中的做法有什么不同？

*我总是花费大量时间探索数据，试图找出发起人实际上试图用机器学习解决的问题是什么。与新手通常对 Kaggle 的看法不同，我没有花太多时间对特定的 ML 算法进行“调整”——显然这种方法取得了成效！*

*在我的日常工作中，理解数据也是极其重要的，但在 Kaggle 竞赛中还有一些额外的阶段是完全缺失的。我得:*

*   *确定一个需要 ML 解决的业务问题(与业务部门的同事一起)*
*   *寻找数据，有时也从外部数据提供商那里寻找*
*   *当 ML 部分完成后，了解如何将其投入生产并管理进展*

告诉我们你参加的一个特别有挑战性的比赛，以及你用什么样的洞察力来完成这个任务。

*我很享受**talking data AdTracking 欺诈检测挑战赛*，凭借它我成为了一代宗师。除了是一个非常有趣的话题(打击来自点击农场的欺诈)，它真的促使我进行有效的特征工程，因为卷是巨大的(超过 1 亿个标记行),为了测试不同的方法，减少计算时间是关键。这也迫使我理解如何以最佳方式利用滞后/超前特征(以及其他窗口函数)，以便在一个经典的 ML 问题中创建一种时间序列。**

 *Kaggle 对你的职业生涯有帮助吗？如果有，如何实现？

*肯定！能够取得非常客观和可证实的结果无疑是让一份简历脱颖而出的原因。当我在 2016 年被 Cerved(一家营销情报服务公司)聘用时，招聘经理非常清楚 Kaggle 是什么——在面试中有一些真实世界的项目可以谈论是非常有价值的事情。毫无疑问，卡格尔在我职业生涯的发展中扮演了重要角色。*

以你的经验来看，没有经验的 Kagglers 经常会忽略什么？你现在知道了什么，你希望在你刚开始的时候就知道？

*我认为每个人都是刚刚开始编码，也许是分叉一个公共内核，只是改变几行或几个参数。这在开始时是非常好的！但是你必须花相当多的时间不是编码，而是研究数据和理解问题。*

你在过去的比赛中犯过什么错误？

不知道这算不算一个错误，但我经常喜欢独自竞赛:一方面，这很好，因为它迫使你处理比赛的每一个方面，你可以按照自己的意愿管理时间。但是我真的很喜欢在一些比赛中与队友合作:我可能应该考虑更经常地合作，因为你可以从合作中学到很多。

对于数据分析或机器学习，你有什么特别推荐的工具或库吗？

*除了* *平常的那些，我一直是* `data.table` *(从 R 版开始)的超级粉丝:我觉得它没有得到应有的赞誉！当您想要在本地机器上处理大量数据时，这确实是一个非常棒的软件包。*

当一个人参加比赛时，他应该记住或做的最重要的事情是什么？

*先了解问题和数据:不要马上开始编码！**  *# 贝叶斯优化

抛开网格搜索(仅在实验空间有限时可行)，从业者通常的选择是应用随机搜索优化或尝试**贝叶斯优化** ( **博**)技术，这需要更复杂的设置。

最初由 Snoek，j .、Larochelle，h .和 Adams，r . p .([http://export.arxiv.org/pdf/1206.2944](http://export.arxiv.org/pdf/1206.2944))在论文*中介绍了机器学习算法*的实用贝叶斯优化，贝叶斯优化背后的关键思想是我们优化一个**代理函数**(也称为**代理函数**)，而不是真正的目标函数(网格搜索和随机搜索都是这样)。如果没有梯度，如果测试真正的目标函数是昂贵的(如果不是，那么我们简单地去随机搜索)，并且如果搜索空间是嘈杂的和足够复杂的，我们这样做。

贝叶斯搜索平衡*探索*与*开发*。在开始时，它随机地探索，从而在探索过程中训练代理函数。基于该替代函数，搜索利用其关于预测器如何工作的初始近似知识，以便采样更多有用的示例并最小化成本函数。正如名字的*贝叶斯*部分所暗示的，我们正在使用先验知识，以便在优化过程中对采样做出更明智的决策。这样，通过限制我们需要进行的评估次数，我们可以更快地达到最小化。

贝叶斯优化使用和**获取函数**来告诉我们一个观察结果有多有希望。事实上，为了管理探索和开发之间的权衡，该算法定义了一个获取函数，该函数提供了对尝试任何给定点的有用程度的单一度量。

通常，贝叶斯优化是由高斯过程驱动的。当搜索空间具有平滑且可预测的响应时，高斯过程表现得更好。当搜索空间更复杂时，另一种选择是使用树算法(例如，随机森林)，或者一种完全不同的方法，称为树 Parzen 估计器(树 Parzen Estimators)或树结构 Parzen 估计器(树结构 Parzen Estimators)。

TPEs 不是直接建立一个模型来估计一组参数的成功，从而像一个先知一样，而是根据实验提供的逐次逼近法来估计定义参数最佳值的多变量分布的参数。以这种方式，TPE 通过从概率分布中对参数进行采样来导出最佳参数集，而不是像高斯过程那样直接从机器学习模型中导出。

我们将讨论这些方法中的每一种，首先通过检查 Scikit-optimize 和 KerasTuner，两者都基于高斯过程(Scikit-optimize 也可以使用随机森林，KerasTuner 可以使用多臂土匪)，然后是 Optuna，它主要基于 TPE(尽管它也提供不同的策略:[https://Optuna . readthedocs . io/en/stable/reference/samplers . html](https://optuna.readthedocs.io/en/stable/reference/samplers.html))。

虽然贝叶斯优化被认为是超参数调整的最先进技术，但请始终记住，对于更复杂的参数空间，使用贝叶斯优化在时间和计算开销方面并不比通过随机搜索找到的解决方案更有优势。例如，在谷歌云机器学习引擎服务中，贝叶斯优化的使用限于涉及最多 16 个参数的问题。对于大量的参数，它诉诸随机抽样。

## 使用 Scikit-optimize

Scikit-optimize ( `skopt`)已经使用与 Scikit-learn 相同的 API 进行了开发，并且大量使用了 NumPy 和 SciPy 函数。此外，它是由 Scikit-learn 项目的一些贡献者创建的，例如 *Gilles Louppe* 。

基于高斯过程算法，该包得到了很好的维护，尽管有时它必须赶上，因为 Scikit-learn、NumPy 或 SciPy 方面有所改进。例如，在撰写本文时，为了在 Kaggle 笔记本上正确运行它，您必须回滚到这些包的旧版本，正如 GitHub 问题中所解释的那样([https://GitHub . com/scikit-optimize/scikit-optimize/issues/981](https://github.com/scikit-optimize/scikit-optimize/issues/981))。

该软件包有一个直观的 API，很容易破解它，并在自定义优化策略中使用它的功能。Scikit-optimize 还因其有用的图形表示而闻名。事实上，通过可视化优化过程的结果(使用 Scikit-optimize 的`plot_objective`函数)，您可以确定是否可以重新定义问题的搜索空间，并对优化如何解决问题做出解释。

在我们的工作示例中，我们将参考可在以下 Kaggle 笔记本中找到的工作:

*   [https://www . ka ggle . com/lucamassaron/tutorial-Bayesian-optimization-with-light GBM](https://www.kaggle.com/lucamassaron/tutorial-bayesian-optimization-with-lightgbm)
*   [https://www . ka ggle . com/lucamassaron/sci kit-optimize-for-light GBM](https://www.kaggle.com/lucamassaron/scikit-optimize-for-lightgbm)

我们在这里的目的是向您展示如何快速处理竞赛中的优化问题，例如 *30 天的 ML* ，这是最近的一次竞赛，许多 Kagglers 参与了学习新技能并在持续 30 天的竞赛中应用它们。这个竞赛的目标是预测一个保险索赔的价值，所以它是一个回归问题。您可以通过访问[https://www.kaggle.com/thirty-days-of-ml](https://www.kaggle.com/thirty-days-of-ml)了解更多关于该计划的信息，并下载我们将要展示的例子所需的数据(公众随时可以获得相关资料)。

如果因为之前没有参加过比赛而无法访问数据，可以使用这个 Kaggle 数据集:[https://www.kaggle.com/lucamassaron/30-days-of-ml](https://www.kaggle.com/lucamassaron/30-days-of-ml)。

下面的代码将介绍如何为这个问题加载数据，然后建立一个贝叶斯优化过程，这将提高 LightGBM 模型的性能。

我们从加载包开始:

```
# Importing core libraries

import numpy as np

import pandas as pd

from time import time

import pprint

import joblib

from functools import partial

# Suppressing warnings because of skopt verbosity

import warnings

warnings.filterwarnings("ignore")

# Classifiers

import lightgbm as lgb

# Model selection

from sklearn.model_selection import KFold

# Metrics

from sklearn.metrics import mean_squared_error

from sklearn.metrics import make_scorer

# Skopt functions

from skopt import BayesSearchCV

from skopt.callbacks import DeadlineStopper, DeltaYStopper

from skopt.space import Real, Categorical, Integer 
```

下一步，我们加载数据。数据不需要太多的处理，除了将一些以字母为级别的分类特征转换为有序的数字特征:

```
# Loading data 

X = pd.read_csv("../input/30-days-of-ml/train.csv")

X_test = pd.read_csv("../input/30-days-of-ml/test.csv")

# Preparing data as a tabular matrix

y = X.target

X = X.set_index('id').drop('target', axis='columns')

X_test = X_test.set_index('id')

# Dealing with categorical data

categoricals = [item for item in X.columns if 'cat' in item]

cat_values = np.unique(X[categoricals].values)

cat_dict = dict(zip(cat_values, range(len(cat_values))))

X[categoricals] = X[categoricals].replace(cat_dict).astype('category')

X_test[categoricals] = X_test[categoricals].replace(cat_dict).astype('category') 
```

在使数据可用之后，我们定义了一个报告函数，Scikit-optimize 可以使用它来完成各种优化任务。该函数将数据和优化器作为输入。它还可以处理**回调函数**，这些函数执行各种操作，例如报告、根据搜索时间是否达到某个阈值或性能是否改善(例如，在一定次数的迭代中没有看到改善)提前停止，或在每次优化迭代后保存处理状态:

```
# Reporting util for different optimizers

def report_perf(optimizer, X, y, title="model", callbacks=None):

    """

    A wrapper for measuring time and performance of optimizers

    optimizer = a sklearn or a skopt optimizer

    X = the training set 

    y = our target

    title = a string label for the experiment

    """

    start = time()

    if callbacks is not None:

        optimizer.fit(X, y, callback=callbacks)

    else:

        optimizer.fit(X, y)

    d=pd.DataFrame(optimizer.cv_results_)

    best_score = optimizer.best_score_

    best_score_std = d.iloc[optimizer.best_index_].std_test_score

    best_params = optimizer.best_params_

    print((title + " took %.2f seconds, candidates checked: %d, best CV            score: %.3f" + u" \u00B1"+" %.3f") % 

                             (time() - start,

                             len(optimizer.cv_results_['params']),

                             best_score, 

                             best_score_std))

    print('Best parameters:')

    pprint.pprint(best_params)

    print()

    return best_params 
```

我们现在必须准备评分函数(评估基于此)、验证策略(基于交叉验证)、模型和搜索空间。对于评分函数，它应该是一个均方根误差度量，我们参考 Scikit-learn 中的实践，在那里你总是最小化一个函数(如果你必须最大化，你最小化它的负值)。

`make_scorer`包装器可以很容易地复制这样的实践:

```
# Setting the scoring function

scoring = make_scorer(partial(mean_squared_error, squared=False),

                      greater_is_better=False)

# Setting the validation strategy

kf = KFold(n_splits=5, shuffle=True, random_state=0)

# Setting the basic regressor

reg = lgb.LGBMRegressor(boosting_type='gbdt',

                        metric='rmse',

                        objective='regression',

                        n_jobs=1, 

                        verbose=-1,

                        random_state=0) 
```

设置搜索空间需要使用 Scikit-optimize 中的不同函数，如`Real`、`Integer`或`Choice`，每一个函数都从您定义为参数的不同类型的分布中采样(通常是均匀分布，但当您对参数的比例效应比对其精确值更感兴趣时，也可以使用对数均匀分布):

```
# Setting the search space

search_spaces = {

     # Boosting learning rate

    'learning_rate': Real(0.01, 1.0, 'log-uniform'),

     # Number of boosted trees to fit

    'n_estimators': Integer(30, 5000),

     # Maximum tree leaves for base learners

    'num_leaves': Integer(2, 512),

     # Maximum tree depth for base learners

    'max_depth': Integer(-1, 256),

     # Minimal number of data in one leaf

    'min_child_samples': Integer(1, 256),

     # Max number of bins buckets

    'max_bin': Integer(100, 1000),

     # Subsample ratio of the training instance 

    'subsample': Real(0.01, 1.0, 'uniform'),

     # Frequency of subsample 

    'subsample_freq': Integer(0, 10),

     # Subsample ratio of columns

    'colsample_bytree': Real(0.01, 1.0, 'uniform'), 

     # Minimum sum of instance weight

    'min_child_weight': Real(0.01, 10.0, 'uniform'),

     # L2 regularization

    'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),

     # L1 regularization

    'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),

   } 
```

一旦定义了:

*   你的交叉验证策略
*   您的评估标准
*   你的基本模型
*   你的超参数搜索空间

剩下的只是把它们输入到你的优化函数中，`BayesSearchCV`。基于所提供的 CV 方案，该函数将基于搜索空间内的值寻找得分函数的最小值。您可以设置执行的最大迭代次数、代理函数的种类(高斯过程(`GP`)在大多数情况下有效)以及再现性的随机种子:

```
# Wrapping everything up into the Bayesian optimizer

opt = BayesSearchCV(estimator=reg,

                    search_spaces=search_spaces,

                    scoring=scoring,

                    cv=kf,

                    n_iter=60,           # max number of trials

                    n_jobs=-1,           # number of jobs

                    iid=False,         

                    # if not iid it optimizes on the cv score

                    return_train_score=False,

                    refit=False,  

                    # Gaussian Processes (GP) 

                    optimizer_kwargs={'base_estimator': 'GP'},

                    # random state for replicability

                    random_state=0) 
```

此时，您可以使用我们之前定义的报告功能开始搜索。过一会儿，该函数将返回问题的最佳参数。

```
# Running the optimizer

overdone_control = DeltaYStopper(delta=0.0001)

# We stop if the gain of the optimization becomes too small

time_limit_control = DeadlineStopper(total_time=60 * 60 * 6)

# We impose a time limit (6 hours)

best_params = report_perf(opt, X, y,'LightGBM_regression', 

                          callbacks=[overdone_control, time_limit_control]) 
```

在示例中，我们通过指定停止和报告最佳结果之前允许的最大时间(6 小时)来设置操作限制。由于贝叶斯优化方法将超参数的不同组合的探索和利用混合在一起，因此在任何时候停止都将总是返回到目前为止找到的最佳解决方案(但不一定是可能的最佳方案)。这是因为获取函数将总是基于由替代函数返回的估计性能及其不确定性区间，优先探索搜索空间中最有希望的部分。

## 定制贝叶斯优化搜索

Scikit-optimize 提供的`BayesSearchCV`函数当然很方便，因为它自己包装和排列了超参数搜索的所有元素，但它也有局限性。例如，你可能会发现在比赛中:

*   对每次搜索迭代有更多的控制，例如混合随机搜索和贝叶斯搜索
*   能够在算法上应用早期停止
*   更多地定制您的验证策略
*   停止早期不起作用的实验(例如，当单个交叉验证折叠可用时，立即评估其性能，而不是等到最后对所有折叠进行平均)
*   创建以相似方式执行的超参数集的聚类(例如，为了创建仅在所用超参数上不同的多个模型，用于混合集合)

如果您可以修改`BayesSearchCV`内部程序，这些任务都不会太复杂。幸运的是，Scikit-optimize 让您可以做到这一点。事实上，在`BayesSearchCV`之后，以及在这个包的其他包装器之后，有一些特定的最小化函数，您可以将它们用作您自己的搜索函数的独立部分:

*   `gp_minimize`:使用高斯过程的贝叶斯优化
*   `forest_minimize`:使用随机森林或极度随机树的贝叶斯优化
*   `gbrt_minimize`:使用梯度推进的贝叶斯优化
*   `dummy_minimize`:只是随机搜索

在下面的示例中，我们将使用自己的自定义搜索功能来修改之前的搜索。新的自定义函数将接受训练期间的提前停止，并且如果折叠验证结果之一不是最佳结果，它将修剪实验。

你可以在[https://www . Kaggle . com/lucamassaron/hacking-Bayesian-optimization](https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization)找到在 ka ggle 笔记本中工作的下一个例子。

和前面的例子一样，我们从导入必要的包开始。

```
# Importing core libraries

import numpy as np

import pandas as pd

from time import time

import pprint

import joblib

from functools import partial

# Suppressing warnings because of skopt verbosity

import warnings

warnings.filterwarnings("ignore")

# Classifier/Regressor

from xgboost import XGBRegressor

# Model selection

from sklearn.model_selection import KFold, StratifiedKFold

from sklearn.model_selection import cross_val_score

from sklearn.model_selection import train_test_split

# Metrics

from sklearn.metrics import mean_squared_error

from sklearn.metrics import make_scorer

# Skopt functions

from skopt import BayesSearchCV

from skopt.callbacks import DeadlineStopper, DeltaYStopper

from skopt.space import Real, Categorical, Integer

from skopt import gp_minimize, forest_minimize

from skopt import gbrt_minimize, dummy_minimize

# Decorator to convert a list of parameters to named arguments

from skopt.utils import use_named_args 

# Data processing

from sklearn.preprocessing import OrdinalEncoder 
```

与之前一样，我们上传了 ML*30 天*比赛的数据:

```
# Loading data 

X_train = pd.read_csv("../input/30-days-of-ml/train.csv")

X_test = pd.read_csv("../input/30-days-of-ml/test.csv")

# Preparing data as a tabular matrix

y_train = X_train.target

X_train = X_train.set_index('id').drop('target', axis='columns')

X_test = X_test.set_index('id')

# Pointing out categorical features

categoricals = [item for item in X_train.columns if 'cat' in item]

# Dealing with categorical data using OrdinalEncoder

ordinal_encoder = OrdinalEncoder()

X_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])

X_test[categoricals] = ordinal_encoder.transform(X_test[categoricals]) 
```

现在我们设置一个超参数搜索的所有必要元素，即评分函数、验证策略、搜索空间和待优化的机器学习模型。评分函数和确认策略将在以后成为构成目标函数的核心元素，贝叶斯优化将努力最小化该函数。

```
# Setting the scoring function

scoring = partial(mean_squared_error, squared=False)

# Setting the cv strategy

kf = KFold(n_splits=5, shuffle=True, random_state=0)

# Setting the search space

space = [Real(0.01, 1.0, 'uniform', name='learning_rate'),

         Integer(1, 8, name='max_depth'),

         Real(0.1, 1.0, 'uniform', name='subsample'),

         # Subsample ratio of columns by tree

         Real(0.1, 1.0, 'uniform', name='colsample_bytree'),  

         # L2 regularization

         Real(0, 100., 'uniform', name='reg_lambda'),

         # L1 regularization

         Real(0, 100., 'uniform', name='reg_alpha'),

         # minimum sum of instance weight (hessian)  

         Real(1, 30, 'uniform', name='min_child_weight')

         ]

model = XGBRegressor(n_estimators=10_000, 

                     booster='gbtree', random_state=0) 
```

请注意，这次我们没有在搜索空间中包括估计量的数量(参数`n_estimators`)。相反，我们在实例化模型时设置它，并输入一个高值，因为我们希望根据验证集尽早停止模型。

作为下一步，您现在需要创建目标函数。目标函数应该只接受要优化的参数作为输入，并返回结果分数。然而，目标函数也需要接受您刚才准备的搜索所必需的元素。自然，你可以在函数内部引用它们。但是，将它们放入函数本身的内部内存空间是一个很好的做法。这有它的好处；例如，您将使元素不可变，并且它们将与目标函数一起运行(通过 pickling 或者如果您在多处理器级别上分配搜索任务)。您可以通过创建一个接受元素的`make`函数来获得第二个结果，修改后的目标函数由`make`函数返回。有了这个简单的结构，您的目标函数将包含所有的元素，比如数据和模型，并且您只需要传递要测试的参数。

让我们开始编写函数代码。我们将沿途停下来讨论一些相关的方面:

```
# The objective function to be minimized

def make_objective(model, X, y, space, cv, scoring, validation=0.2):

    # This decorator converts your objective function 

    # with named arguments into one that accepts a list as argument,

    # while doing the conversion automatically.

    @use_named_args(space) 

    def objective(**params):

        model.set_params(**params)

        print("\nTesting: ", params)

        validation_scores = list()

        for k, (train_index, test_index) in enumerate(kf.split(X, y)):

            val_index = list()

            train_examples = int(train_examples * (1 - validation))

            train_index, val_index = (train_index[:train_examples], 

                                      train_index[train_examples:])

            start_time = time()

            model.fit(X.iloc[train_index,:], y[train_index],

                      early_stopping_rounds=50,

                      eval_set=[(X.iloc[val_index,:], y[val_index])], 

                      verbose=0

                    )

            end_time = time()

            rounds = model.best_iteration

            test_preds = model.predict(X.iloc[test_index,:])

            test_score = scoring(y[test_index], test_preds)

            print(f"CV Fold {k+1} rmse:{test_score:0.5f}-{rounds} 

                  rounds - it took {end_time-start_time:0.0f} secs")

            validation_scores.append(test_score) 
```

在函数的第一部分，您只需创建一个目标函数，进行交叉验证，并使用提前停止来拟合数据。我们已经使用了积极的早期停止策略来节省时间，但是如果您认为这可能对您的问题更有效，您可以增加查房的次数。请注意，验证示例是从训练文件夹中的示例中顺序取出的(参见`train_index`和`val_index`在代码中是如何定义的)，而非文件夹中的示例(`test_index`源自`kf`交叉验证拆分)在最终验证中保持不变。如果您不想对用于提前停止的数据进行自适应过度拟合，这一点很重要。

在下一部分中，在继续进行交叉验证循环并继续进行要训练和测试的剩余交叉验证折叠之前，您将分析折叠在折叠外集合上获得的结果:

```
 if len(history[k]) >= 10:

                threshold = np.percentile(history[k], q=25)

                if test_score > threshold:

                    print(f"Early stopping for under-performing fold: 

                          threshold is {threshold:0.5f}")

                    return np.mean(validation_scores)

            history[k].append(test_score)

        return np.mean(validation_scores)

    return objective 
```

请注意，我们保存了一个全局字典`history`，其中包含了到目前为止从每个文件夹中获得的结果。我们可以跨多个实验和交叉验证来比较结果；由于随机种子，交叉验证是可重复的，因此相同折叠的结果是完全可比较的。如果当前折叠的结果低于之前在其他迭代中获得的折叠(使用底部四分位数作为参考)，则停止并返回到目前为止测试的折叠的平均值。这样做的理由是，如果一个折叠没有给出可接受的结果，那么整个交叉验证可能也不会。因此，您可以退出并继续使用另一组更有前途的参数。这是一种提前停止交叉验证的方法，可以加快你的搜索速度，让你在更短的时间内完成更多的实验。

接下来，使用我们的`make_objective`函数，我们将所有元素(模型、数据、搜索空间、验证策略和评分函数)放在一个函数中，即目标函数。作为的结果，我们现在有了一个函数，它只接受要优化的参数并返回一个分数，优化的最小化引擎将根据这个分数决定下一个实验:

```
objective = make_objective(model,

                           X_train, y_train,

                           space=space,

                           cv=kf,

                           scoring=scoring) 
```

由于我们希望控制优化的每一步并保存它以备后用，我们还准备了一个回调函数，它将在最小化过程的每次迭代中保存所执行的实验及其结果的列表。简单地通过使用这两条信息，最小化引擎可以在任何时候停止，并且之后可以从检查点恢复优化:

```
def onstep(res):

    global counter

    x0 = res.x_iters   # List of input points

    y0 = res.func_vals # Evaluation of input points

    print('Last eval: ', x0[-1], 

          ' - Score ', y0[-1])

    print('Current iter: ', counter, 

          ' - Best Score ', res.fun, 

          ' - Best Args: ', res.x)

    # Saving a checkpoint to disk

    joblib.dump((x0, y0), 'checkpoint.pkl') 

    counter += 1 
```

至此，我们准备开始了。贝叶斯优化需要一些起点才能正常工作。我们用随机搜索(使用`dummy_minimize`函数)创建了许多实验，并保存了它们的结果:

```
counter = 0

history = {i:list() for i in range(5)}

used_time = 0

gp_round = dummy_minimize(func=objective,

                          dimensions=space,

                          n_calls=30,

                          callback=[onstep],

                          random_state=0) 
```

然后，我们可以检索保存的实验，并打印贝叶斯优化已经测试过的超参数集序列，以及它们的结果。事实上，我们可以在`x0`和`y0`列表中找到参数集及其结果:

```
x0, y0 = joblib.load('checkpoint.pkl')

print(len(x0)) 
```

此时，我们甚至可以通过对搜索空间、获取函数、调用次数或回调进行一些更改来恢复贝叶斯优化:

```
x0, y0 = joblib.load('checkpoint.pkl')

gp_round = gp_minimize(func=objective,

                       x0=x0,    # already examined values for x

                       y0=y0,    # observed values for x0

                       dimensions=space,

                       acq_func='gp_hedge',

                       n_calls=30,

                       n_initial_points=0,

                       callback=[onstep],

                       random_state=0) 
```

一旦我们确定不需要继续调用优化函数，我们就可以打印获得的最佳分数(基于我们的输入和验证方案)和最佳超参数集:

```
x0, y0 = joblib.load('checkpoint.pkl')

print(f"Best score: {gp_round.fun:0.5f}")

print("Best hyperparameters:")

for sp, x in zip(gp_round.space, gp_round.x):

    print(f"{sp.name:25} : {x}") 
```

基于最佳结果，我们可以重新训练我们的模型以用于比赛。

现在我们有了组参数及其结果(`x0`和`y0`列表)，我们还可以探索不同的结果，并将输出相似但使用的参数组不同的结果聚集在一起。这将有助于我们训练一组具有相似性能但不同优化策略的更多样化的模型。这是**混合**的理想情况，这是多个模型的平均，以便降低估计值的方差，并获得更好的公共和私人排行榜分数。

参考*第 9 章*、*用混合和堆叠溶液组装*，了解混合的讨论。

## 将贝叶斯优化扩展到神经结构搜索

继续深入学习，神经网络似乎也有相当多的超参数需要修正:

*   批量
*   学习率
*   优化器的种类及其内部参数

所有这些参数都会影响网络的学习方式，它们会产生很大的影响；批量大小或学习速率的微小差异就可以决定网络是否可以将误差降低到某个阈值之外。

也就是说，当使用**深度神经网络** ( **DNNs** )工作时，这些学习参数并不是唯一可以优化的参数。网络如何分层组织以及其架构的细节会产生更大的差异。

事实上，从技术上来说，**架构**意味着深度神经网络的表示能力，这意味着，根据您使用的层，网络要么能够读取和处理数据中所有可用的信息，要么不能。虽然在其他机器学习算法中，你有大量但有限的选择，但在 DNNs 中，你的选择似乎是无限的，因为唯一明显的限制是你在处理部分神经网络并将它们放在一起的知识和经验。

当组装性能良好的 dnn 时，伟大的深度学习实践者的常见最佳实践主要取决于:

*   依靠预先训练好的模型(所以你必须非常了解可用的解决方案，比如在拥抱脸([https://huggingface.co/models](https://huggingface.co/models))或 GitHub 上找到的那些)
*   阅读前沿论文
*   复制同一个竞赛或以前的顶级 Kaggle 笔记本
*   反复试验
*   独创性和运气

在 Geoffrey Hinton 教授的一堂著名的课中，他说你可以使用自动方法，比如贝叶斯优化，来达到相似的甚至更好的结果。贝叶斯优化也将避免你陷入困境，因为你不能在许多可能的超参数中找出最佳组合。

关于杰弗里·辛顿教授的课程录音，见 https://www.youtube.com/watch?v=i0cKa0di_lo。

幻灯片见[https://www . cs . Toronto . edu/~ hint on/coursera/lech 16/LEC 16 . pdf](https://www.cs.toronto.edu/~hinton/coursera/lecture16/lec16.pdf)。

正如我们之前提到的，即使在最复杂的 AutoML 系统中，当您有太多的超参数时，依靠随机优化可能会产生更好的结果，或者在与贝叶斯优化相同的时间内产生相同的结果。另外，在这种情况下，你还得对抗一个有急转弯和曲面的优化景观；在 DNN 优化中，许多参数不是连续的，而是布尔型的，仅仅一个变化就可能出乎意料地使网络性能变好或变坏。

我们的经验告诉我们，随机优化可能不适合 Kaggle 竞赛，因为:

*   你的时间和资源有限
*   您可以利用之前的优化结果来找到更好的解决方案

在这种情况下，贝叶斯优化是理想的:您可以根据您拥有的时间和计算资源来设置它，并分阶段进行，通过多个会话来完善您的设置。此外，您不太可能轻松地利用并行性来调优 dnn，因为它们使用 GPU，除非您手头有多台非常强大的机器。通过顺序工作，贝叶斯优化只需要一台好机器来执行任务。最后，即使很难通过搜索找到最佳架构，由于优化环境，您可以利用以前实验的信息，尤其是在开始时，完全避免无效的参数组合。使用随机优化，除非你改变搜索空间，所有的组合总是容易被测试。

然而，也有缺点。贝叶斯优化使用从以前的试验中构建的代理函数对超参数空间建模，这不是一个无错误的过程。这个过程很有可能最终只关注搜索空间的一部分，而忽略了其他部分(这些部分可能包含您正在寻找的最小值)。解决这个问题的方法是运行大量实验以确保安全，或者在随机搜索和贝叶斯优化之间交替进行，用随机试验挑战贝叶斯模型，这些随机试验可以迫使它以更优化的方式重塑其搜索模型。

对于我们的例子，我们再次使用来自 Kaggle 的 ML 计划的 *30 天的数据，这是一个回归任务。我们的例子基于 TensorFlow，但稍加修改，它可以在 PyTorch 或 MXNet 等其他深度学习框架上运行。*

和之前一样，你可以在这里找到 Kaggle 上的例子:[https://www . ka ggle . com/lucamassaron/hacking-Bayesian-optimization-for-dnns](https://www.kaggle.com/lucamassaron/hacking-bayesian-optimization-for-dnns)。

让我们开始吧:

```
import tensorflow as tf 
```

在导入 TensorFlow 包之后，我们利用它的`Dataset`函数来创建一个能够向我们的神经网络提供批量数据的 iterable:

```
def df_to_dataset(dataframe, shuffle=True, batch_size=32):

    dataframe = dataframe.copy()

    labels = dataframe.pop('target')

    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),   

                                             labels))

    if shuffle:

        ds = ds.shuffle(buffer_size=len(dataframe))

    ds = ds.batch(batch_size)

    return ds

tf.keras.utils.get_custom_objects().update({'leaky-relu': tf.keras.layers.Activation(tf.keras.layers.LeakyReLU(alpha=0.2))}) 
```

我们还为我们的模型定制了一个【leaky ReLU activation 对象；可以用字符串调用，不需要直接使用函数。

我们继续编写一个函数，该函数基于一组超参数创建我们的深度神经网络模型:

```
def create_model(cat0_dim, cat1_dim, cat2_dim,

                 cat3_dim, cat4_dim, cat5_dim, 

                 cat6_dim, cat7_dim, cat8_dim, cat9_dim,

                 layers, layer_1, layer_2, layer_3, layer_4, layer_5, 

                 activation, dropout, batch_normalization, learning_rate, 

                 **others):

    dims = {'cat0': cat0_dim, 'cat1': cat1_dim, 'cat2': cat2_dim, 

            'cat3': cat3_dim, 'cat4': cat4_dim, 'cat5': cat5_dim,

            'cat6': cat6_dim, 'cat7': cat7_dim, 'cat8': cat8_dim, 

            'cat9': cat9_dim}

    vocab = {h:X_train['cat4'].unique().astype(int) 

             for h in ['cat0', 'cat1', 'cat2', 'cat3', 

                       'cat4', 'cat5', 'cat6', 'cat7', 

                       'cat8', 'cat9']}

    layers = [layer_1, layer_2, layer_3, layer_4, layer_5][:layers]

    feature_columns = list()

    for header in ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 

                   'cont6','cont7', 'cont8', 'cont9', 'cont10',

                   'cont11', 'cont12', 'cont13']:

        feature_columns.append(tf.feature_column.numeric_column(header))

    for header in ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 

                   'cat6', 'cat7', 'cat8', 'cat9']:

        feature_columns.append(

            tf.feature_column.embedding_column(

            tf.feature_column.categorical_column_with_vocabulary_list(

            header, vocabulary_list=vocab[header]),  

            dimension=dims[header]))

    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

    network_struct = [feature_layer]

    for nodes in layers:

        network_struct.append(

                 tf.keras.layers.Dense(nodes, activation=activation))

        if batch_normalization is True:

                   network_struct.append(

                   tf.keras.layers.BatchNormalization())

        if dropout > 0:

            network_struct.append(tf.keras.layers.Dropout(dropout))

    model = tf.keras.Sequential(network_struct + 

                                [tf.keras.layers.Dense(1)])

    model.compile(optimizer=tf.keras.optimizers.Adam(

                          learning_rate=learning_rate),

                  loss= tf.keras.losses.MeanSquaredError(),

                  metrics=['mean_squared_error'])

    return model 
```

在内部，`create_model`函数中的代码根据提供的输入定制神经网络架构。例如，作为函数的参数，您可以提供每个分类变量的嵌入维数，或者定义网络中存在的密集层的结构和数量。所有这些参数都与您希望通过贝叶斯优化探索的参数空间相关，因此创建模型的函数的每个输入参数都应该与搜索空间中定义的**采样函数**相关。你所要做的就是将采样函数放在一个列表中，按照`create_model`函数所期望的顺序:

```
# Setting the search space

space = [Integer(1, 2, name='cat0_dim'),

         Integer(1, 2, name='cat1_dim'),

         Integer(1, 2, name='cat2_dim'),

         Integer(1, 3, name='cat3_dim'),

         Integer(1, 3, name='cat4_dim'),

         Integer(1, 3, name='cat5_dim'),

         Integer(1, 4, name='cat6_dim'),

         Integer(1, 4, name='cat7_dim'),

         Integer(1, 6, name='cat8_dim'),

         Integer(1, 8, name='cat9_dim'),

         Integer(1, 5, name='layers'),

         Integer(2, 256, name='layer_1'),

         Integer(2, 256, name='layer_2'),

         Integer(2, 256, name='layer_3'),

         Integer(2, 256, name='layer_4'),

         Integer(2, 256, name='layer_5'),

         Categorical(['relu', 'leaky-relu'], name='activation'),

         Real(0.0, 0.5, 'uniform', name='dropout'),

         Categorical([True, False], name='batch_normalization'),

         Categorical([0.01, 0.005, 0.002, 0.001], name='learning_rate'),

         Integer(256, 1024, name='batch_size')

        ] 
```

如前所示，您现在将与搜索相关的所有元素合并到一个目标函数中，该目标函数将由一个合并了基本搜索元素的函数创建，例如数据和交叉验证策略:

```
def make_objective(model_fn, X, space, cv, scoring, validation=0.2):

    # This decorator converts your objective function with named arguments

    # into one that accepts a list as argument, while doing the conversion

    # automatically.

    @use_named_args(space) 

    def objective(**params):

        print("\nTesting: ", params)

        validation_scores = list()

        for k, (train_index, test_index) in enumerate(kf.split(X)):

            val_index = list()

            train_examples = len(train_index)

            train_examples = int(train_examples * (1 - validation))

            train_index, val_index = (train_index[:train_examples], 

                                      train_index[train_examples:])

            start_time = time()

            model = model_fn(**params)

            measure_to_monitor = 'val_mean_squared_error'

            modality='min'

            early_stopping = tf.keras.callbacks.EarlyStopping(

                                 monitor=measure_to_monitor,

                                 mode=modality,

                                 patience=5, 

                                 verbose=0)

            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(

                                   'best.model',

                                   monitor=measure_to_monitor, 

                                   mode=modality, 

                                   save_best_only=True, 

                                   verbose=0)

            run = model.fit(df_to_dataset(

                                X_train.iloc[train_index, :], 

                                batch_size=params['batch_size']),

                            validation_data=df_to_dataset(

                                X_train.iloc[val_index, :], 

                                batch_size=1024),

                            epochs=1_000,

                            callbacks=[model_checkpoint, 

                                       early_stopping],

                            verbose=0)

            end_time = time()

            rounds = np.argmin(

                     run.history['val_mean_squared_error']) + 1

            model = tf.keras.models.load_model('best.model')

            shutil.rmtree('best.model')

            test_preds = model.predict(df_to_dataset(

                            X.iloc[test_index, :], shuffle=False, 

                            batch_size=1024)).flatten()

                            test_score = scoring(

                            X.iloc[test_index, :]['target'], 

                            test_preds)

            print(f"CV Fold {k+1} rmse:{test_score:0.5f} - {rounds} 

                  rounds - it took {end_time-start_time:0.0f} secs")

            validation_scores.append(test_score)

            if len(history[k]) >= 10:

                threshold = np.percentile(history[k], q=25)

                if test_score > threshold:

                    print(f"Early stopping for under-performing fold: 

                          threshold is {threshold:0.5f}")

                    return np.mean(validation_scores)

            history[k].append(test_score)

        return np.mean(validation_scores)

    return objective 
```

下一步是提供一系列随机搜索运行(作为从搜索空间开始构建一些反馈的一种方式)并收集结果作为起点。然后，我们可以将输入到贝叶斯优化中，并通过使用`forest_minimize`作为替代函数来继续:

```
counter = 0

history = {i:list() for i in range(5)}

used_time = 0

gp_round = dummy_minimize(func=objective,

                          dimensions=space,

                          n_calls=10,

                          callback=[onstep],

                          random_state=0)

gc.collect()

x0, y0 = joblib.load('checkpoint.pkl')

gp_round = gp_minimize(func=objective,

                           x0=x0,  # already examined values for x

                           y0=y0,  # observed values for x0

                           dimensions=space,

                           n_calls=30,

                           n_initial_points=0,

                           callback=[onstep],

                           random_state=0)

gc.collect() 
```

请注意，在前十轮随机搜索之后，我们使用随机森林算法作为替代函数继续搜索。这将确保比使用高斯过程更好更快的结果。

和以前一样，在这个过程中，我们必须努力使优化在我们拥有的时间和资源范围内变得可行(例如，通过设置一个较低的`n_calls`)。因此，我们可以通过保存优化状态、检查获得的结果，并在此后决定继续或结束优化过程，而不是投入更多的时间和精力来寻找更好的解决方案，来进行成批的搜索迭代。

## 使用 KerasTuner 创建更轻、更快的模型

如果前一部分的因为其复杂性而让你困惑，那么 KerasTuner 可以为你提供一个快速的解决方案来设置一个优化，而没有太多的麻烦。虽然它默认使用贝叶斯优化和高斯过程，但 KerasTuner 背后的新想法是**超波段优化**。超波段优化使用 bandit 方法计算出最佳参数(参见[http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf](http://web.eecs.umich.edu/~mosharaf/Readings/HyperBand.pdf))。这对于神经网络来说工作得很好，神经网络的优化前景非常不规则和不连续，因此并不总是适合高斯过程。

请记住，您无法避免构建使用输入超参数构建自定义网络的函数；KerasTuner 只是让它更容易处理。

让我们从头开始。Keras 的创造者 Fran ois Chollet 宣布,Keras tuner([https://keras.io/keras_tuner/](https://keras.io/keras_tuner/))为“Keras 模型的灵活高效的超参数调谐”。

Chollet 提出的运行 KerasTuner 的方法由简单的步骤组成，从您现有的 Keras 模型开始:

1.  将您的模型包装在一个以`hp`作为第一个参数的函数中。
2.  在函数的开始定义超参数。
3.  用超参数替换 DNN 静态值。
4.  根据给定的超参数编写模拟复杂神经网络的代码。
5.  如有必要，在构建网络时动态定义超参数。

我们现在将通过一个例子来探索所有这些步骤如何在 Kaggle 比赛中为你所用。目前，KerasTuner 是任何 Kaggle 笔记本提供的堆栈的一部分，因此您不需要安装它。此外，TensorFlow 附加组件是笔记本预装包的一部分。

如果您没有使用 Kaggle 笔记本，并且需要尝试 KerasTuner，您可以使用以下命令轻松安装这两个程序:

```
!pip install -U keras-tuner

!pip install -U tensorflow-addons 
```

你可以在 Kaggle 笔记本上找到这个例子:[https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/](https://www.kaggle.com/lucamassaron/kerastuner-for-imdb/)。

我们的第一步是导入必要的包(为一些命令创建快捷方式，例如`pad_sequences`)并直接从 Keras 上传我们将使用的数据:

```
import numpy as np

import pandas as pd

import tensorflow as tf

from tensorflow import keras

import tensorflow_addons as tfa

from sklearn.model_selection import train_test_split

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import LeakyReLU

from tensorflow.keras.layers import Activation

from tensorflow.keras.optimizers import SGD, Adam

from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

pad_sequences = keras.preprocessing.sequence.pad_sequences

imdb = keras.datasets.imdb(train_data, train_labels),

(test_data, test_labels) = imdb.load_data(num_words=10000)

train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.30,

                 shuffle=True, random_state=0) 
```

这一次，我们使用的是 Keras 包([https://keras.io/api/datasets/imdb/](https://keras.io/api/datasets/imdb/))中的 IMDb 数据集。该数据集有一些有趣的特征:

*   这是来自 IMDb 的 25，000 条电影评论的数据集
*   这些评论由情绪(正面/负面)标记
*   目标类别是平衡的(因此准确性作为一个评分标准)
*   每个评论都被编码成一个单词索引(整数)列表
*   为方便起见，单词按总频率进行索引

此外，它还被成功地用于一个流行的单词嵌入的 Kaggle 竞赛中(【https://www.kaggle.com/c/word2vec-nlp-tutorial/overview】T21)。

这个例子涉及自然语言处理。这类问题通常通过使用基于 LSTM 或 GRU 层的**递归神经网络** ( **RNNs** )来解决。BERT、RoBERTa 和其他基于 transformer 的模型通常会取得更好的结果——作为依赖于大型语言语料库的预训练模型——但这不一定在所有问题中都是正确的，rnn 可以证明是一个强大的基线，或者是神经模型集合的一个很好的补充。在我们的例子中，所有的单词都已经进行了数字索引。我们只需在现有的索引中添加数字代码，这些代码表示填充符(因此我们可以轻松地将所有文本标准化为短语长度)、句子的开头、一个未知单词和一个未使用的单词:

```
# A dictionary mapping words to an integer index

word_index = imdb.get_word_index()

# The first indices are reserved

word_index = {k:(v+3) for k,v in word_index.items()} 

word_index["<PAD>"] = 0

word_index["<START>"] = 1

word_index["<UNK>"] = 2  # unknown

word_index["<UNUSED>"] = 3

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):

    return ' '.join([reverse_word_index.get(i, '?') for i in text]) 
```

下一步是为**注意力**创建一个自定义层。注意力是变压器模型的基础，也是最近神经 NLP 中最具创新性的想法之一。

关于这些层次如何工作的所有细节，请参见关于注意力的开创性论文:Vaswani，a .等人。神经信息处理系统的进展。2017([https://proceedings . neur IPS . cc/paper/2017/file/3 F5 ee 243547 dee 91 FBD 053 C1 C4 a845 aa-paper . pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf))。

注意力的概念很容易传达。LSTM 和 GRU 图层输出经过处理的序列，但并非这些输出序列中的所有元素对您的预测都很重要。除了使用分层序列上的池层对所有输出序列进行平均，您实际上可以对它们进行*加权平均*(并在训练阶段学习要使用的正确权重)。这个加权过程(**注意**)肯定会提高你将要进一步传递的结果。当然，您可以使用多个注意力层使这种方法更加复杂(我们称之为**多头注意力**)，但是在我们的例子中，一个层就足够了，因为我们想要证明在这个问题中使用注意力比简单地平均或只是将所有结果连接在一起更有效:

```
from tensorflow.keras.layers import Dense, Dropout

from tensorflow.keras.layers import Flatten, RepeatVector, dot, multiply, Permute, Lambda

K = keras.backend

def attention(layer):

    # --- Attention is all you need --- #

    _,_,units = layer.shape.as_list()

    attention = Dense(1, activation='tanh')(layer)

    attention = Flatten()(attention)

    attention = Activation('softmax')(attention)

    attention = RepeatVector(units)(attention)

    attention = Permute([2, 1])(attention)

    representation = multiply([layer, attention])

    representation = Lambda(lambda x: K.sum(x, axis=-2), 

                            output_shape=(units,))(representation)

    # ---------------------------------- #

    return representation 
```

作为我们对这个问题的 DNNs 架构的实验的进一步变化，我们还想测试使用不同种类的优化器的有效性，例如作为**修正的 Adam** (一个自适应学习 Adam 优化器；阅读本帖了解更多:[https://lessw . medium . com/new-state-of-art-ai-optimizer-rectified-Adam-radam-5d 854730807 b](https://lessw.medium.com/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b)或**随机加权平均** ( **SWA** )。SWA 是一种在优化过程中平均遍历权重的方法，基于修改的学习率计划:如果您的模型倾向于过拟合或过冲，SWA 有助于接近最优解，并且它被证明特别适用于 NLP 问题。

```
def get_optimizer(option=0, learning_rate=0.001):

    if option==0:

        return tf.keras.optimizers.Adam(learning_rate)

    elif option==1:

        return tf.keras.optimizers.SGD(learning_rate, 

                                       momentum=0.9, nesterov=True)

    elif option==2:

        return tfa.optimizers.RectifiedAdam(learning_rate)

    elif option==3:

        return tfa.optimizers.Lookahead(

                   tf.optimizers.Adam(learning_rate), sync_period=3)

    elif option==4:

        return tfa.optimizers.SWA(tf.optimizers.Adam(learning_rate))

    elif option==5:

        return tfa.optimizers.SWA(

                   tf.keras.optimizers.SGD(learning_rate, 

                                       momentum=0.9, nesterov=True))

    else:

        return tf.keras.optimizers.Adam(learning_rate) 
```

定义了两个关键函数后，我们现在面对最重要的编码函数:在给定参数的情况下提供不同神经架构的函数。我们并不编码我们想要连接到不同架构选择的所有各种参数；我们只提供了`hp`参数，它应该包含我们想要使用的所有可能的参数，并且将由 KerasTuner 运行。除了函数输入中的`hp`,我们固定词汇的大小和要填充的长度(如果有效长度较短，则添加虚拟值，如果长度较长，则删除短语):

```
layers = keras.layers

models = keras.models

def create_tunable_model(hp, vocab_size=10000, pad_length=256):

    # Instantiate model params

    embedding_size = hp.Int('embedding_size', min_value=8, 

                            max_value=512, step=8)

    spatial_dropout = hp.Float('spatial_dropout', min_value=0, 

                               max_value=0.5, step=0.05)

    conv_layers = hp.Int('conv_layers', min_value=1,

                         max_value=5, step=1)

    rnn_layers = hp.Int('rnn_layers', min_value=1,

                        max_value=5, step=1)

    dense_layers = hp.Int('dense_layers', min_value=1,

                          max_value=3, step=1)

    conv_filters = hp.Int('conv_filters', min_value=32, 

                          max_value=512, step=32)

    conv_kernel = hp.Int('conv_kernel', min_value=1,

                         max_value=8, step=1)

    concat_dropout = hp.Float('concat_dropout', min_value=0, 

                              max_value=0.5, step=0.05)

    dense_dropout = hp.Float('dense_dropout', min_value=0, 

                             max_value=0.5, step=0.05) 
```

在函数的第一部分，我们简单地从`hp`参数中恢复所有的设置。我们还明确了它们各自的搜索空间范围。与我们目前看到的解决方案相反，这部分工作是在模型函数内部完成的，而不是在外部。

功能继续使用从`hp`提取的参数定义不同的层。在某些情况下，参数将打开或关闭执行某些数据处理的网络的一部分。例如，在代码中，我们插入了一个图的分支(`conv_filters`和`conv_kernel`)，该分支使用卷积层处理单词序列，其 1D 形式也可以证明对 NLP 问题是有用的，因为它们可以捕捉 LSTMs 可能难以理解的单词和含义的本地序列。

现在我们可以定义实际的模型了:

```
 inputs = layers.Input(name='inputs',shape=[pad_length])

    layer  = layers.Embedding(vocab_size, embedding_size, 

                              input_length=pad_length)(inputs)

    layer  = layers.SpatialDropout1D(spatial_dropout)(layer)

    for l in range(conv_layers):

        if l==0:

            conv = layers.Conv1D(filters=conv_filters, 

                       kernel_size=conv_kernel, padding='valid',

                       kernel_initializer='he_uniform')(layer)

        else:

            conv = layers.Conv1D(filters=conv_filters,  

                       kernel_size=conv_kernel, padding='valid', 

                       kernel_initializer='he_uniform')(conv) 

    avg_pool_conv = layers.GlobalAveragePooling1D()(conv)

    max_pool_conv = layers.GlobalMaxPooling1D()(conv)

    representations = list()

    for l in range(rnn_layers):

        use_bidirectional = hp.Choice(f'use_bidirectional_{l}',

                                      values=[0, 1])

        use_lstm = hp.Choice(f'use_lstm_{l}', values=[0, 1])

        units = hp.Int(f'units_{l}', min_value=8, max_value=512, step=8)

        if use_lstm == 1:

            rnl = layers.LSTM

        else:

            rnl = layers.GRU

        if use_bidirectional==1:

            layer = layers.Bidirectional(rnl(units, 

                              return_sequences=True))(layer)

        else:

            layer = rnl(units, return_sequences=True)(layer)

        representations.append(attention(layer))

    layer = layers.concatenate(representations + [avg_pool_conv, 

                                                  max_pool_conv])

    layer = layers.Dropout(concat_dropout)(layer)

    for l in range(dense_layers):

        dense_units = hp.Int(f'dense_units_{l}', min_value=8, 

                             max_value=512, step=8)

        layer = layers.Dense(dense_units)(layer)

        layer = layers.LeakyReLU()(layer)

        layer = layers.Dropout(dense_dropout)(layer)

    layer = layers.Dense(1, name='out_layer')(layer)

    outputs = layers.Activation('sigmoid')(layer)

    model = models.Model(inputs=inputs, outputs=outputs) 
```

我们从定义输入层开始，用后续的嵌入层对其进行变换，嵌入层将序列值编码成密集层。使用`SpatialDropout1D`将一些丢弃正则化应用于该过程，该函数将随机丢弃输出矩阵的所有列(标准丢弃将随机丢弃矩阵中的单个元素)。在这些初始阶段之后，我们将网络分成一个基于卷积的管道(`Conv1D`)和另一个基于递归层的管道(GRU 或 LSTM)。正是在循环层之后，我们应用了注意力层。最后，这两个管道的输出被连接，在几个更密集的层之后，它们到达最终输出节点，一个 sigmoid，因为我们必须表示范围在 0 到 1 之间的概率。

在模型定义之后，我们设置学习参数并在返回之前编译模型:

```
 hp_learning_rate = hp.Choice('learning_rate', 

                                 values=[0.002, 0.001, 0.0005])

    optimizer_type = hp.Choice('optimizer', values=list(range(6)))

    optimizer = get_optimizer(option=optimizer_type,  

                              learning_rate=hp_learning_rate)

    model.compile(optimizer=optimizer,

                  loss='binary_crossentropy',

                  metrics=['acc'])

    return model 
```

请注意，我们已经使用 Keras 的功能 API 构建了模型，而不是顺序 API。事实上，我们会建议你避免连续的；它更容易设置，但是严重限制了您的潜在架构。

此时，大部分工作已经完成。作为一个建议，在我们自己使用 KerasTuner 进行了许多优化之后，我们更愿意首先构建一个*非参数*模型，使用我们想要测试的所有可能的架构特性，将网络的互斥部分设置为最复杂的解决方案。在我们建立了生成函数并且我们的模型看起来工作正常之后，我们可以，例如，表示它的图形，并且让它成功地适合一些例子作为测试。之后，我们开始将参数变量插入架构，并设置`hp`参数定义。

根据我们的经验，立即从参数函数开始将花费更多的时间和调试工作。KerasTuner 背后的想法是让你把你的 dnn 想象成一组模块化电路，并帮助你优化数据在其中的流动。

现在，我们进口 KerasTuner。首先，我们设置调谐器本身，然后我们开始搜索:

```
import keras_tuner as kt

tuner = kt.BayesianOptimization(hypermodel=create_tunable_model,

                                objective='val_acc',

                                max_trials=100,

                                num_initial_points=3,

                                directory='storage',

                                project_name='imdb',

                                seed=42)

tuner.search(train_data, train_labels, 

             epochs=30,

             batch_size=64, 

             validation_data=(val_data, val_labels),

             shuffle=True,

             verbose=2,

             callbacks = [EarlyStopping('val_acc',

                                        patience=3,

                                        restore_best_weights=True)]

             ) 
```

作为一个调谐器，我们选择贝叶斯优化，但你也可以尝试超波段调谐器([https://keras.io/api/keras_tuner/tuners/hyperband/](https://keras.io/api/keras_tuner/tuners/hyperband/))并检查它是否能更好地解决你的问题。我们将我们的模型函数提供给`hypermodel`参数。然后，我们使用字符串或函数设置目标、最大试验次数(如果没有更多的事情要做，KerasTuner 会更早停止)和初始随机试验次数(越多越好),以便通知贝叶斯过程。早期停止是 dnn 建模中的一个标准且表现良好的实践，您绝对不能忽视。最后，也是重要的一点，我们设置了保存搜索结果的目录，以及优化步骤可重复性的种子编号。

search 阶段像 Keras 模型的标准拟合一样运行，并且——这很重要——它接受回调。因此，您可以轻松地将提前停止添加到您的模型中。在这种情况下，给定的历元数因此应该被认为是最大的历元数。您可能还想优化批量大小，这在我们的示例中还没有实现。这仍然需要一些额外的工作，但是你可以通过阅读这期 GitHub 封闭版:[https://github.com/keras-team/keras-tuner/issues/122](https://github.com/keras-team/keras-tuner/issues/122)来了解如何实现。

优化完成后，您可以提取最佳参数并保存最佳模型，而无需重新训练它:

```
best_hps = tuner.get_best_hyperparameters()[0]

model = tuner.hypermodel.build(best_hps)

print(best_hps.values)

model.summary()

model.save("best_model.h5") 
```

在本例中，KerasTuner 找到了一个解决方案，它使用了:

*   更大的嵌入层
*   只有普通的 GRU 和 LSTM 图层(没有双向图层)
*   多个一维卷积层的叠加(Conv1D)
*   更多更大的致密层

有趣的是，该解决方案不仅更有效，而且比我们以前基于直觉和经验解决问题的尝试更轻更快。

Chollet 自己建议使用 KerasTuner 不仅仅是为了让你的 DNS 性能更好，也是为了把它们缩小到一个更容易管理的规模，这可能会在代码竞赛中产生影响。这让你可以在竞赛赞助商提供的有限推断时间内，把更多协同工作的模型放在一起。

如果您想查看更多使用 KerasTuner 的示例，Franç ois Chollet 还为 Kaggle 竞赛制作了一系列笔记本，以展示他的优化器的工作方式和功能:

*   [https://www . ka ggle . com/fchollet/keras-keras tuner-最佳实践](https://www.kaggle.com/fchollet/keras-kerastuner-best-practices)用于*数字识别器*数据集
*   [https://www . ka ggle . com/fchollet/Titanic-keras-keras tuner-best-practices](https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices)用于 *Titanic* 数据集
*   [https://www . ka ggle . com/fchollet/moa-keras-kera stuner-best-practices](https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices)参加*行动机制(MoA)预测*竞赛

## Optuna 的 TPE 方法

我们用另一个有趣的工具和方法完成了对贝叶斯优化的概述。正如我们所讨论的，Scikit-optimize 使用高斯过程(以及树算法),它直接对代理函数和采集函数建模。

作为对这些主题的提醒，当您尝试一组超参数时，**代理函数**有助于优化过程对潜在的性能结果进行建模。使用先前的实验及其结果建立替代函数；它只是一个预测模型，用于预测特定机器学习算法在特定问题上的行为。对于提供给代理函数的每个参数输入，您将获得一个预期的性能输出。正如我们所见，这很直观，也很容易被破解。

**获取函数**代替指出可以测试哪组超参数，以便提高替代函数预测机器学习算法性能的能力。如果我们能够根据代理函数的预测得出最佳的结果，这对于真正的测试也是有用的。这两个目标代表了贝叶斯优化过程的*探索*部分(在那里你运行实验)和*利用*部分(在那里你测试性能)。

相反，基于 **TPE** 的优化器通过估计参数值成功的可能性来解决问题。换句话说，他们使用连续的细化来模拟参数本身的成功分布，为更成功的值组合分配更高的概率。

在这种方法中，超参数集被这些分布分为好的和坏的，它们在贝叶斯优化中扮演代理和获取函数的角色，因为分布告诉您在哪里采样以获得更好的性能或探索哪里存在不确定性。

为了探索 TPE 的技术细节，我们建议阅读 Bergstra，j .等人的*超参数优化算法*。神经信息处理系统进展 24，2011([https://proceedings . neur IPS . cc/paper/2011/file/86 E8 f 7 ab 32 CFD 12577 BC 2619 BC 635690-paper . pdf](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf))。

因此，TPE 可以对搜索空间建模，同时通过从调整后的参数概率分布中采样，建议算法接下来可以尝试什么。

长期以来，**hyperpt**是那些更喜欢使用 TPE 而不是基于高斯过程的贝叶斯优化的人的选择。然而，在 2018 年 10 月，Optuna 出现在开源中，由于其多功能性(它也适用于神经网络，甚至用于集成)，速度以及与之前的优化器相比在找到更好的解决方案方面的效率，它已经成为 Kagglers 的首选。

在这一节中，我们将演示建立一个搜索是多么容易，在 Optuna 术语中，这被称为*研究*。您所需要做的就是编写一个目标函数，将 Optuna 要测试的参数作为输入，然后返回一个评估。验证和其他算法方面可以在目标函数中以直接的方式处理，也可以使用对函数本身外部变量的引用(全局变量或局部变量)。Optuna 还允许**修剪**，也就是说，发出信号表明某个特定的实验进展不顺利，Optuna 可以停止并忘记它。Optuna 提供了激活这个回调的函数列表(见[https://optuna . readthe docs . io/en/stable/reference/integration . html](https://optuna.readthedocs.io/en/stable/reference/integration.html))；之后算法会高效地为你运行一切，这将大大减少优化所需的时间。

所有这些都在我们的下一个例子中。我们回到为 *30 天的 ML* 比赛进行优化。这一次，我们试图找出什么参数使 XGBoost 适用于这个比赛。

你可以在[https://www . ka ggle . com/lucamassaron/optuna-Bayesian-optimization](https://www.kaggle.com/lucamassaron/optuna-bayesian-optimization)找到这个例子的笔记本。

作为第一步，我们像以前一样上传库和数据:

```
import pandas as pd

import numpy as np

from sklearn import preprocessing

from sklearn.metrics import mean_squared_error

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OrdinalEncoder

from xgboost import XGBRegressor

import optuna

from optuna.integration import XGBoostPruningCallback

# Loading data 

X_train = pd.read_csv("../input/30-days-of-ml/train.csv").iloc[:100_000, :]

X_test = pd.read_csv("../input/30-days-of-ml/test.csv")

# Preparing data as a tabular matrix

y_train = X_train.target

X_train = X_train.set_index('id').drop('target', axis='columns')

X_test = X_test.set_index('id')

# Pointing out categorical features

categoricals = [item for item in X_train.columns if 'cat' in item]

# Dealing with categorical data using OrdinalEncoder

ordinal_encoder = OrdinalEncoder()

X_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])

X_test[categoricals] = ordinal_encoder.transform(X_test[categoricals]) 
```

当使用 Optuna 时，您只需定义一个包含模型、交叉验证逻辑、评估度量和搜索空间的目标函数。

当然，对于数据，您可以引用函数本身之外的对象，这使得函数的构造更加容易。和在 KerasTuner 中一样，这里需要一个基于 Optuna 类的特殊输入参数:

```
def objective(trial):

    params = {

            'learning_rate': trial.suggest_float("learning_rate", 

                                                 0.01, 1.0, log=True),

            'reg_lambda': trial.suggest_loguniform("reg_lambda", 

                                                   1e-9, 100.0),

            'reg_alpha': trial.suggest_loguniform("reg_alpha", 

                                                  1e-9, 100.0),

            'subsample': trial.suggest_float("subsample", 0.1, 1.0),

            'colsample_bytree': trial.suggest_float(

                                      "colsample_bytree", 0.1, 1.0),

            'max_depth': trial.suggest_int("max_depth", 1, 7),

            'min_child_weight': trial.suggest_int("min_child_weight", 

                                                  1, 7),

            'gamma': trial.suggest_float("gamma", 0.1, 1.0, step=0.1)

    }

    model = XGBRegressor(

        random_state=0,

        tree_method="gpu_hist",

        predictor="gpu_predictor",

        n_estimators=10_000,

        **params

    )

    model.fit(x, y, early_stopping_rounds=300, 

              eval_set=[(x_val, y_val)], verbose=1000,

              callbacks=[XGBoostPruningCallback(trial, 'validation_0-rmse')])

    preds = model.predict(x_test)

    rmse = mean_squared_error(y_test, preds, squared=False)

    return rmse 
```

在本例中，出于性能原因，我们不会交叉验证，而是使用一个固定数据集用于训练，一个用于验证(提前停止)，一个用于测试。我们在这个例子中使用了 GPU，并且我们还对可用数据进行了子集化，以便在合理的时间长度内执行 60 次试验。如果不想使用 GPU，只需从`XGBRegressor`实例化中移除`tree_method`和`predictor`参数即可。还要注意我们如何在`fit`方法中设置回调，以便为 Optuna 提供关于模型执行情况的反馈，这样优化器就可以尽早停止性能不佳的实验，为其他尝试留出空间。

```
x, x_val, y, y_val = train_test_split(X_train, y_train, random_state=0,

                                      test_size=0.2)

x, x_test, y, y_test = train_test_split(x, y, random_state=0, test_size=0.25)

study = optuna.create_study(direction="minimize")

study.optimize(objective, n_trials=100) 
```

另一个值得注意的方面是，根据您的问题，您可以决定优化最小化还是最大化(Scikit-optimize 只对最小化问题有效)。

```
print(study.best_value)

print(study.best_params) 
```

要完成运行，您只需打印或导出最佳测试性能和通过优化找到的最佳参数。

![](img/Ruchi_Bhatia.png)

鲁奇·巴蒂亚

[https://www.kaggle.com/ruchi798](https://www.kaggle.com/ruchi798)

作为对这一密集章节的总结，让我们来看最后一次采访。这一次，我们采访了鲁奇·巴蒂亚，一位数据集和笔记本方面的大师。Ruchi 目前是卡内基梅隆大学的研究生，OpenMined 的数据科学家，HP 在 Z 的数据科学全球大使。

你最喜欢哪种比赛，为什么？从技术和解决途径来说，你在 Kaggle 上的特长是什么？

*我最喜欢的竞赛类型是 NLP 和分析竞赛。掌握多种语言在我的主要关注点和兴趣中发挥了重要作用:自然语言处理。*

*至于分析竞赛，我喜欢从复杂的数据中寻找意义，并在数据的支持下回答问题！Kaggle 上的每个比赛都很新颖，需要不同的技术。我主要遵循数据驱动的算法选择方法，没有固定的偏好。*

你是如何对待一场 Kaggle 比赛的？这种方法与你在日常工作中的做法有什么不同？

*当宣布一项新的竞赛时，我的首要任务是深入理解问题陈述。有时，问题陈述可能会超出我们的舒适区或领域，因此* *至关重要，以确保我们在继续进行探索性数据分析之前很好地掌握它们。在执行 EDA 时，我的目标是理解数据分布，并专注于了解手头的数据。在此期间，我们可能会遇到一些模式，我们应该努力理解这些模式，并为异常值和例外情况形成一个假设。*

在此之后，我花时间了解竞赛指标。我的下一步是创建一个无泄漏的交叉验证策略。之后，我选择了一个基线模型，并提交了我的第一份报告。如果本地验证和竞赛排行榜之间的相关性不令人满意，我会根据需要进行迭代，以了解可能的差异并解释它们。

然后，我会继续改进我的建模方法。除此之外，调整参数和尝试新的实验有助于了解什么对手头的数据最有效(确保我在整个过程中防止过度拟合)。最后，在比赛的最后几周，我进行了模型组装，并检查了我的解决方案的健壮性。

至于我在 Kaggle 之外的项目，我的大部分时间都花在数据收集、清理和从数据中获取相关价值上。

Kaggle 对你的职业生涯有帮助吗？如果有，如何实现？

Kaggle 极大地帮助了我加快职业发展。它不仅帮助我找到了对数据科学的热情，还激励我有效地做出贡献并保持一致。这是一个完美的地方，可以用我们手边的大量数据进行动手实验，并在全球范围内展示我们的工作。此外，我们的作品易于获取，因此我们也可以接触到更广泛的受众。

*我已经在我的文件夹中使用了我的大部分 Kaggle 作品，来表明我在迄今为止的旅程中所完成的作品的多样性。Kaggle 竞赛旨在解决新奇和现实世界的问题，我觉得雇主在寻找我们解决这些问题的能力和资质。我还策划了广泛的数据集，帮助我突出了我在处理原始数据方面的敏锐度。这些项目帮助我获得了多个工作机会。*

以你的经验来看，没有经验的 Kagglers 经常会忽略什么？你现在知道了什么，你希望在你刚开始的时候就知道？

*在我的经历中，我注意到许多 Kagglers 在比赛中的排名没有达到他们的预期时会感到沮丧。经过几个星期和几个月的努力，我可以理解为什么他们可能会提前放弃，但赢得 Kaggle 比赛不是一件容易的事。有几个不同教育背景和工作经验的人在竞赛，勇于尝试才是最重要的。我们应该关注我们的个人成长，看看我们在旅程中走了多远。*

对于数据分析或机器学习，你有什么特别推荐的工具或库吗？

*全面的探索性数据分析与相关可视化相结合，帮助我们发现数据趋势和背景，从而改进我们的方法。因为我相信可视化的力量，我最喜欢的数据科学库是 Seaborn 和 TensorBoard。Seaborn 用于 EDA，TensorBoard 用于机器学习工作流程中所需的可视化。我偶尔也用 Tableau。*

当一个人参加比赛时，他应该记住或做的最重要的事情是什么？

当人们参加比赛时，我认为他们应该做好深入问题陈述和研究的准备。Kaggle 上的比赛尤其具有挑战性，在许多情况下有助于解决现实生活中的问题。人们应该有积极的心态，不要灰心丧气。Kaggle 竞赛提供了学习和成长的绝佳机会！

# 摘要

在本章中，我们详细讨论了超参数优化，这是一种提高模型性能和在排行榜上获得更高分数的方法。我们从解释 Scikit-learn 的代码功能开始，比如网格搜索和随机搜索，以及更新的减半算法。

然后，我们进展到贝叶斯优化，并探讨了 Scikit-optimize、KerasTuner，最后是 Optuna。我们花了更多的时间讨论通过高斯过程对代理函数的直接建模以及如何破解它，因为它可以让你有更强的直觉和更特别的解决方案。我们认识到，目前，Optuna 已经成为 Kaggler 中的黄金标准，用于表格比赛以及深度神经网络比赛，因为它在 ka ggle 笔记本允许的时间内更快地收敛到最佳参数。

然而，如果你想在竞赛中脱颖而出，你也应该努力测试来自其他优化器的解决方案。

在下一章，我们将继续讨论另一种提高你在 Kaggle 比赛中表现的方法:集合模型。通过发现平均、混合和叠加的工作原理，我们将说明如何提高结果，使之超过仅通过调整超参数所能获得的结果。

# 加入我们书的不和谐空间

加入这本书的 Discord workspace，每月与作者进行一次*向我提问*会议:

[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)

![](img/QR_Code40480600921811704671.png)**