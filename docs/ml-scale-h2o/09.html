<html><head/><body>


	
		<title>B16721_07_Final_SK_ePub</title>
		
	
	
		<div><h1 id="_idParaDest-127"><em class="italic"> <a id="_idTextAnchor127"/>第七章</em>:了解ML模型</h1>
			<p>现在我们已经使用H2O软件建立了一些模型，生产前的下一步是了解模型是如何做决定的。这被不同地称为<strong class="bold">机器学习可解释性</strong> ( <strong class="bold"> MLI </strong>)、<strong class="bold">可解释的人工智能</strong> ( <strong class="bold"> XAI </strong>)、模型可解释性等等。所有这些术语的要点是，建立一个预测良好的模型是不够的。在完全信任之前部署任何模型都存在固有的风险。在这一章中，我们概述了H2O解释ML模型的一系列能力。</p>
			<p>本章结束时，您将能够做到以下几点:</p>
			<ul>
				<li>选择合适的模型度量来评估您的模型。</li>
				<li>解释什么是Shapley值以及如何使用它们。</li>
				<li>描述全局和局部可解释性的区别。</li>
				<li>使用多种诊断方法来建立对模型的理解和信任。</li>
				<li>使用全局和局部解释以及模型性能度量来从一组候选模型中选择最佳模型。</li>
				<li>评估模型预测性能、评分速度和单个候选模型中满足的假设之间的权衡。</li>
			</ul>
			<p>在本章中，我们将讨论以下主要话题:</p>
			<ul>
				<li>选择模型性能指标</li>
				<li>解释H2O制造的车型(全球和本地)</li>
				<li>通过H2O AutoDoc自动化<a id="_idTextAnchor128"/>模型文档</li>
			</ul>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor129"/>选择模型性能指标</h1>
			<p>与任何模型最相关的问题是，<em class="italic">它的预测能力如何？</em>不考虑模型可能具有的任何其他积极属性，预测不佳的模型就是没有多大用处。如何最好地衡量预测性能取决于正在解决的具体问题和数据科学家可用的选择。H2O为衡量模型性能提供了多种选择。</p>
			<p>为了测量回归问题中预测模型的性能，H2O提供了R2、<strong class="bold">均方误差</strong> ( <strong class="bold"> MSE </strong>)、<strong class="bold">均方根误差</strong> ( <strong class="bold"> RMSE </strong>)、<strong class="bold">均方根对数误差</strong> ( <strong class="bold"> RMSLE </strong>)和<strong class="bold">平均绝对误差</strong> ( <strong class="bold"> MAE </strong>)作为度量。MSE <a id="_idIndexMarker509"/>和RMSE <a id="_idIndexMarker510"/>是很好的默认选项，RMSE<a id="_idIndexMarker511"/>是我们的首选，因为度量标准<a id="_idIndexMarker512"/>用与预测相同的单位表示(而不是像MSE那样用平方单位)。所有基于平方误差的度量通常对异常值敏感。如果对异常值的健壮性是一个需求，那么MAE是一个更好的选择。最后，RMSLE在欠预测比过预测更糟糕的特殊情况下是有用的。</p>
			<p>对于分类<a id="_idIndexMarker513"/>模型，H2O增加了基尼系数、绝对<strong class="bold">马修斯相关系数</strong> ( <strong class="bold"> MCC </strong>)、F1、F0.5、F2、精确度、对数损失、<strong class="bold">ROC曲线下面积</strong> ( <strong class="bold"> AUC </strong>)、精确度-召回曲线下面积 ( <strong class="bold"> AUCPR </strong>)和<strong class="bold">Kolmogorov-Smirnov</strong>(<strong class="bold">根据我们的经验，AUC是业务中最常用的指标。因为与业务合作伙伴和高管的沟通对数据科学家来说至关重要，所以我们建议在适合工作时使用众所周知的指标。在AUC的情况下，当数据相对平衡时，它在二进制分类模型方面做得很好。对于不平衡数据，AUCPR是更好的选择。</strong></p>
			<p>基于信息论的逻辑损失度量具有一些数学优势。特别是，如果您对类成员的预测概率本身感兴趣，而不仅仅是对预测的分类感兴趣，对数损失是一个更好的度量选择。关于这些评分选项的更多文档可以在<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html">https://docs . H2O . ai/H2O/latest-stable/H2O-docs/performance-and-prediction . html</a>找到。</p>
			<p>在AutoML中为分类问题创建的排行榜包括AUC、Logloss、AUCPR、平均每类误差、RMSE和MSE作为性能指标。在<a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"> <em class="italic">第五章</em> </a>、<em class="italic">高级建模-第一部分</em>中创建的<code>check</code> AutoML对象的排行榜如图<em class="italic">图7.1 </em>所示为例:</p>
			<div><div><img src="img/B16721_07_01.jpg" alt="Figure 7.1 – An AutoML leaderboard for the check object &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.1–支票对象的AutoML排行榜</p>
			<p>除了预测性能的<a id="_idIndexMarker517"/>之外，模型性能的其他指标在企业环境中也很重要。其中两个默认包含在<a id="_idIndexMarker518"/> AutoML排行榜中的是拟合一个模型所需的时间量(<code>training_time_ms</code>)和预测一行数据所需的时间量(<code>predict_time_per_row_ms</code>)。</p>
			<p>在<em class="italic">图7.1 </em>中，根据AUC和Logloss的最佳模型是所有模型的堆叠集合(顶行中的模型)。这个模型也是最慢的，比任何单个模型慢一个数量级。特别是对于流或实时应用，不能足够快地评分的模型可能自动被取消候选资格，而不管其预测性能如何。</p>
			<p>为了理解和评估我们的ML模型，我们接下来处理模型可解释性。</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor130"/>解释H2O制造的车型</h1>
			<p>根据我们的测试数据测量的模型性能<a id="_idIndexMarker519"/>指标可以告诉我们一个模型预测得有多好，预测得有多快。正如在引言一章中提到的，知道一个模型预测得很好并不是将其投入生产的充分理由。单独的性能指标不能提供任何关于<em class="italic">为什么</em>模型预测是这样的洞察。如果我们不理解为什么模型预测得很好，我们就没有希望能够预测到会使模型不能很好工作的条件。在将模型推广到生产之前，解释模型推理的能力是关键的一步。这个过程可以描述为获得对模型的信任。</p>
			<p>可解释性<a id="_idIndexMarker520"/>通常分为全局组件和局部组件。全局可解释性<a id="_idIndexMarker522"/>描述了<a id="_idIndexMarker523"/>模型如何适用于整个群体。获得对模型的信任主要是决定它如何在全球范围内工作的功能。局部解释<a id="_idIndexMarker524"/>在单独的<a id="_idIndexMarker525"/>行上操作。他们提出了一些问题，比如个人预测是如何产生的。<code>h2o.explain</code>和<code>h2o.explain_row</code>方法分别为全局和局部解释捆绑了一组可解释函数和可视化。</p>
			<p>我们从简单介绍Shapley值开始这一部分，Shapley值是模型可解释性的基础方法之一，第一次遇到时可能会感到困惑。我们用<code>h2o.explain</code>覆盖单个模型的全局解释，用<code>h2o.explain_row</code>覆盖局部解释。然后我们使用<code>h2o.explain</code>来解决AutoML的全局解释，我们用它来证明可解释性在模型选择中的作用。我们使用在第5章 、<em class="italic">高级模型构建-第1部分</em>中开发的两个模型来说明这些方法的输出。第一个，<code>gbm</code>，是使用<a id="_idIndexMarker526"/>默认值和H2O <code>check</code>构建的个人基线模型。选择这些模型仅作为示例，承认原始基线模型是通过多个特征工程和模型优化步骤改进的。</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor131"/>沙普利值的简单介绍</h2>
			<p>Shapley值<a id="_idIndexMarker527"/>已经成为ML可解释性的一个重要部分，作为一种将每个特征的贡献归因于整体或个体预测的手段。Shapley值在数学上是优雅的，非常适合归因的任务。在本节中，我们将介绍Shapley值:它们的来源、计算以及如何使用它们进行解释。</p>
			<p>2012年诺贝尔经济学奖得主劳埃德·沙普利(Lloyd Shapley，1923-2016)在1953年推导出沙普利值，作为博弈论中一个特定问题的解决方案。假设一群一起工作的玩家得到了一份奖品。奖金应该如何在玩家之间公平分配？</p>
			<p>沙普利从定义公平的数学公理<a id="_idIndexMarker528"/>开始:<strong class="bold">对称</strong>(贡献相同金额的玩家获得相同的支出)<strong class="bold">哑元</strong>(没有贡献的玩家什么也得不到)<strong class="bold">可加性</strong>(如果游戏可以分成<a id="_idIndexMarker529"/>可加性<a id="_idIndexMarker530"/>部分，那么就可以分解支出)。Shapley值是满足这些公理的唯一数学解<a id="_idIndexMarker531"/>。简而言之，沙普利价值法根据玩家的边际贡献按比例支付报酬。</p>
			<p>接下来，我们演示几个简单场景的Shapley值的计算。</p>
			<h3>沙普利计算图解–两个玩家</h3>
			<p>为了说明Shapley值的计算，考虑以下简单的例子。两个音乐家，约翰和保罗，单独表演可以分别赚4和3。约翰和保罗一起玩赚了10英镑。他们应该如何划分10？</p>
			<p>为了计算他们的边际贡献，考虑这些玩家可以被排序的方式的数量。对于两个玩家来说，只有两种独特的顺序:约翰在玩，然后保罗加入，或者保罗在玩，然后约翰加入。这在<em class="italic">图7.2 </em>中进行了说明:</p>
			<div><div><img src="img/B16721_07_02.jpg" alt="Figure 7.2 – Unique player sequences for John and Paul&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.2-约翰和保罗的独特球员序列</p>
			<p>作为唯一玩家序列的公式允许我们计算每个玩家的Shapley值。我们在<em class="italic">图7.3 </em>中说明了约翰的沙普利值的计算:</p>
			<div><div><img src="img/B16721_07_03.jpg" alt="Figure 7.3 – Sequence values for John&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.3-约翰的序列值</p>
			<p>约翰是第一个出现在序列1中的<a id="_idIndexMarker533"/>玩家，因此沙普利的贡献只是边际价值<em class="italic"> v(J) = 4 </em>。在第二个序列中，约翰在保罗之后加入。约翰的边际价值是约翰和保罗的共同价值，<em class="italic"> v(JP) </em>，减去保罗的边际价值，<em class="italic"> v(P) </em>。换句话说，<em class="italic">10–3 = 7</em>。John的Shapley值是每个序列的平均值:<em class="italic"> S(J) = 11/2 = 5.5 </em>。因此，约翰应该得到10英镑中的5.50英镑。</p>
			<p>Paul的Shapley值以类似的方式计算(显然，也可以通过减法计算)。顺序计算如<em class="italic">图7.4 </em>所示:</p>
			<div><div><img src="img/B16721_07_04.jpg" alt="Figure 7.4 – Sequence values for Paul&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.4-Paul的序列值</p>
			<p>在<em class="italic">图7.4 </em>中的第一个序列中，保罗在约翰之后加入，所以序列值为joint，<em class="italic"> v(JP) = 10 </em>，减去约翰的边际，<em class="italic"> v(J) = 4 </em>。第二个序列正好是保罗的边际值:<em class="italic"> v(P)=3 </em>。Paul的Shapley值为<em class="italic"> S(P) = 9/2 = 4.5 </em>。</p>
			<p>这些计算很简单，对两个玩家都有意义。让我们看看当我们添加第三个玩家时会发生什么。</p>
			<h3>沙普利计算图解——三名玩家</h3>
			<p>假设第三个音乐家乔治加入了约翰和保罗。乔治自己挣2英镑，和约翰一起演出挣7英镑，和保罗一起演出挣9英镑，三个人一起演出挣20英镑。为清晰起见，我们在<em class="italic">图7.5 </em>中总结了收益:</p>
			<div><div><img src="img/B16721_07_05.jpg" alt="Figure 7.5 – Earnings for John, Paul, and George&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.5-约翰、保罗和乔治的收入</p>
			<p>因为<a id="_idIndexMarker535"/>有三个玩家，所以有3个！=约翰、保罗和乔治可以到达的6个独特序列。图7.6 总结了这种三人游戏场景中John的Shapley值的计算:</p>
			<div><div><img src="img/B16721_07_06.jpg" alt="Figure 7.6 – Arrival sequences and values for calculating the Shapley value for John&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.6-用于计算John Shapley值的到达顺序和值</p>
			<p>在<em class="italic">图7.6 </em>中，序列1和2很简单:John是第一个玩家，所以只需要<em class="italic"> v(J) </em>值。在序列3和序列5中，约翰是第二个玩家。序列值的计算方法是，取John和第一个玩家的联合值，然后减去该玩家的边际值。序列4和序列6是相同的:约翰是最后一个玩家。他的边际贡献是取三方互动，<em class="italic"> v(JPG) </em>，减去保罗和乔治的联合价值，<em class="italic"> v(PG) </em>计算出来的。沙普利值为<em class="italic"> S(J) = 42/6 = 7 </em>。</p>
			<p>我们可以继续，用同样的方法找到保罗和乔治的沙普利值。</p>
			<h3>计算N个玩家的Shapley值</h3>
			<p>正如你所看到的，随着玩家数量的增加，Shapley值的计算会很快变得难以承受。Shapley序列的计算依赖于知道主效应的值以及从双向到双向的所有相互作用，如图<em class="italic">图7.5 </em>所示。另外还有<em class="italic"> N！</em>待解序列。随着玩家数量的增加，计算任务急剧增加。</p>
			<p>在预测模型的上下文中，每个特征是一个玩家，而预测是共享的奖励。我们可以使用Shapley值来描述每个特征对最终预测的影响。有些模型有几十个、几百个甚至更多的特征，在现实世界中计算Shapley值是很重要的。幸运的是，现代计算和计算某些模型家族的Shapley值的数学捷径的结合使得Shapley计算站得住脚。</p>
			<p>无论是在我们展示的简单例子中，还是在大型复杂ML模型中，Shapley值的解释都是相同的。</p>
			<p>接下来我们将注意力转向单一模型的整体解释。</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor132"/>单一型号的整体说明</h2>
			<p>我们使用<a href="B16721_05_Final_SK_ePub.xhtml#_idTextAnchor082"> <em class="italic">第5章</em> </a>、<em class="italic">高级模型构建-第1部分</em>中构建的基线GBM模型来说明<a id="_idIndexMarker537"/> e单个模型的解释。我们将该型号标记为<code>gbm</code>，并在<em class="italic">图5.5 </em>至<em class="italic">图5.10 </em>中记录其性能。</p>
			<p>全局解释的基本命令如下:</p>
			<pre class="source-code">model_object.explain(test)</pre>
			<p>这里，<code>test</code>是模型评估中使用的维持测试数据集。其他可选参数包括:</p>
			<ul>
				<li><code>top_n_features</code>:整数，表示在基于列的方法<a id="_idIndexMarker538"/>如<code>5</code>中使用多少列。</li>
				<li><code>columns</code>:在基于列的方法中使用的列名向量，作为<code>top_n_features</code>的替代。</li>
				<li><code>include_explanations</code>或<code>exclude_explanations</code>:分别为<code>include</code>或<code>exclude</code>方法，如<code>confusion_matrix</code>、<code>varimp</code>、<code>shap_summary</code>或<code>pdp</code>。</li>
			</ul>
			<p>对于<a id="_idIndexMarker540"/>单一分类模型，如<code>gbm</code>，该命令将按照重要性顺序显示前五个变量的混淆矩阵、变量重要性图、SHAP汇总图和部分相关性图。</p>
			<p>我们使用带有<code>gbm.explain(test)</code>命令的<code>gbm</code>模型来演示这一点，并依次讨论每个显示。</p>
			<h3>混乱矩阵</h3>
			<p>第一个<a id="_idIndexMarker541"/>输出结果是混淆矩阵，如图<em class="italic">图7.7 </em>所示:</p>
			<div><div><img src="img/B16721_07_07.jpg" alt="Figure 7.7 – Confusion matrix for the GBM baseline model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.7-GBM基线模型的混淆矩阵</p>
			<p><code>explain</code>方法的一个很好的特性是为每个显示提供简单的摘要描述:<code>gbm</code>显示真阴性(17616)、假阳性(2087)、假阴性(1716)和真阳性(2057)，以及假<a id="_idIndexMarker542"/>阳性率(10.59%)和假阴性率(45.48%)。</p>
			<h3>可变重要性图</h3>
			<p>来自<code>explain</code>方法的<a id="_idIndexMarker543"/>第二个可视化是变量重要性图，如图<em class="italic">图7.8 </em>所示:</p>
			<div><div><img src="img/B16721_07_08.jpg" alt="Figure 7.8 – Variable importance plot for the GBM baseline model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.8-GBM基线模型的可变重要性图</p>
			<p>注意<em class="italic">图7.8 </em>中的变量重要性图与我们使用<code>varimp_plot</code>命令手动创建的<em class="italic">图5.10 </em>中显示的图相同。在这里包含它是使用<code>explain</code>方法的好处之一。</p>
			<h3>SHAP摘要情节</h3>
			<p><code>explain</code>的第三个可视化输出是SHAP汇总图。<strong class="bold">基于Shapley值的SHAP </strong>，为<a id="_idIndexMarker544"/>提供了黑箱模型的信息视角。GBM基线模型的SHAP汇总图如<em class="italic">图7.9 </em>所示:</p>
			<div><div><img src="img/B16721_07_09.jpg" alt="Figure 7.9 – SHAP summary plot for the GBM baseline model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.9–GBM基线模型的SHAP汇总图</p>
			<p>让我们更详细地解释一下<em class="italic">图7.9 </em>中的<a id="_idIndexMarker545"/> SHAP汇总情节。在这个信息丰富的情节中，发生了很多事情:</p>
			<ul>
				<li>在左侧，我们根据Shapley值按照要素重要性递减的顺序列出了要素(数据列)。(注意Shapley特征重要性等级不一定与图7.8 中的特征重要性相同。)</li>
				<li>在右侧，我们有一个归一化特征值范围，从<strong class="bold"> 0.0 </strong>到<strong class="bold">1.0</strong>(H2O输出的蓝色到红色)。换句话说，对于每个特征，我们按颜色对原始数据值进行编码:低原始值为蓝色，中间值为紫色，高原始值为红色(在此图中显示为不同的灰色阴影)。</li>
				<li>每个观测值的水平位置由其SHAP值决定。SHAP值衡量每个要素对预测的贡献。较低的SHAP值与较低的预测相关联，而较高的值与较高的预测相关联。</li>
			</ul>
			<p>有了这个<a id="_idIndexMarker546"/>的初步了解，我们可以做出如下观察:</p>
			<ul>
				<li>右侧为红色值、左侧为蓝色值的要素与响应正相关。由于我们正在对不良贷款的概率进行建模，因此诸如更长期限(<code>term</code>)或更高循环利用率(<code>revol_util</code>)等特征与贷款违约正相关。(循环信用利用率实质上是客户的信用卡逐月余额。)</li>
				<li>左侧为红色值、右侧为蓝色值的要素与响应呈负相关。因此，举例来说，较高的年收入(<code>annual_inc</code>)与贷款违约呈负相关。</li>
			</ul>
			<p>这些来自SHAP概要图的模型观察具有直观的意义。你可能会认为信用卡余额较高或年收入较低的人拖欠贷款的可能性较大。</p>
			<p>注意，我们可以使用<code>gbm.shap_summary_plot(test)</code>命令得到相同的图。</p>
			<h3>部分相关图</h3>
			<p><code>explain</code>输出的<a id="_idIndexMarker547"/>第四个可视化是一组部分依赖图。显示的具体曲线取决于<code>top_n_features</code>或<code>columns</code>可选参数。默认情况下，前五个功能按照变量重要性递减的顺序显示。<em class="italic">图7.10 </em>显示了地址状态的部分依赖图:</p>
			<div><div><img src="img/B16721_07_10.jpg" alt="Figure 7.10 – Partial dependence plot for address state&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.10–地址状态的部分相关图</p>
			<p><em class="italic">图7.11 </em>显示了<a id="_idIndexMarker548"/>循环利用变量的部分相关图:</p>
			<div><div><img src="img/B16721_07_11.jpg" alt="Figure 7.11 – Partial dependence plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.11-循环利用率的部分相关图</p>
			<p>由<code>explain</code>输出的部分相关图<a id="_idIndexMarker549"/>包括由平均响应及其可变性(阴影区域包围的线)覆盖的样本大小的表示(从图形底部开始的阴影区域)。在分类变量的情况下，平均响应是一个带条的点，表示可变性，如图<em class="italic">图7.10 </em>。在数值变量的情况下，平均响应是一条带有较浅阴影的黑线，表示可变性，如图<em class="italic">图7.11 </em>所示。</p>
			<p>请注意，我们可以使用以下方法为任何单独的列创建一个部分相关性图:</p>
			<pre class="source-code">gbm.pd_plot(test, column='revol_util')</pre>
			<h3>全球个体条件期望(ICE)图</h3>
			<p>ICE图<a id="_idIndexMarker550"/>将在后面的<em class="italic">单个模型的局部解释</em>部分介绍。然而，为了完整起见，我们在这里包括了全球版本的ICE图。注意该图不是由<code>explain</code>输出的。<code>gbm.ice_plot(test, column='revol_util')</code>命令返回如图<em class="italic">图7.12 </em>所示的全球冰图:</p>
			<div><div><img src="img/B16721_07_12.jpg" alt="Figure 7.12 – Global ICE plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.12-循环利用的全球ICE图</p>
			<p>变量的<a id="_idIndexMarker551"/>全局ICE图是该变量的部分相关图的扩展。部分相关图显示了平均响应与特定变量的值之间的关系。阴影，如图<em class="italic">图7.11 </em>所示，表示部分依赖线的可变性。全球冰图通过使用多条线来代表人口，放大了这一点。(在分类变量的情况下，线用点代替，阴影用条代替。)</p>
			<p>如<em class="italic">图7.12 </em>所示，全球ICE图包括最小值(第0个百分位数)、十分位数(第10个百分位数到第90个百分位数乘以10秒)、最大值(第100个百分位数)和部分相关性本身的线条。这在视觉上比单独的部分依赖更准确地描绘了总体。平行于部分相关线的百分位数对应于部分相关是其良好代表的人口部分。通常情况下，总体的最小值和最大值的行为可能与部分相关线描述的平均行为有很大不同。在<em class="italic">图7.12 </em>中，有三条线与其他线不同:最小值、最大值和第10百分位。</p>
			<p>接下来我们把注意力转向单一模型的局部解释。</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor133"/>单一型号的局部说明</h2>
			<p><code>h2o.explain_row</code>方法<a id="_idIndexMarker552"/>允许<a id="_idIndexMarker553"/>数据科学家调查模型的本地解释。虽然全局解释用于理解模型如何表示总体，但是局部解释使我们能够在每行的基础上询问模型。当行代表客户时，这在业务中可能特别重要，正如我们的Lending Club分析中的情况。</p>
			<p>当使用预测模型做出直接影响客户的决策时(例如，不批准贷款申请或提高客户的保险费率)，全局解释不足以满足业务、法律或监管要求。这就是地方解释的关键所在。</p>
			<p><code>explain_row</code>方法<a id="_idIndexMarker554"/>为指定的<code>row_index</code>值返回这些本地解释。<code>gbm.explain_row(test, row_index=10)</code>命令根据不同的重要性为各列提供一个SHAP解释图和多个ICE图。与部分相关图一样，可以选择性地提供<code>top_n_features</code>或<code>columns</code>参数。</p>
			<p>得到的SHAP解释图如图7.13 所示:</p>
			<div><div><img src="img/B16721_07_13.jpg" alt="Figure 7.13 – SHAP explanation for index = 10&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.13–SHAP对指数= 10的解释</p>
			<p>SHAP解释<a id="_idIndexMarker555"/>显示了每个变量对基于Shapley值的整体预测的贡献。对于<em class="italic">图7.13 </em>中显示的客户，正的SHAP值可以认为是增加了贷款违约的概率，而负的SHAP值是降低了违约的概率。对于该客户，78.5%的循环利用率是预测概率的最大正贡献因素。最大的负贡献因素是年收入9万，这比其他任何变量都更能降低贷款违约的概率。SHAP解释可用于提供<strong class="bold">原因代码</strong>，以便<a id="_idIndexMarker556"/>帮助解释型号。原因代码也可用作直接与客户共享信息的基础，例如，在适用于某些金融和保险相关监管模型的不利行动代码中。</p>
			<p>我们接下来访问一些从<code>explain_row</code>方法输出的ICE图。</p>
			<h3>冰图为当地的解释</h3>
			<p>ICE图是部分相关图的单个或每行对应图。正如特性的部分相关图在改变特性值时显示目标变量的平均响应，ICE图在改变单行特性值时测量目标变量响应。考虑作为<code>gbm.explain_row</code>调用结果的<em class="italic">图7.14 </em>中显示的地址状态的ICE图:</p>
			<div><div><img src="img/B16721_07_14.jpg" alt="Figure 7.14 – ICE plot for address state&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.14–地址状态的ICE图</p>
			<p>图7.14 中<em class="italic">的垂直黑色虚线代表相关行的实际响应。在这种情况下，状态是<strong class="bold"> NJ </strong>(新泽西)，响应大约为0.10。如果这一行的状态是<strong class="bold"> VA </strong>(弗吉尼亚)，那么响应会更低(大约0.07)。如果这一行的州名改为<strong class="bold"> NV </strong>(内华达州)，响应会更高，大约为0.16。</em></p>
			<p>考虑下一个<em class="italic">图7.15 </em>，贷款期限的ICE图:</p>
			<div><div><img src="img/B16721_07_15.jpg" alt="Figure 7.15 – ICE plot for the loan term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.15-贷款期限的ICE图</p>
			<p><em class="italic">图7.13 </em>中的<a id="_idIndexMarker558"/> SHAP解释值，期限为36个月，是第二大负面因素(它降低贷款违约的概率仅次于年收入，是最大的)。根据<em class="italic">图7.15 </em>，60个月的贷款期限会导致违约概率略高于0.35，显著高于36个月期限的违约概率约为0.10。虽然SHAP解释和ICE图测量的是两种不同的东西，但它们的解释可以共同用来理解特定预测的行为。</p>
			<p>我们考虑的最后一个ICE图是针对循环利用率的，这是一个数字特征，而不是分类特征。该图如图7.16 所示:</p>
			<div><div><img src="img/B16721_07_16.jpg" alt="Figure 7.16 – ICE plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.16-循环利用的ICE图</p>
			<p>根据图7.13 中<em class="italic">的SHAP解释，循环利用<a id="_idIndexMarker559"/>是最重要的积极因素(增加贷款违约的可能性)。<em class="italic">图7.16 </em>中的ICE图显示了响应与循环利用率值之间的关系。如果<code>revol_util</code>是50%,贷款违约的概率会降低到大约0.08。在20%的情况下，违约概率约为0.05。如果该客户被拒绝贷款，循环利用的高价值将是一个可以辩护的理由。相应ICE图的结果可用于通知客户他们可以采取哪些步骤来获得贷款资格。</em></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor134"/>多个模型的全局解释</h2>
			<p>在决定将哪个模型提升到生产中时，例如，从AutoML运行中，数据科学家可以完全依赖预测模型指标。这可能意味着简单地推广具有最佳AUC值的模型。然而，有很多信息可以用来帮助这个决定，预测能力只是多个标准中的一个。</p>
			<p>H2O的全局和局部解释功能提供了额外的信息，这些信息对于结合预测属性评估模型非常有用。我们使用第5章 、<em class="italic">高级模型构建-第1部分</em>中的<code>check</code> AutoML对象进行演示。</p>
			<p>为多个模型启动全局解释的代码简单如下:</p>
			<pre class="source-code">check.explain(test)</pre>
			<p>这产生了可变重要性热图、模型相关性热图和多模型<a id="_idIndexMarker561"/>部分相关性图。我们将按顺序逐一查看。</p>
			<h3>可变重要性热图</h3>
			<p>可变重要性热图<a id="_idIndexMarker562"/>通过添加颜色作为与变量(作为行)和模型(作为列)一起查看的维度，直观地组合了多个模型的可变重要性图。<code>check.explain</code>产生的变量重要性热图如图<em class="italic">图7.17 </em>所示:</p>
			<div><div><img src="img/B16721_07_17.jpg" alt="Figure 7.17 – Variable importance heatmap for an AutoML object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.17-AutoML对象的可变重要性热图</p>
			<p>可变重要性值被编码为从低值的蓝色(冷)到高值的红色(热)的连续颜色。最终的图形在视觉上是有意义的。在<em class="italic">图7.17 </em>中，垂直条带对应每个型号，水平条带对应单个特征。相似的垂直带表示模型使用其特征的方式之间的高度对应。例如，<code>XGBoost_1</code>和<code>XGBoost_2</code>型号(最后两列)显示相似的模式。</p>
			<p>您还可以看到类似颜色的<a id="_idIndexMarker563"/>水平条带，用于变量，如<code>delinq_2yrs</code>、<code>verification_status</code>，或者在较小程度上用于变量<code>annual_inc</code>。这表明所有的候选模型都以同等的重要性对待这些变量。最后一行中的<code>term</code>变量是最引人注目的，在不同的模型中是不同的。这些模型对其绝对重要性的看法并不一致。然而，你必须注意不要对此做过多的解读。注意对于<code>term</code>，十个模型中的六个<em class="italic">相对</em>重要性是相同的(除了蓝色方块:<code>DRF_1</code>、<code>GBM_4</code>、<code>XGBoost_2</code>和<code>XGBoost_1</code>)。对于这六个模型，<code>term</code>是最重要的特征，尽管它的确切值变化很大。</p>
			<p>直接创建该显示的代码如下:</p>
			<pre class="source-code">check.varimp_heatmap()</pre>
			<p>接下来，让我们考虑模型关联热图。</p>
			<h3>模型关联热图</h3>
			<p><a id="_idIndexMarker564"/>变量重要性热图允许我们根据模型如何查看和使用其组成变量来比较多个模型。模型相关性热图解决了一个不同的问题:<em class="italic">这些不同模型的预测相关性如何？</em>为了回答这个问题，我们求助于<em class="italic">图7.18 </em>中的模型相关性热图:</p>
			<div><div><img src="img/B16721_07_18.jpg" alt="Figure 7.18 – Model correlation heatmap for an AutoML object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.18–AutoML对象的模型关联热图</p>
			<p>沿着图7.18 的<em class="italic">对角线的<a id="_idIndexMarker565"/>最暗的块显示了模型与其自身之间的完美关联。顺序变浅的阴影表示模型之间的相关性降低。您可能如何使用此展示来确定将哪个型号提升到生产中？</em></p>
			<p>这就是业务或监管约束发挥作用的地方。在我们的例子中，<code>StackedEnsemble_AllModels</code>在AUC方面具有最好的模型性能。假设我们不被允许将一个集合模型推广到产品中，不管是什么原因。与我们的最佳模型关联度最高的单个模型包括<code>XGBoost_3</code>、<code>GBM_5</code>和<code>GLM_1</code>。然后，这些可以成为提升到生产中的候选者，最终的决定基于附加的标准(可能是测试集上的AUC值)。</p>
			<p>如果这些附加标准之一是本机可解释性，那么这个AutoML对象的<code>GLM_1</code>是唯一的选择。请注意，可解释的模型在模型关联热图中用红色字体表示。</p>
			<p>我们可以使用以下内容直接创建此显示:</p>
			<pre class="source-code">check.model_correlation_heatmap(test)</pre>
			<p>让我们在下一小节继续介绍多模型的<a id="_idIndexMarker566"/>部分相关图。</p>
			<h3>多模型部分相关图</h3>
			<p>用于多个模型的<code>explain</code>方法的<a id="_idIndexMarker567"/>第三个输出是部分相关性图的延伸。对于分类变量，对应于不同模型的图符号和颜色显示在单独的图上。<em class="italic">图7.19 </em>是使用<code>term</code>变量的一个例子:</p>
			<div><div><img src="img/B16721_07_19.jpg" alt="Figure 7.19 – Multiple model partial dependence plot for a loan term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.19-贷款期限的多模型部分相关图</p>
			<p>对于数字变量，多个模型在同一个部分相关图上用不同颜色的线表示。<em class="italic">图7.20 </em>是使用<code>revol_util</code>变量的一个例子:</p>
			<div><div><img src="img/B16721_07_20.jpg" alt="Figure 7.20 – Multiple model partial dependence plot for revolving utilization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.20-循环利用的多模型部分相关图</p>
			<p>在<em class="italic">图7.19 </em>和<em class="italic">图7.20 </em>中，竞争模型产生了非常相似的结果。情况并不总是这样。例如，<em class="italic">图7.21 </em>显示了年收入的多模型部分相关图:</p>
			<div><div><img src="img/B16721_07_21.jpg" alt="Figure 7.21 – Multiple model partial dependence plot for annual income&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.21-年收入的多模型部分相关图</p>
			<p>尽管<a id="_idIndexMarker569"/>图7.21 中的大多数模型对于低收入人群来说是相似的，但随着收入的增加，它们会出现相当大的差异。这部分是由于年收入分布尾部的样本量非常小。数据科学家也可能基于不现实或不合理的尾部行为决定取消某些模型的资格。例如，根据我们的经验，贷款违约风险随着年收入的增加而增加是没有意义的。在最坏的情况下，我们预计收入和违约之间的关系不会超过某一点。随着收入的增加，我们更有可能期待贷款违约率单调下降。基于这种推理，我们将从考虑中移除顶部两条线(<code>DRF_1</code>和<code>GBM_1</code>)的模型。</p>
			<p>与其他<code>explain</code>方法一样，我们可以使用以下命令直接创建这个图:</p>
			<pre class="source-code">check.pd_multi_plot(test, column='annual_inc')</pre>
			<p>我们接下来访问模型文档。</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor135"/>自动化模型文档(H2O AutoDoc)</h1>
			<p>数据科学团队在企业环境中扮演的重要角色之一是记录投入生产的模型的历史、属性和性能。至少，模型文档应该是数据科学团队最佳实践的一部分。更常见的是，在企业环境中，全面的模型文档或白皮书被强制要求满足内部和外部控制以及法规或遵从性要求。</p>
			<p>一般来说，模型文档<a id="_idIndexMarker570"/>应该足够全面，以允许重新创建被记录的模型。这需要确定所有数据源，包括培训和测试数据特征、指定硬件系统组件、记录软件版本、建模代码、软件设置和种子、采用的建模假设、考虑的替代模型、性能指标和适当的诊断，以及基于业务或监管条件的任何其他必要内容。这个过程虽然至关重要，但非常耗时，而且可能很乏味。</p>
			<p><strong class="bold"> H2O AutoDoc </strong>是一款商业软件产品，可为H2O-3和scikit-learn中构建的模型自动创建全面的文档。类似的功能已经存在于H2O.ai的<strong class="bold">无人驾驶AI </strong>中，这是一款<a id="_idIndexMarker571"/>商业产品，它将自动特征工程与增强的AutoML相结合，以构建和部署监督学习模型。AutoDoc已经成功地用于记录生产中的模型。我们在此简要介绍使用AutoDoc自动创建文档<a id="_idIndexMarker572"/>:</p>
			<ol>
				<li>创建了模型对象后，我们将<code>Config</code>和<code>render_autodoc</code>模块导入Python: <pre>from h2o_autodoc import Config from h2o_autodoc import render_autodoc</pre></li>
				<li>接下来，我们将指定输出文件的路径:<pre>config = Config(output_path = "autodoc_report.docx")</pre></li>
				<li>然后，我们将通过传递配置信息和模型对象来呈现报告:<pre>doc_path = render_autodoc(h2o=h2o, config=config,                           model=gbm)</pre></li>
				<li>一旦<a id="_idIndexMarker573"/>报告被创建，报告的位置可以用下面的内容来表示:<pre>print(doc_path)</pre></li>
			</ol>
			<p><em class="italic">图7.22 </em>显示了H2O AutoDoc在Microsoft Word中创建的44页报告的目录:</p>
			<div><div><img src="img/B16721_07_22.jpg" alt="Figure 7.22 – Table of contents for model documentation created by H2O AutoDoc&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图7.22-H2O AutoDoc创建的模型文档目录</p>
			<p>以一致的方式用最少的人工工作量生成完整的文档的优势是不言而喻的。输出为微软Word文档或markdown格式，报告可以单独编辑和进一步定制。报告模板也很容易编辑，允许数据科学团队针对不同用途使用不同的报告结构:例如，内部白皮书或监管审查报告。AutoDoc功能一直是企业版H2O软件最受欢迎的功能之一。</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor136"/>总结</h1>
			<p>在本章中，我们回顾了多个模型性能指标，并学习了如何选择一个来评估模型的预测性能。我们通过一些简单的例子介绍了Shapley值，以进一步理解它们在预测模型评估中的目的和用途。在H2O，我们使用<code>explain</code>和<code>explain_row</code>命令为单一模型创建全局和局部解释。我们学习了如何解释由此产生的诊断和可视化，以获得对模型的信任。对于AutoML对象和其他模型列表，我们生成了全局和局部解释，并了解了如何使用它们和模型性能度量来剔除不合适的候选模型。综上所述，我们现在可以评估模型性能、评分速度和解释之间的权衡，以确定将哪个模型投入生产。最后，我们讨论了模型文档的重要性，并展示了H2O AutoDoc如何为任何在H2O制造的模型自动生成详细的文档。</p>
			<p>在下一章中，我们将把我们在H2O学到的所有关于建立和评估模型的知识整合在一起，创建一个部署就绪的模型，用于预测Lending Club数据中的不良贷款。</p>
		</div>
	

</body></html>