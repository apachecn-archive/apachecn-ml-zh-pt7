

# 一、机遇与挑战

机器学习 ( **ML** )和数据科学正在赢得各种各样的流行竞赛，这可以从它们在流行和专业媒体上的头条报道以及整个技术领域不断扩大的职位空缺中看出来。学生通常使用自己的计算机在相对较小的数据集上学习 ML 技术。那些进入这个领域的人经常会发现自己置身于一个大公司的不同环境中，那里的工人们忙碌着履行专业的工作职责，同时与分散在全国或世界各地的其他人合作。数据科学专业的学生和数据科学工作者都有一些关键的共同点——他们都在一个令人兴奋的不断发展的领域，企业认为这个领域对他们的未来越来越重要，他们赖以发展的数据正变得越来越丰富和多样化。

对于企业来说，ML 有着巨大的机会，因为 ML 对企业、客户、病人等等的变革性影响是多样的、广泛的、有利可图的，并且是改变生活的。竞争对手都在尝试同样的事情，这也是一个紧迫的背景。因此，企业被激励投资于重大的 ML 变革，并提供必要的数据、工具、生产系统和人员，以走向 ML 的成功。但是挑战也很大，这些挑战通常围绕着规模。规模的挑战在企业层面上以 ML 固有的多种形式出现。

在这一章中，我们将通过涵盖以下主要主题来定义和探索大规模制造的挑战:

*   按比例毫升
*   ML 生命周期和大规模 ML 的三个挑战领域
*   H2O.ai 对这些挑战的回答

# 毫升刻度

这本书是关于大规模实施 ML 以及如何使用 H2O.ai 技术来成功做到这一点的。我们所说的大规模营销具体指的是什么？在 ML 生命周期中，我们可以看到三种背景和规模挑战——从大型数据集构建模型，在企业生产环境中部署这些模型，以及在复杂的企业流程和利益相关者中执行全方位的 ML 活动。下图对此进行了总结:

![Figure 1.1 – The challenges of ML at scale
](img/B16721_01_01.jpg)

图 1.1–大规模 ML 的挑战

让我们深入探讨一下这些挑战。在此之前，我们将审视 ML 生命周期的一般概念，这将是整本书的有用参考。

# ML 生命周期和大规模 ML 的三个挑战领域

ML 生命周期是一个过程，数据科学家和企业利益相关者遵循该过程来构建 ML 模型并将其投入生产环境，在生产环境中他们进行预测并实现价值。在本节中，我们将定义一个简化的 ML 生命周期，并详细阐述两个对大规模 ML 提出特殊挑战的广泛领域。

## 简化的 ML 生命周期

我们将使用下面的表示 ML 生命周期。目标是实现一个简化的描述，我们都可以认识到它是 ML 的核心，同时避免尝试一个规范的定义。让我们把它作为讨论的工作框架:

![Figure 1.2 – A simplified ML life cycle
](img/B16721_01_02.jpg)

图 1.2–简化的 ML 生命周期

以下是一个简短的衔接。

### 模型结构

模型构建是一个高度迭代的过程，在构建值得在业务环境中部署的预测模型的过程中，会有频繁且不可预测的反馈循环。这些步骤可以总结如下:

*   **数据摄取**:数据是从建模环境中的源或存储层中提取的。从这里开始，在寻找和访问潜在有用的数据源并将数据转换成可用的形式方面，通常会有大量的工作要做。通常，这是作为更大的数据管道和架构的一部分来完成的。
*   **数据探索**:探索数据以了解其质量(例如，数据剖析、相关性分析、异常值检测和数据可视化)。
*   **数据处理**:清理数据(例如，缺失数据的插补、分类特征的减少和标准化)并设计新的特征。
*   **模型训练**:选择 ML 算法、评分标准和验证方法，并在一系列超参数间调整模型，并对照测试数据集进行测试。
*   **模型评估和可解释性**:针对性能度量、过度拟合和其他诊断来诊断模型的拟合；模型可解释性用于验证领域知识，在个人和全球层面解释模型决策，并防范制度风险，如对人口统计群体的不公平偏见。
*   **模型部署**:将模型作为评分工件部署到软件系统中，进行实时评分。
*   **模型监控**:对模型进行监控，以检测输入到模型中的数据与它被训练的数据分布相比是否随时间而变化。这称为数据漂移，通常会导致模型的预测能力下降。这通常会导致需要使用更新的数据集重新训练模型，然后重新部署更新后的模型。还可以监视该模型的其他模式，例如它是否对特定的人口统计组有偏见的决策，以及是否正在进行恶意攻击以试图使该模型发生故障。

如前所述，工作流中的一个关键属性是在模型被部署之前，或者在项目被认为不成功地到达那个阶段之前，在这些步骤之间采用的未知数量和迭代路径序列。

## 模型制作挑战——最先进的大规模模型

现在，让我们将大型数据集定义为任何超出您在笔记本或本地工作站上构建 ML 模型的能力的数据集。它可能太大，因为你的库只是崩溃，或者因为他们需要不合理的时间来完成。这可能发生在模型训练或数据摄取、探索和操作期间。

我们可以看到从大量数据中构建 ML 模型的四个不同的挑战，每一个都导致了一个更大的问题，我们称之为迭代的*摩擦。下图显示了这一点:*

![Figure 1.3 – The challenge of model building with large data volumes
](img/B16721_01_03.jpg)

图 1.3–利用大量数据构建模型的挑战

我们来详细阐述一下这个。

### 挑战一—数据大小和位置

企业收集并存储大量不同的数据，这对希望建立精确模型的数据科学家来说是一个福音。这些数据集要么存储在许多系统中，要么集中在一个公共的存储层(数据湖)，如 **Hadoop 分布式文件系统** ( **HDFS** )或 **AWS S3** 。对企业来说，构建数据并使其对内部消费者可用是一项重大的工作和挑战。然而，从大型数据集开始 ML 生命周期的数据科学家通常不能将这些数据移动到本地环境，因为安全原因或数据量大..结果是，数据科学家必须执行以下操作之一:

*   将对数据的操作(换句话说，将计算)转移到数据本身。
*   将数据移动到他们有权使用的高计算环境中。

### 挑战二—数据大小和数据操作

操作数据可能是计算密集型的，在资源不足的情况下尝试这样做要么会导致计算失败(例如，脚本、库或工具会崩溃)，要么会花费不合理的长时间。当可以在 10 分钟内完成时，谁愿意等 10 个小时来加入和过滤表数据呢？您可能认为不合理的时间量显然与数据集大小有关；数兆字节的数据总是比几兆字节的数据需要更长的处理时间。无论如何，数据处理的速度对于减少迭代的总时间是至关重要的。

### 挑战三—数据规模和数据探索

数据探索期间的数据大小挑战与数据操作期间的挑战相同。数据可能非常大，以至于在浏览模型时，您的处理会崩溃或花费不合理的时间来完成。

### 挑战四—数据规模和模型训练

ML 算法是极其*计算密集型的*，因为它们遍历数据集的每条记录，每次都执行复杂的计算，然后针对数据集重复迭代这些计算，以优化训练指标，从而在噪声中学习预测数学模式。在模型训练期间，我们的计算环境压力尤其大。

到目前为止，我们一直在讨论相对意义上的数据集大小；也就是说，在给定的计算环境中，大型数据卷会导致对它们的操作失败或需要很长时间才能完成。

从绝对意义上来说，数据科学家经常探索尽可能大的数据集来理解它，然后**对其进行采样**用于模型训练。其他人总是试图使用最大的数据集进行模型训练。然而，可以从 10 GB 或更少的采样或未采样数据中构建精确的模型。

正确使用采样的关键在于您遵循了适当的统计和理论实践，而不是因为您的 ML 处理因数据量大而崩溃或需要很长时间才能完成而被迫这样做。后者是一种糟糕的做法，会产生劣质的模型，而 H2O.ai 通过允许用海量数据建立模型来克服这一点。

也有数据采样可能无法产生可接受的模型的情况。换句话说，数据科学家可能需要数百千兆字节或一万亿字节或更多的数据来构建一个有价值的模型。以下情况适用:

*   数据科学家不相信采样会产生最佳模型，并认为 lift 中的每一个小增益都保证了完整数据集的使用。
*   数据科学家不希望将数据分割成单独的数据集，因此不希望进行单独的模型构建工作，或者更大的利益相关者群体希望生产中的单个模型可以预测所有数据段，而不是多个模型都可以预测单个数据段。
*   数据是高度多维的，稀疏的，或者两者兼有。在这种情况下，需要大量记录来减少方差和对训练数据集的过度拟合。这种类型的数据集通常用于异常检测、推荐引擎、预测性维护、安全威胁检测、个性化医疗等。值得注意的是，未来将给我们带来越来越多的数据，因此高维度和稀疏数据集将变得更加常见。
*   数据极不平衡。目标变量在数据集中非常罕见，需要大规模数据集来避免对这些不常见记录中的目标变量进行欠拟合、过拟合或加权。
*   数据非常不稳定。收集的每个数据子集都不代表其他子集，因此采样或交叉验证折叠可能不具有代表性。时间序列预测可能对这个问题特别敏感，尤其是当预测类别相对于单个验证数据集而言是高度细化的(例如，每年、每月、每天和每小时)时。

### 迭代的摩擦

建模是一个高度迭代的过程，任何减慢它的事情我们都称之为迭代的摩擦。如前所述，这些原因可能是由于处理大量数据所带来的挑战。它们也可能来自简单的工作流模式，例如在每次迭代之间切换系统，或者启动新的环境来进行迭代。

在单个迭代中的任何缓慢可能看起来是可以接受的，但是当在从项目开始到失败或成功的看似无止境的迭代中倍增时，这种摩擦的时间成本变得显著，并且减少摩擦可能是有价值的。正如我们将在下一节中看到的，缓慢的模型构建延迟了企业中 ML 的主要目标——实现商业价值。

## 业务挑战——将您的模型引入企业生产系统

关于 ML 计划的真相是，直到它们被部署到实时评分环境中，它们才真正实现价值。模型必须满足评估标准并投入生产才能被认为是成功的。在此之前，从商业角度来看，几乎没有什么成就。这似乎有点苛刻，但这是数据科学计划中成功的典型定义。下图将这种想法映射到 ML 生命周期中:

![Figure 1.4 – The ML life cycle value chain
](img/B16721_01_04.jpg)

图 1.4–ML 生命周期价值链

从这个角度来看，迭代的摩擦是一种成本。迭代模型构建所花费的时间是从获得业务结果所花费的时间。换句话说，更少的摩擦意味着更少的时间来构建和部署模型以实现商业价值，而更多的时间来解决其他问题，因此每个季度或每年都会有更多的模型。

从同样的角度来看，**到** **部署一个型号**的时间，出于类似的原因也被视为成本。模型部署步骤可能看起来像是将模型转换到开发运维的简单的一步序列，但通常不是这样。任何使模型更容易、更可重复地部署、记录和治理的东西都有助于企业更快地实现价值。

现在，让我们继续扩展企业利益相关者的更大范围，数据科学家必须与他们一起构建最终实现商业价值的模型。

## 导航挑战——导航企业利益相关者的前景

任何企业中的数据科学家都不是孤立工作的。有多个利益相关者直接参与到 ML 生命周期中，或者更广泛地说，参与到启动和消费 ML 项目的商业周期中。这些利益相关者可能是谁？至少，他们包括为 ML 项目提供资金的业务利益相关者、为数据科学家提供权限和能力的管理员、负责模型部署和支持它的基础设施的开发人员或工程团队成员、其职能受到模型直接影响的营销或销售人员，以及模型的内部或外部消费者的任何其他代表。在监管更严格的行业，如银行、保险或制药行业，可能包括各种审计和风险职能部门的代表或办公室，如数据风险、代码风险、模型风险、法律风险、声誉风险、合规性、外部监管机构等。下图显示了一个总体视图:

![Figure 1.5 – Data scientists working with enterprise stakeholders and processes 
](img/B16721_01_05.jpg)

图 1.5—数据科学家与企业利益相关者和流程合作

因此，利益相关者的互动是复杂的。是什么导致了这种复杂性？显然，工作职能的专业化和条块分割使事情变得复杂，而这又因企业的规模而进一步放大。创建可重复过程和最小化风险的更大动力也有所贡献。解释这种复杂性是另一本书的任务，但它在企业中的现实是不可避免的。对于数据科学家来说，识别、影响、协商、交付并最终与这些不同的利益相关者建立信任的能力是成功的大规模 ML 解决方案的必要条件。

既然我们已经了解了 ML 的生命周期及其成功大规模执行所固有的挑战，那么是时候简要介绍一下 H2O.ai 如何解决这些挑战了。

# H2O.ai 对这些挑战的回答

H2O.ai 提供了软件来建立大规模的 ML 模型，并克服这样做的挑战——大规模的模型建立，大规模的模型部署，以及处理企业利益相关者的关注和沿途的内在摩擦。下图简要描述了这些组件:

![Figure 1.6 – H2O ML at scale
](img/B16721_01_06.jpg)

图 1.6–H2O 毫升秤

本书的后续章节详细阐述了如何在复杂的企业环境中使用这些组件来构建和部署最先进的模型。

让我们试着一眼就理解这些组件:

*   **H2O 核心**:这是一款开源软件，它在 Kubernetes、Hadoop 或 Spark 环境中的指定数量的服务器上分发最先进的 ML 算法和数据操作。数据在内存中跨指定数量的服务器进行分区，并使用它并行运行 ML 算法计算。

这种体系结构将模型构建的水平可伸缩性扩展到数百吉字节或太字节的数据，并且通常在较低的数据量下具有较快的处理时间。数据科学家使用熟悉的 ide、语言和算法，并从底层架构中抽象出来。因此，例如，数据科学家可以在 Jupyter 笔记本上运行 Python 中的 XGBoost 模型，而在 Hadoop 中运行 500 GB 的数据，类似于将数据加载到笔记本中。

H2O 核心通常被称为 H2O 开源，有两种形式:H2O-3 和苏打水，我们将在接下来的章节中详细介绍。H2O 核心可以作为一个缩小的沙盒运行在一台服务器或笔记本上。

*   **H2O 企业 Steam** :这是一个 web UI 或 API，供数据科学家自助供应和管理他们各自的 H2O 核心环境。自我调配包括根据描述数据的用户输入自动计算水平扩展。管理员还使用 Enterprise Steam 来管理用户，包括定义用户资源消耗的界限，以及针对 Hadoop、Spark 或 Kubernetes 配置 H2O 核心集成。
*   **H2O MOJO**:这是一个易于部署的评分工件，可从 H2O 核心构建的模型中导出。MOJOs 是低延迟(通常为< 100 毫秒或更快)的 Java 二进制文件，可以在任何 **Java 虚拟机(JVM)** 上运行，从而在不同的软件系统上提供预测，例如 REST 服务器、数据库客户端、Amazon SageMaker、Kafka 队列、Spark 管道、Hive **用户定义函数(UDF)**和**物联网(IoT)** 设备。
*   **API**:每个组件都有一组丰富的 API，以便您可以自动化工作流，包括**持续集成和持续交付(CI/CD)** 和再训练管道。

本书的重点是在 Enterprise Steam 的帮助下，使用 H2O 核心大规模构建和部署最先进的模型，并将这些模型作为 MOJOs 部署在复杂的企业环境中。

H2O 在规模和 H2O 人工智能云

我们在本书中称为**大规模 H2O**为 H2O 企业 Steam、H2O Core 和 H2O Mojo，因为它解决了本章前面描述的大规模 ML 挑战，特别是通过 H2O Core 为模型构建提供的分布式 ML 可扩展性。

注意，H2O.ai 提供了一个更大的端到端人工智能平台，称为 **H2O 人工智能云**。H2O 人工智能云集成了一个超先进的 AutoML 工具(称为 **H2O 无人驾驶人工智能**)和其他模型构建引擎，一个 MLOps 评分、监控和治理环境(称为 **H2O MLOps** )，以及一个低代码软件开发工具包，或 SDK(称为 **H2O Wave** ，带有 H2O API 挂钩，用于构建发布到**应用商店**的人工智能应用。它也整合了本书中定义的 H2O 规模。

大规模 H2O 可以独立部署，也可以作为 H2O 人工智能云的一部分。作为一个独立的实现，企业级 Steam 实际上并不是必需的，但是由于本书后面阐述的原因，企业级 Steam 被认为是企业级实现所必需的。

这本书的大部分是集中在 H2O 的规模。本书的最后一部分将扩展我们对 H2O 人工智能云的理解，以及 H2O at scale components 如何利用这个更大的集成平台，反之亦然。

# 总结

在这一章中，我们已经为使用 H2O.ai 技术大规模理解和实现 ML 奠定了基础。我们已经在企业环境中定义了多种形式的规模，并从模型构建、模型部署和企业利益相关者的角度阐述了对 ML 的挑战。我们将这些挑战最终锚定到 ML 的最终目标——提供商业价值。最后，我们简要介绍了 H2O at scale 组件，企业使用这些组件来克服这些挑战并实现商业价值。

在下一章，我们将从更详细的技术细节开始理解这些组件，这样我们就可以开始编写代码和进行数据科学研究了。