<html><head/><body>

  
    <title>Bypassing Machine Learning Malware Detectors</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">绕过机器学习恶意软件检测器</h1>
                
            
            
                
<p class="calibre2">在前一章中，您已经了解到，您可以通过使用对抗性的机器学习技术来侵入机器学习模型，并让它们执行恶意活动。在这一章中，我们将探索进一步的技术，比如如何欺骗人工神经网络和深度学习网络。我们将反恶意软件系统规避作为一个案例来研究。</p>
<p class="calibre2">在本章中，我们将介绍以下内容:</p>
<ul class="calibre9">
<li class="calibre25">对抗性深度学习</li>
<li class="calibre25">如何通过生成式对抗网络绕过下一代恶意软件检测器</li>
<li class="calibre10">用强化学习绕过机器学习</li>
</ul>


            

            
        
    



  
    <title>Technical requirements</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">技术要求</h1>
                
            
            
                
<p class="calibre2">你可以在<a href="https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter09" class="calibre8">https://github . com/packt publishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/chapter 09</a>找到这一章的代码文件。</p>


            

            
        
    



  
    <title>Adversarial deep learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">对抗性深度学习</h1>
                
            
            
                
<p class="calibre2">信息安全专业人员正在尽最大努力想出新的技术来检测恶意软件和恶意软件。趋势技术之一是使用机器学习算法的能力来检测恶意软件。另一方面，攻击者和网络罪犯也在想方设法绕过下一代系统。在前一章中，我们看了如何攻击机器学习模型以及如何绕过入侵检测系统。</p>
<p class="calibre2">恶意软件开发人员使用许多技术来绕过机器学习恶意软件检测器。之前，我们探索了一种通过用灰度图像向量训练系统来构建恶意软件分类器的方法。在 UCSB 视觉研究实验室的<strong class="calibre4">搜索和检索恶意软件</strong> ( <strong class="calibre4"> SARVAM </strong>)研究小组所做的演示中，研究人员举例说明，通过改变几个字节，一个模型就可以将恶意软件归类为好软件。攻击者可以通过改变几个字节和像素来绕过恶意软件分类器。在演示中，研究人员使用了 NETSTAT 程序的一个变种，这是一个命令行网络实用工具，可以显示网络连接。在下图中，左侧是一个代表<kbd class="calibre12">NETSTAT.EXE</kbd>的恶意软件，第二个被检测为一个好软件。如您所见，在将两种类型的文件转换为灰度图像并检查两者之间的差异后，两个程序之间的差异是不明显的(36，864 字节中的 88 字节:0.78%):</p>
<div><img src="img/00202.gif" class="calibre217"/></div>
<p class="calibre2">这项技术只是开始；在这一章中，我们将深入探讨如何欺骗他们(机器学习模型，在我们的例子中是恶意软件分类器)进行恶意活动。</p>
<p class="calibre2">前一章是对对抗性机器学习的概述。我们了解了机器学习如何被攻击者绕过。在这一章中，我们将更深入，发现如何绕过基于恶意软件机器学习的检测器；在此之前，我们要学习如何用 Python，开源库，开源项目来忽悠人工神经网络，避免深度学习网络。神经网络可以被<strong class="calibre4">敌对样本</strong>欺骗。对立样本被用作神经网络的输入，以影响学习结果。由伊恩·j·古德费勒、黄邦贤·施伦斯和克里斯蒂安·塞格迪(谷歌)进行的一项名为<em class="calibre16">解释和利用敌对网络的开创性研究项目</em>表明，少量精心构建的噪声可以欺骗神经网络，使其认为输入的图像是一只长臂猿而不是一只熊猫，置信度为 99.3%。神经网络原本认为提供的图像是熊猫，有 57.7%的置信度，这是真的；但在愚弄网络之后，第二个示例中的情况并非如此:</p>
<div><img src="img/00203.jpeg" class="calibre218"/></div>
<div><p class="calibre2">许多电子设备和系统依赖深度学习作为保护机制，包括人脸识别；想象一下攻击者可以做些什么来攻击他们并获得对关键系统的未授权访问。</p>
<p class="calibre2">现在，让我们试着愚弄一个神经网络。我们将使用著名的 MNIST 数据集来欺骗手写数字检测系统。在<a href="part0081.html#2D7TI0-49a67f1d6e7843d3b2296f38e3fe05f5" class="calibre8">第 4 章</a>、<em class="calibre16">深度学习恶意软件检测</em>中，我们学习了如何构建一个。为了演示，我们要愚弄一个由迈克尔·尼尔森设计的预先训练好的神经网络。他使用了 50，000 张训练图像和 10，000 张测试图像。或者，你可以简单地使用你自己的神经网络。你可以在本章的 GitHub 资源库中找到培训信息。文件名为<kbd class="calibre12">trained_network.pkl</kbd>；你还会找到 MNIST 档案(<kbd class="calibre12">mnist.pkl.gz</kbd>):</p>
<pre class="calibre17"><strong class="calibre1">import network.network as network</strong><br class="title-page-name"/><strong class="calibre1">import network.mnist_loader as mnist_loader</strong><br class="title-page-name"/><strong class="calibre1"># To serialize data</strong><br class="title-page-name"/><strong class="calibre1">import pickle</strong><br class="title-page-name"/><strong class="calibre1">import matplotlib.pyplot as plt</strong><br class="title-page-name"/><strong class="calibre1">import numpy as np</strong></pre></div>
<p class="calibre2">让我们检查一下模型是否训练有素。加载<kbd class="calibre12">pickle</kbd>文件。用<kbd class="calibre12">pickle.load()</kbd>加载数据，并识别训练、验证和测试数据:</p>
<pre class="calibre17"><strong class="calibre1">Model = pickle.load( open( "trained_network.pkl", "rb" ) )    trainData, valData, testData =mnist_loader.load_data_wrapper()</strong></pre>
<p class="calibre2">例如，为了检查数字 2，我们将选择<kbd class="calibre12">test_data[1][0]</kbd>:</p>
<pre class="calibre17"><strong class="calibre1">&gt;&gt;&gt; data = test_data[1][0]</strong><br class="title-page-name"/><strong class="calibre1">&gt;&gt;&gt; activations = Model.feedforward(data)</strong><br class="title-page-name"/><strong class="calibre1">&gt;&gt;&gt; prediction = np.argmax(activations)<br class="title-page-name"/></strong></pre>
<p class="calibre2">下面的屏幕截图说明了前面的代码:</p>
<div><img src="img/00204.gif" class="calibre219"/></div>
<p class="calibre2">使用<kbd class="calibre12">matplotlib.pyplot (plt)</kbd>绘制结果以便进一步检查:</p>
<pre class="calibre17"><strong class="calibre1">&gt;&gt;&gt; plt.imshow(data.reshape((28,28)), cmap='Greys')</strong><br class="title-page-name"/><strong class="calibre1">&gt;&gt;&gt; plt.show()</strong></pre>
<p class="calibre2">如您所见，我们生成了数字<strong class="calibre4"> 2 </strong>，因此模型训练良好:</p>
<p class="calibre2"> </p>
<div><img src="img/00205.gif" class="calibre220"/></div>
<p class="calibre2">一切都设置正确。现在我们要用两种类型的攻击来攻击神经网络:<strong class="calibre4">针对性</strong>和<strong class="calibre4">非针对性。</strong></p>
<p class="calibre2">对于非针对性攻击，我们要生成一个对抗性样本，让网络给出一定的输出，例如<em class="calibre16"> 6 </em>:</p>
<div><img class="fm-editor-equation8" src="img/00206.gif"/></div>
<p class="calibre2">在这次攻击中，我们希望神经网络认为输入的图像是<em class="calibre16"> 6 </em>。目标图像(姑且称之为<em class="calibre16"> X) </em>是一个<em class="calibre16"> 784 </em>维度的向量，因为图像维度是<em class="calibre16"> 28×28 </em>像素。我们的目标是找到一个向量<kbd class="calibre12"><em class="calibre26">⃗x</em></kbd>，使成本<em class="calibre16"> C，</em>最小化，从而产生神经网络预测为我们的目标标签的图像。成本函数<em class="calibre16">C</em>定义如下:</p>
<div><img class="fm-editor-equation9" src="img/00207.jpeg"/></div>
<p class="calibre2">以下代码块是一个导数函数的实现:</p>
<pre class="calibre17">def input_derivative(net, x, y):<br class="title-page-name"/>    """ Calculate derivatives wrt the inputs"""<br class="title-page-name"/>    nabla_b = [np.zeros(b.shape) for b in net.biases]<br class="title-page-name"/>    nabla_w = [np.zeros(w.shape) for w in net.weights]<br class="title-page-name"/>    <br class="title-page-name"/>    # feedforward<br class="title-page-name"/>    activation = x<br class="title-page-name"/>    activations = [x] # list to store all the activations, layer by layer<br class="title-page-name"/>    zs = [] # list to store all the z vectors, layer by layer<br class="title-page-name"/>    for b, w in zip(net.biases, net.weights):<br class="title-page-name"/>        z = np.dot(w, activation)+b<br class="title-page-name"/>        zs.append(z)<br class="title-page-name"/>        activation = sigmoid(z)<br class="title-page-name"/>        activations.append(activation)<br class="title-page-name"/>        <br class="title-page-name"/>    # backward pass<br class="title-page-name"/>    delta = net.cost_derivative(activations[-1], y) * \<br class="title-page-name"/>        sigmoid_prime(zs[-1])<br class="title-page-name"/>    nabla_b[-1] = delta<br class="title-page-name"/>    nabla_w[-1] = np.dot(delta, activations[-2].transpose())<br class="title-page-name"/><br class="title-page-name"/>    for l in xrange(2, net.num_layers):<br class="title-page-name"/>        z = zs[-l]<br class="title-page-name"/>        sp = sigmoid_prime(z)<br class="title-page-name"/>        delta = np.dot(net.weights[-l+1].transpose(), delta) * sp<br class="title-page-name"/>        nabla_b[-l] = delta<br class="title-page-name"/>        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())<br class="title-page-name"/>    return net.weights[0].T.dot(delta)</pre>
<p class="calibre2">为了生成对抗性样本，我们需要设定目标:</p>
<pre class="calibre17">goal = np.zeros((10, 1))<br class="title-page-name"/>goal[n] = 1</pre>
<p class="calibre2">为梯度下降初始化创建随机图像，如下所示:</p>
<pre class="calibre17">x = np.random.normal(.5, .3, (784, 1))</pre>
<p class="calibre2">计算梯度下降，如下所示:</p>
<pre class="calibre17">for i in range(steps):<br class="title-page-name"/>        # Calculate the derivative<br class="title-page-name"/>        d = input_derivative(net,x,goal)       <br class="title-page-name"/>        x -= eta * d       <br class="title-page-name"/>    return x<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">现在，您可以生成示例:</p>
<pre class="calibre17">a = adversarial(net, n, 1000, 1)<br class="title-page-name"/>x = np.round(net.feedforward(a), 2)<br class="title-page-name"/>Print ("The input is:", str(x))<br class="title-page-name"/>Print ("The prediction is", str(np.argmax(x)))</pre>
<p class="calibre2">绘制对立样本，如下所示:</p>
<pre class="calibre17">plt.imshow(a.reshape(28,28), cmap='Greys')<br class="title-page-name"/>plt.show()</pre>
<div><img src="img/00208.gif" class="calibre221"/></div>
<p class="calibre2">在目标攻击中，我们使用相同的技术和相同的代码，但是我们在代价函数中添加了一个新的项。因此，它将如下:</p>
<div><img class="fm-editor-equation10" src="img/00209.jpeg"/></div>


            

            
        
    



  
    <title>Foolbox</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">傻瓜盒子</h1>
                
            
            
                
<p class="calibre2">Foolbox 是一个 Python 工具箱，用于测试机器学习模型的健壮性。它受到许多框架的支持，包括:</p>
<ul class="calibre9">
<li class="calibre25">张量流</li>
<li class="calibre25">PyTorch</li>
<li class="calibre25">Theano</li>
<li class="calibre25">克拉斯</li>
<li class="calibre25">千层面</li>
<li class="calibre25">MXNet</li>
</ul>
<p class="calibre2">要安装 Foolbox，请使用<kbd class="calibre12">pip</kbd>实用程序:</p>
<pre class="calibre17"><strong class="calibre1">pip install foolbox</strong></pre>
<div><img src="img/00210.jpeg" class="calibre222"/></div>
<p class="calibre2">以下是一些愚蠢的攻击:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">基于梯度的攻击</strong>:通过线性化输入周围的损耗，<em class="calibre26"> x </em></li>
<li class="calibre10"><strong class="calibre1">梯度符号攻击(FGSM) </strong>:通过计算梯度，<em class="calibre26"> g(x0) </em>，一次，然后求最小步长</li>
<li class="calibre10"><strong class="calibre1">迭代梯度攻击</strong>:通过沿梯度方向的小步长最大化损失，<em class="calibre26"> g(x) </em></li>
<li class="calibre10"><strong class="calibre1">迭代梯度符号攻击</strong>:通过最大化沿上升方向的小步长的损失，<em class="calibre26">符号(g(x)) </em></li>
<li class="calibre10"><strong class="calibre1"> DeepFool L2Attack </strong>:通过计算，对于每个类，到达类边界所需的最小距离<em class="calibre26"> d(ℓ，ℓ0) </em></li>
<li class="calibre10"><strong class="calibre1">deep fool L∞攻击</strong>:类似 L2 攻击，但是最小化了<em class="calibre26"> L∞范数</em></li>
<li class="calibre10"><strong class="calibre1">基于雅克比的显著性图攻击</strong>:通过计算每个输入特征的显著性得分</li>
<li class="calibre10"><strong class="calibre1">单像素攻击</strong>:将单个像素设置为白色或黑色</li>
</ul>
<p class="calibre2">要使用 Foolbox 实施攻击，请使用以下代码:</p>
<pre class="calibre17"><strong class="calibre1">import foolbox</strong><br class="title-page-name"/><strong class="calibre1">import keras</strong><br class="title-page-name"/><strong class="calibre1">import numpy as np</strong><br class="title-page-name"/><strong class="calibre1">from keras.applications.resnet50 import ResNet50</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">keras.backend.set_learning_phase(0)</strong><br class="title-page-name"/><strong class="calibre1">kmodel = ResNet50(weights='imagenet')</strong><br class="title-page-name"/><strong class="calibre1">preprocessing = (np.array([104, 116, 123]), 1)</strong><br class="title-page-name"/><strong class="calibre1">fmodel = foolbox.models.KerasModel(kmodel, bounds=(0, 255), preprocessing=preprocessing)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">image, label = foolbox.utils.imagenet_example()</strong><br class="title-page-name"/><strong class="calibre1">attack = foolbox.attacks.FGSM(fmodel)</strong><br class="title-page-name"/><strong class="calibre1">adversarial = attack(image[:, :, ::-1], label)</strong></pre>
<p>如果收到错误<kbd class="calibre151">ImportError('`load_weights` requires h5py.')</kbd>，通过安装<strong class="calibre211"> h5py </strong>库(<kbd class="calibre151">pip install h5py</kbd>)来解决。</p>
<p class="calibre2">要绘制结果，请使用以下代码:</p>
<pre class="calibre17"><strong class="calibre1">import matplotlib.pyplot as plt</strong><br class="title-page-name"/><strong class="calibre1">plt.figure()</strong><br class="title-page-name"/><strong class="calibre1">plt.subplot(1, 3, 1)</strong><br class="title-page-name"/><strong class="calibre1">plt.title('Original')</strong><br class="title-page-name"/><strong class="calibre1">plt.imshow(image / 255)  </strong><br class="title-page-name"/><strong class="calibre1">plt.axis('off')</strong><br class="title-page-name"/><strong class="calibre1">plt.subplot(1, 3, 2)</strong><br class="title-page-name"/><strong class="calibre1">plt.title('Adversarial')</strong><br class="title-page-name"/><strong class="calibre1">plt.imshow(adversarial[:, :, ::-1] / 255)  # ::-1 to convert BGR to RGB</strong><br class="title-page-name"/><strong class="calibre1">plt.axis('off')</strong><br class="title-page-name"/><strong class="calibre1">plt.subplot(1, 3, 3)</strong><br class="title-page-name"/><strong class="calibre1">plt.title('Difference')</strong><br class="title-page-name"/><strong class="calibre1">difference = adversarial[:, :, ::-1] - image</strong><br class="title-page-name"/><strong class="calibre1">plt.imshow(difference / abs(difference).max() * 0.2 + 0.5)</strong><br class="title-page-name"/><strong class="calibre1">plt.axis('off')</strong><br class="title-page-name"/><strong class="calibre1">plt.show()</strong></pre>
<div><img src="img/00211.jpeg" class="calibre224"/></div>


            

            
        
    



  
    <title>Deep-pwning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">深冲</h1>
                
            
            
                
<p class="calibre2">Deep-pwning 是一个轻量级框架，用于实验机器学习模型，目标是评估它们对有动机的对手的鲁棒性。它被称为机器学习的<strong class="calibre4">元程序</strong>。你可以从位于 https://github.com/cchio/deep-pwning 的 GitHub 仓库中克隆它。</p>
<p class="calibre2">不要忘记安装所有的要求:</p>
<pre class="calibre17"><strong class="calibre1">pip install -r requirements.txt<br class="title-page-name"/></strong></pre>
<p class="calibre2">以下是使用深度挖掘所需的 Python 库:</p>
<ul class="calibre9">
<li class="calibre10">张量流 0.8.0</li>
<li class="calibre10">Matplotlib &gt;= 1.5.1</li>
<li class="calibre10">数字&gt; = 1.11.1</li>
<li class="calibre10">熊猫&gt; = 0.18.1</li>
<li class="calibre10">六&gt; = 1.10.0</li>
</ul>


            

            
        
    



  
    <title>EvadeML</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">EvadeML</h1>
                
            
            
                
<p class="calibre2">EvadeML(<a href="https://evademl.org/" class="calibre8">https://evademl.org</a>)是一个基于遗传编程的进化框架，用于自动寻找逃避基于机器学习的恶意软件分类器检测的变体。它是由弗吉尼亚大学的机器学习小组和安全研究小组开发的。</p>
<p class="calibre2">要下载 EvadeML，从<a href="https://github.com/uvasrg/EvadeML" class="calibre8">https://github.com/uvasrg/EvadeML</a>克隆它。</p>
<p class="calibre2">要安装 EvadeML，您需要安装这些必需的工具:</p>
<ul class="calibre9">
<li class="calibre25">用于解析 pdf 的 pdfrw 的修改版本:<a href="https://github.com/mzweilin/pdfrw" class="calibre8">https://github.com/mzweilin/pdfrw</a></li>
<li class="calibre25">布谷鸟沙盒 v1.2，如神谕:<a href="https://github.com/cuckoosandbox/cuckoo/releases/tag/1.2" class="calibre8">https://github.com/cuckoosandbox/cuckoo/releases/tag/1.2</a></li>
<li class="calibre25">目标分类器 PD frate-Mimi cus:<a href="https://github.com/srndic/mimicus" class="calibre8">https://github.com/srndic/mimicus</a></li>
<li class="calibre25">目标分类器 Hidost:<a href="https://github.com/srndic/hidost" class="calibre8">https://github.com/srndic/hidost</a></li>
</ul>
<p class="calibre2">要配置项目，请复制模板，并使用编辑器对其进行配置:</p>
<pre class="calibre17"><strong class="calibre1">cp project.conf.template project.conf</strong><br class="title-page-name"/><strong class="calibre1">Vi  project.conf</strong></pre>
<p class="calibre2">在运行主程序<kbd class="calibre12">./gp.py</kbd>之前，运行带有预定义恶意软件签名的集中式检测代理，如文档中所示:</p>
<pre class="calibre17"><strong class="calibre1">./utils/detection_agent_server.py ./utils/36vms_sigs.pickle</strong></pre>
<p class="calibre2">选择几个良性 PDF 文件:</p>
<pre class="calibre17"><strong class="calibre1">./utils/generate_ext_genome.py [classifier_name] [benign_sample_folder] [file_number]</strong></pre>
<p class="calibre2">要添加新的分类器来规避，只需在<kbd class="calibre12">./classifiers/</kbd>中添加一个包装器。</p>


            

            
        
    



  
    <title>Bypassing next generation malware detectors with generative adversarial networks</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">利用生成性对抗网络绕过下一代恶意软件检测器</h1>
                
            
            
                
<p class="calibre2">2014 年，Ian Goodfellow、Yoshua Bengio 和他们的团队提出了一个名为<strong class="calibre4">生成对抗网络(GAN) </strong>的框架。生成敌对网络具有从随机噪声中生成图像的能力。例如，我们可以训练一个生成网络来从 MNIST 数据集中生成手写数字的图像。</p>
<p class="calibre2">生成对抗网络由两个主要部分组成:一个<strong class="calibre4">生成器</strong>和一个<strong class="calibre4">鉴别器</strong>。</p>


            

            
        
    



  
    <title>The generator</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">发电机</h1>
                
            
            
                
<p class="calibre2">生成器将潜在样本作为输入；它们是随机生成的数字，并被训练生成图像:</p>
<div><img src="img/00212.jpeg" class="calibre225"/></div>
<p class="calibre2">例如，为了生成一个手写数字，生成器将是一个完全连接的网络，它获取潜在样本并生成<kbd class="calibre12">784</kbd>数据点，将它们重塑为<em class="calibre16"> 28x28 </em>像素图像(MNIST 数字)。强烈建议使用<kbd class="calibre12">tanh</kbd>作为激活功能:</p>
<pre class="calibre17">generator = Sequential([<br class="title-page-name"/>Dense(128, input_shape=(100,)),<br class="title-page-name"/>LeakyReLU(alpha=0.01),<br class="title-page-name"/>Dense(784),<br class="title-page-name"/>Activation('tanh')<br class="title-page-name"/>], name='generator')</pre>


            

            
        
    



  
    <title>The discriminator</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">鉴别器</h1>
                
            
            
                
<p class="calibre2">鉴别器只是一个用监督学习技术训练的分类器，用来检查图像是真的(<kbd class="calibre12">1</kbd>)还是假的(<kbd class="calibre12">0</kbd>)。它由 MNIST 数据集和生成器样本训练。鉴别器将 MNIST 数据分类为真实的，而发生器样本分类为假的:</p>
<pre class="calibre17">discriminator = Sequential([<br class="title-page-name"/>Dense(128, input_shape=(784,)),<br class="title-page-name"/>LeakyReLU(alpha=0.01),<br class="title-page-name"/>Dense(1),<br class="title-page-name"/>Activation('sigmoid')], name='discriminator')</pre>
<p class="calibre2">通过连接两个网络，生成器和鉴别器，我们产生了一个生成性对抗网络:</p>
<pre class="calibre17">gan = Sequential([<br class="title-page-name"/>generator,<br class="title-page-name"/>discriminator])</pre>
<p class="calibre2">这是一个生成性对抗网络的高级表示:</p>
<div><img src="img/00213.jpeg" class="calibre226"/></div>
<p class="calibre2">为了训练 GAN，我们需要训练发生器(在进一步的步骤中鉴别器被设置为不可训练的)；在训练中，反向传播更新生成器的权重以生成逼真的图像。因此，为了训练 GAN，我们使用以下步骤作为循环:</p>
<ul class="calibre9">
<li class="calibre25">用真实图像训练鉴别器(鉴别器在这里是可训练的)</li>
<li class="calibre25">将鉴别器设置为不可训练</li>
<li class="calibre25">训练发电机</li>
</ul>
<p class="calibre2">训练循环将会发生，直到两个网络都不能被进一步改进。</p>
<p class="calibre2">要使用 Python 构建 GAN，请使用以下代码:</p>
<pre class="calibre17">import pickle as pkl<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>batch_size = 100<br class="title-page-name"/>epochs = 100<br class="title-page-name"/>samples = []<br class="title-page-name"/>losses = []<br class="title-page-name"/>saver = tf.train.Saver(var_list=g_vars)<br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/>    sess.run(tf.global_variables_initializer())<br class="title-page-name"/>    for e in range(epochs):<br class="title-page-name"/>        for ii in range(mnist.train.num_examples//batch_size):<br class="title-page-name"/>            batch = mnist.train.next_batch(batch_size)<br class="title-page-name"/>            <br class="title-page-name"/>            <br class="title-page-name"/>            batch_images = batch[0].reshape((batch_size, 784))<br class="title-page-name"/>            batch_images = batch_images*2 - 1<br class="title-page-name"/>            <br class="title-page-name"/> <br class="title-page-name"/>            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))<br class="title-page-name"/>            <br class="title-page-name"/>     <br class="title-page-name"/>            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})<br class="title-page-name"/>            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})<br class="title-page-name"/>        <br class="title-page-name"/>       <br class="title-page-name"/>        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})<br class="title-page-name"/>        train_loss_g = g_loss.eval({input_z: batch_z})<br class="title-page-name"/>            <br class="title-page-name"/>        print("Epoch {}/{}...".format(e+1, epochs),<br class="title-page-name"/>              "Discriminator Loss: {:.4f}...".format(train_loss_d),<br class="title-page-name"/>              "Generator Loss: {:.4f}".format(train_loss_g))    <br class="title-page-name"/>        <br class="title-page-name"/>        losses.append((train_loss_d, train_loss_g))<br class="title-page-name"/>        <br class="title-page-name"/>       <br class="title-page-name"/>        sample_z = np.random.uniform(-1, 1, size=(16, z_size))<br class="title-page-name"/>        gen_samples = sess.run(<br class="title-page-name"/>                       generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha),<br class="title-page-name"/>                       feed_dict={input_z: sample_z})<br class="title-page-name"/>        samples.append(gen_samples)<br class="title-page-name"/>        saver.save(sess, './checkpoints/generator.ckpt')<br class="title-page-name"/>with open('train_samples.pkl', 'wb') as f:<br class="title-page-name"/>    pkl.dump(samples, f)</pre>
<div><img src="img/00214.gif" class="calibre227"/></div>
<p class="calibre2">为了用 Python 构建一个 GAN，我们将使用 NumPy 和 TensorFlow。</p>


            

            
        
    



  
    <title>MalGAN</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">马尔甘</h1>
                
            
            
                
<p class="calibre2">为了生成恶意软件样本来攻击机器学习模型，攻击者现在使用 GANs 来实现他们的目标。使用我们之前讨论的相同技术(生成器和鉴别器)，网络犯罪分子对下一代反恶意软件系统进行攻击，甚至不知道所使用的机器学习技术(黑盒攻击)。其中一种技术是 MalGAN，它出现在由机器感知重点实验室(MOE)和机器智能部的胡玮炜和 Ying Tan 进行的名为<em class="calibre16">基于 GAN 为黑盒攻击生成对抗性恶意软件示例的研究项目中。MalGAN 的架构如下:</em></p>
<div><img src="img/00215.jpeg" class="calibre228"/></div>
<p class="calibre2">生成器通过将恶意软件(特征向量<em class="calibre16"> m </em>和噪声向量<em class="calibre16"> z、</em>作为输入来创建敌对的恶意软件样本。替代检测器是一个多层前馈神经网络，它采用一个程序特征向量，<em class="calibre16"> X，</em>作为输入。它将程序分为良性程序和恶意程序。</p>
<p class="calibre2">为了训练生成性对抗网络，研究人员使用了这种算法:</p>
<pre class="calibre17">While not converging do:<br class="title-page-name"/>    Sample a minibatch of Malware M<br class="title-page-name"/>    Generate adversarial samples M' from the generator<br class="title-page-name"/>    Sample a minibatch of Goodware B<br class="title-page-name"/>    Label M' and B using the detector<br class="title-page-name"/>    Update the weight of the detector<br class="title-page-name"/>    Update the generator weights<br class="title-page-name"/>End while</pre>
<p class="calibre2">生成的许多样本可能不是有效的 PE 文件。为了保留突变和格式，系统需要一个沙箱来确保功能得以保留。</p>
<p class="calibre2">生成性对抗性网络训练不能简单地产生巨大的效果；这就是为什么许多黑客需要达到更好的效果。一些技巧被 Soumith Chintala、Emily Denton、Martin Arjovsky 和迈克尔·马修引入，以获得改进的结果:</p>
<ul class="calibre9">
<li class="calibre25">归一化<em class="calibre26"> -1 </em>和<em class="calibre26"> 1 </em>之间的图像</li>
<li class="calibre25">使用最大对数<em class="calibre26"> D，</em>作为损失函数，优化<em class="calibre26"> G </em>而不是最小值(<em class="calibre26">对数 1-D </em></li>
<li class="calibre25">从高斯分布取样，而不是均匀分布</li>
<li class="calibre25">为真货和假货构建不同的小批量</li>
<li class="calibre25">避免使用 ReLU 和 MaxPool，而是使用 LeakyReLU 和 Average Pooling</li>
<li class="calibre25">使用<strong class="calibre1">深度卷积 GAN </strong> ( <strong class="calibre1"> DCGAN </strong>)，如果可能的话</li>
<li class="calibre25">使用<kbd class="calibre12">ADAM</kbd>优化器</li>
</ul>


            

            
        
    



  
    <title>Bypassing machine learning with reinforcement learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">用强化学习绕过机器学习</h1>
                
            
            
                
<p class="calibre2">在前面的技术中，我们注意到，如果我们正在生成对立的样本，特别是如果结果是二进制的，我们将面临一些问题，包括生成无效的样本。信息安全研究人员提出了一种新技术，可以通过强化学习绕过机器学习反恶意软件系统。</p>


            

            
        
    



  
    <title>Reinforcement learning</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">强化学习</h1>
                
            
            
                
<p class="calibre2">之前(尤其是在第一章)，我们探讨了不同的机器学习模型:监督、半监督、非监督和强化模型。强化机器学习模型是构建智能机器的重要途径。在强化学习中，代理通过与环境交互，从经验中学习；它根据状态和奖励函数选择最佳决策:</p>
<div><img src="img/00216.jpeg" class="calibre229"/></div>
<p class="calibre2">强化学习的一个著名例子是基于人工智能的雅达利突破。在这种情况下，环境包括以下内容:</p>
<ul class="calibre9">
<li class="calibre25">球和砖块</li>
<li class="calibre25">移动桨(左或右)</li>
<li class="calibre25">消除砖块的奖励</li>
</ul>
<p class="calibre2">下图高度概括了用于教授模型如何玩 Atari 突破的强化模型:</p>
<div><img src="img/00217.jpeg" class="calibre230"/></div>
<p class="calibre2">以 Atari Breakout 环境作为类比来学习如何避开反恶意软件系统，我们的环境将如下:</p>
<div><img src="img/00218.jpeg" class="calibre231"/></div>
<p class="calibre2">对于代理，它利用环境状态(一般文件信息、标题信息、导入和导出的函数、字符串等)来优化其性能，并从反病毒报告和结果操作(创建入口点和新部分、修改部分等)中获得回报输入。换句话说，执行和学习代理需要两个输入(状态和奖励)。</p>
<p class="calibre2">作为我们讨论的概念的实现，信息安全专业人员在 OpenAI 环境中工作，构建一个可以使用强化学习技术逃避检测的恶意软件。其中一个环境是<strong class="calibre4">健身房恶意软件</strong>。这个伟大的环境是由 endgame 开发的。</p>
<p class="calibre2">OpenAI gym 包含一个开源 Python 框架，由一家名为 open AI(<a href="https://openai.com/" class="calibre8">https://openai.com/</a>)的非营利人工智能研究公司开发，用于开发和评估强化学习算法。要安装 OpenAI Gym，请使用以下代码(您需要安装 Python 3.5 以上版本):</p>
<pre class="calibre17"><strong class="calibre1">git clone https://github.com/openai/gym</strong><br class="title-page-name"/><strong class="calibre1">cd gym</strong><br class="title-page-name"/><strong class="calibre1">pip install -e</strong></pre>
<p class="calibre2">OpenAI Gym 加载了预先制作的环境。你可以在 http://gym.openai.com/envs/查看所有可用的环境:</p>
<div><img src="img/00219.jpeg" class="calibre232"/></div>
<pre>CartPole-v0 environment:</pre>
<pre class="calibre17">import gym<br class="title-page-name"/> env = gym.make('CartPole-v0')<br class="title-page-name"/> env.reset()<br class="title-page-name"/> for _ in range(1000): # run for 1000 steps<br class="title-page-name"/>    env.render()<br class="title-page-name"/>    action = env.action_space.sampe() # pick a random action<br class="title-page-name"/>    env.step(action) # take action</pre>
<p class="calibre2">要使用 Gym-malware 环境，您需要安装 Python 3.6 和一个名为 Instrument Executable Formats 的库，名为<kbd class="calibre12">LIEF</kbd>。您可以通过键入以下内容来添加它:</p>
<pre class="calibre17"><strong class="calibre1">pip install https://github.com/lief-project/LIEF/releases/download/0.7.0/linux_lief-0.7.0_py3.6.tar.gz</strong></pre>
<p class="calibre2">从 https://github.com/endgameinc/gym-malware 下载恶意软件。<a href="https://github.com/endgameinc/gym-malware" class="calibre8">将已安装的健身房恶意软件环境移至<kbd class="calibre233">gym_malware/gym_malware/envs/utils/samples/</kbd>。</a></p>
<p class="calibre2">要检查样本是否在正确的目录中，请键入以下内容:</p>
<pre class="calibre17"><strong class="calibre1">python test_agent_chainer.py</strong></pre>
<p class="calibre2">此环境中可用的操作如下:</p>
<ul class="calibre9">
<li class="calibre10"><kbd class="calibre12">append_zero</kbd></li>
<li class="calibre10"><kbd class="calibre12">append_random_ascii</kbd></li>
<li class="calibre10"><kbd class="calibre12">append_random_bytes</kbd></li>
<li class="calibre10"><kbd class="calibre12">remove_signature</kbd></li>
<li class="calibre10"><kbd class="calibre12">upx_pack</kbd></li>
<li class="calibre10"><kbd class="calibre12">upx_unpack</kbd></li>
<li class="calibre10"><kbd class="calibre12">change_section_names_from_list</kbd></li>
<li class="calibre10"><kbd class="calibre12">change_section_names_to random</kbd></li>
<li class="calibre10"><kbd class="calibre12">modify_export</kbd></li>
<li class="calibre10"><kbd class="calibre12">remove_debug</kbd></li>
<li class="calibre10"><kbd class="calibre12">break_optional_header_checksum</kbd></li>
</ul>


            

            
        
    



  
    <title>Summary</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">摘要</h1>
                
            
            
                
<p class="calibre2">在这一章中，我们继续了学习如何绕过机器学习模型的旅程。在前一章，我们发现了对抗性机器学习；在这一延续中，我们探索了对抗性深度学习以及如何欺骗深度学习网络。我们查看了一些真实世界的案例，以了解如何通过使用最先进的技术逃离反恶意软件系统。在下一章也是最后一章，我们将获得更多的知识，学习如何构建健壮的模型。</p>


            

            
        
    



  
    <title>Questions</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">问题</h1>
                
            
            
                
<ol class="calibre13">
<li value="1" class="calibre10">生成性对抗网络的组成部分是什么？</li>
<li value="2" class="calibre10">生成器和鉴别器有什么区别？</li>
<li value="3" class="calibre10">我们如何确保恶意软件对抗样本在生成时仍然有效？</li>
<li value="4" class="calibre10">做一点研究，然后简要解释如何检测敌对样本。</li>
<li value="5" class="calibre10">强化学习和深度学习有什么区别？</li>
<li value="6" class="calibre10">监督学习和强化学习有什么区别？</li>
<li value="7" class="calibre10">一个 agent 如何在强化学习中学习？</li>
</ol>


            

            
        
    



  
    <title>Further reading</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  


  
        

                            
                    <h1 class="header-title" id="calibre_pb_0">进一步阅读</h1>
                
            
            
                
<p class="calibre2">以下资源包含大量信息:</p>
<ul class="calibre9">
<li class="calibre10"><em class="calibre26">解释和治理敌对样本</em>:【https://arxiv.org/pdf/1412.6572.pdf T2】</li>
<li class="calibre10">【https://arxiv.org/pdf/1611.02770.pdf】钻研可转移的对抗性例子和黑盒攻击:<a href="https://arxiv.org/pdf/1611.02770.pdf" class="calibre8">T7】</a></li>
<li class="calibre10"><em class="calibre26">fool box——一个测试机器学习模型健壮性的 Python 工具箱</em>:<a href="https://arxiv.org/pdf/1707.04131.pdf" class="calibre8">https://arxiv.org/pdf/1707.04131.pdf</a></li>
<li class="calibre10"><em class="calibre26">傻瓜盒子</em>GitHub:<a href="https://github.com/bethgelab/foolbox" class="calibre8">https://github.com/bethgelab/foolbox</a></li>
<li class="calibre10"><em class="calibre26">基于 GAN 生成对抗性恶意软件黑盒攻击实例</em>:<a href="https://arxiv.org/pdf/1702.05983.pdf" class="calibre8">https://arxiv.org/pdf/1702.05983.pdf</a></li>
<li class="calibre10"><em class="calibre26">恶意软件图片:可视化和自动分类【https://arxiv.org/pdf/1702.05983.pdf T21】:<a href="https://arxiv.org/pdf/1702.05983.pdf" class="calibre8"/></em></li>
<li class="calibre10"><em class="calibre26"> SARVAM:恶意软件的搜索和检索</em>:<a href="http://vision.ece.ucsb.edu/sites/vision.ece.ucsb.edu/files/publications/2013_sarvam_ngmad_0.pdf" class="calibre8">http://vision . ECE . ucsb . edu/sites/vision . ECE . ucsb . edu/files/publications/2013 _ sar VAM _ ng mad _ 0 . pdf</a></li>
<li class="calibre10"><em class="calibre26"> SigMal:基于静态信号处理的恶意软件分类</em>:<a href="http://vision.ece.ucsb.edu/publications/view_abstract.cgi?416" class="calibre8">http://vision.ece.ucsb.edu/publications/view_abstract.cgi?416 </a></li>
</ul>


            

            
        
    
</body></html>