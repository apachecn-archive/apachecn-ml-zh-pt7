

# 三、集成学习中的表现

到目前为止，我们了解到没有两个模型会给出相同的结果。换句话说，不同的数据或算法组合会产生不同的结果。这种结果可能对某个特定组合有利，但对另一个组合就不那么有利了。如果我们有一个模型，试图将这些组合考虑在内，并得出一个通用的更好的结果，会怎么样？这被称为**集合模型**。

在这一章中，我们将学习一些关于集合建模的概念，如下所示:

*   制袋材料
*   随机森林
*   提升
*   梯度推进
*   参数优化



# 什么是集成学习？

有时，一个机器学习模型对于某个场景或用例来说不够好，因为它可能不会给你期望的准确度、召回率和精确度。因此，多个学习模型——或模型集合捕捉数据的模式并给出更好的输出。

举个例子，假设我们正试图决定一个我们夏天想去的地方。通常，如果我们正在计划一次旅行，关于这个地方的建议会从各个角落涌来。也就是说，这些建议可能来自我们的家人、网站、朋友和旅行社，然后我们必须根据我们过去的良好经历做出决定:

*   **家人**:比方说，每当我们咨询家人并听取他们的意见时，他们有 60%的机会被证明是正确的，我们最终在旅行中获得了良好的体验。
*   **朋友**:同样，如果我们听朋友的话，他们会建议我们去一些可能会有美好经历的地方。在这些情况下，50%的案例都有良好的体验。
*   旅游网站:旅游网站是另一个来源，在那里我们可以获得大量关于去哪里旅游的信息。如果我们选择接受他们的建议，有 35%的可能性他们是对的，我们有一个很好的经历。
*   旅行社:如果我们先去旅行社核实一下，他们可能会提供另一条建议和信息。根据我们过去的经验，我们发现他们在 45%的情况下是正确的。

然而，我们必须积累所有之前的输入并做出决定，因为迄今为止没有任何来源是 100%正确的。如果我们将这些结果结合起来，精确度方案将如下:

```
1 - (60% * 50% * 35% * 45%)                                                                                                                                                                              1- 0.04725 = 0.95275

# Accuracy is close to 95%.
```

由此，我们能够看到集合建模的影响。



# 集成方法

主要有三种建立集合模型的方法，即**打包**、**提升**和**堆叠**:

![](img/f2ec24a4-2d97-43aa-8d0a-71c5a58a59ff.png)

我们将逐一讨论每种方法。然而，在我们进入这个之前，我们需要了解什么是自举，它为**装袋**和**提升**奠定了基础。



# 拔靴带

Bootstrapping 是一种统计技术，用于根据替换抽取的样本对总体参数进行推断，并对这些结果进行平均。在替换抽样的情况下，样本被一个接一个地抽取，一旦从总体中抽取一个样本，总体就被补充抽样数据:

![](img/bd6e63b8-5481-4f46-8fe6-c3ecc23016dd.png)

在上图中，有一个数据集有多个组件( **A** 、 **B** 、 **C** 、 **D** 、 **E** 、 **F** 、 **G** 、 **H** 和 **I** )。首先，我们需要画三个同样大小的样本。我们随机抽取**样本 1** 说第一个元素原来是 **A** 。然而，在我们绘制第二个元素**样本 1** 之前，**一个**被返回到数据集。整个抽奖过程也是如此。这被称为**取样和替换**。因此，我们有机会在集合中多次选择相同的项目。按照这个过程，我们抽取了三个样品，即**样品 1** 、**样品 2** 和**样品 3** 。

当我们更进一步，确定**样本 1** 、**样本 2** 和**样本 3** 的统计数据(各种指标)时，我们找出所有统计数据的平均值，以推断数据集(总体)的一些情况。这个整个过程被称为**自举**，抽取的样本被称为自举样本。这可以用下面的等式来定义:

*关于数据集的推断(总体)=平均值(样本 1，样本 2，............，样本 N)*

如果您仔细观察前面的图表，可能会发现数据集的一些元素没有被选取，或者不是这三个样本的一部分:

**样品 1** : ( **A** 、 **E** 、 **H** 、 **C** )

*   **样品 2** : ( **F** 、 **G** 、 **A** 、 **C** )
*   **样品 3** : ( **E** ， **H** ， **G** ， **F**
*   因此，没有被拾取的元素是 **B** 、 **D** 和 **I** 。不属于抽取样品的样品称为**开箱** ( **OOB** )样品。

让我们做一个简单的编码练习，看看如何在 Python 中实现这一点:

这里，我们将使用`sklearn`和`resample`函数。让我们导入必要的库:

1.  接下来，创建我们需要采样的数据集:

```
#importing Libraries
from sklearn.utils import resample
```

2.  现在，我们将在`resample`函数的帮助下提取一个引导样本:

```
dataset=[10,2
```

3.  我们将使用列表理解提取一个 OOB 样本:

```
0,30,40,50,60,70,80,90,100]
```

```
#using "resample" function generate a bootstrap sample
boot_samp = resample(dataset, replace=True, n_samples=5, random_state=1)
```

4.  We will use list comprehension to extract an OOB sample:

```
#extracting OOB sample
OOB=[x for x in dataset if x not in boot_samp]
```

现在，让我们把它打印出来:

5.  我们得到以下输出:

```
print(boot_samp)
```

我们可以看到在抽样中有一个 60 的重复。这是由于替换取样造成的。

```
[60, 90, 100, 60, 10]
```

接下来，我们需要打印以下代码:

6.  我们得到以下输出:

```
print(OOB)
```

到此结束时，我们希望得到如下结果:

```
[20, 30, 40, 50, 70, 80]
```

*OOB =数据集-启动 _ 样本*

*=[10，20，30，40，50，60，70，80，90，100] - [60，90，100，60，10]*

*=【20，30，40，50，70，80】*

这和我们从代码中得到的结果是一样的。

制袋材料



# Bagging 代表引导**聚合**。因此，很明显，打包的概念源于自举。暗示着 bagging 已经具备了自举的元素。这是一种 bootstrap 集成方法，其中多个分类器(通常来自同一算法)在样本上进行训练，这些样本是用来自训练集/群体的替换(bootstrap 样本)随机抽取的。所有分类器的聚集以平均或投票的形式发生。它试图减少模型中过度拟合问题的影响，如下图所示:

![](img/a942be0b-22b5-45c0-9fbf-007ec88a7e7e.png)

装袋有三个阶段:

**自举** : 这是一种统计技术，用于生成随机样本或带有替换的自举样本。

*   **模型拟合** : 在这个阶段，我们在 bootstrap 样本上建立模型。通常，相同的算法用于建立模型。但是，使用不同的算法没有限制。
*   **组合模型** : 这一步包括组合所有模型并取平均值。例如，如果我们应用了一个决策树分类器，那么每个分类器产生的概率是平均的。
*   **Combining models**:This step involves combining all the models and taking an average. For example, if we have applied a decision tree classifier, then the probability that's coming out of every classifier is averaged.

决策图表



# 决策树是一种监督学习技术，它基于分治法工作。它可用于处理分类和回归。基于最显著的特征，总体被分成两个或多个同质样本。

例如，假设我们有一个向银行申请贷款的人的样本。对于这个例子，我们将把计数作为 50。在这里，我们得到了三个属性，即性别、收入和此人持有的其他贷款数量，以预测是否给他们贷款。

我们需要根据性别、收入和他们持有的其他贷款数量对这些人进行细分，并找出最重要的因素。这往往会创建最同质的集合。

让我们先来看看收入，并尝试在此基础上创造细分市场。申请贷款的总人数是 50 人。在 50 人中，有 20 人获得了贷款。但是，如果我们把这个按收入分解，我们可以看到，分解已经按收入<100,000 and > =10 万完成了。这不会生成一个同质的组。我们可以看到 40%的申请人(20 人)获得了贷款。在收入低于 10 万英镑的人群中，30%的人设法获得了贷款。同样，收入大于或等于 10 万的人群中，有 46.67 %的人设法获得了贷款。下图显示了基于收入的树拆分:

![](img/92c46e1e-2cb7-4b7e-8f24-a15ef8a9a299.png)

现在我们来看看贷款的数量。即使这一次，我们也无法看到一个同质群体的产生。下图显示了基于贷款数量的树拆分:

![](img/6d36994c-4d82-4617-a5d3-4e8c4f27e2fc.png)

让我们继续讨论性别问题，看看它在创造一个同质群体方面表现如何。这就是同质群体。有 15 名女性，其中 53.3%获得了贷款。34.3%的男性最终也获得了贷款。下图显示了基于性别的树拆分:

![](img/48b51511-931c-4a3f-a32a-9be7d3edf359.png)

在此帮助下，找到了最重要的变量。现在，我们将详细讨论这些变量有多重要。

在此之前，我们必须了解与决策树相关的术语和命名法:

**根节点**:代表分裂成两个或更多同类组的整个群体或数据集

*   **决策节点**:这是在一个节点被划分为更多的子节点时创建的
*   **叶节点**:当节点不可能进一步分裂时，该节点被称为叶节点或终端节点
*   **分支**:整棵树的一个子部分称为**分支**或**子树**:
*   ![](img/83181071-c958-46b3-86ae-372d6b81f921.png)

树木分裂



# 有各种算法可以帮助我们进行树分裂，所有这些算法都将我们带到叶节点。决策树将所有可用的特征(变量)考虑在内，并选择将导致最纯粹或最同质分裂的特征。用来分割树的算法也取决于目标变量。让我们一步一步来:

基尼指数(Gini index):这表示如果我们从总体中随机选择两个项目，它们必须来自同一个阶层。如果种群是完全纯的，这一事件的概率将会是 1。它只执行二进制分割。**分类回归树** ( **推车**)就利用了这种拆分。

1.  以下公式是计算基尼指数的方法:

![](img/adcdc6ab-e709-4f65-a616-2d00813d3e13.png)

这里， *p(t)* 是目标变量的值为 *t.* 的观测值的比例

对于二元目标变量， *t=1* ，最大基尼指数值如下:

*= 1—(1/2)^2—(1/2)^2*
*= 1–2*(1/2)^2*
*= 1-2 *(1/4)*
*= 1–0.5*
*= 0.5*

基尼系数给出了一个概念，通过分裂产生的两个群体中的阶级混合程度来判断分裂有多好。完美的分离导致基尼系数为 0，而最坏的分离导致 50/50 的等级。

对于一个具有 *k* 水平的名义变量，基尼指数的最大值为 *(1- 1/k)。*

**信息增益**:我们来深究一下这个，看看到底是什么。如果我们碰巧有三个场景，如下图所示，哪个可以很容易的描述出来？

2.  ![](img/2ecceb2f-c502-4b47-9276-be973ecc1baf.png)

由于 **Z** 看起来相当均匀，并且它的所有值都相似，所以它被称为**纯集合**。因此，它需要较少的努力来解释它。然而， **Y** 需要更多的信息来解释，因为它不是纯的。X 是他们中最不纯洁的。它试图传达的是，随机性和无组织性增加了复杂性，因此它需要更多的信息来解释。这种随机程度被称为熵。如果样本是完全同质的，那么熵就是 *0* 。如果样本被等分，它的熵将是 *1* :

*熵=-p log[2]p-q log[2]q*

这里， *p* 表示成功的概率， *q* 表示失败的概率。

熵也用于分类目标变量。它挑选与父节点相比具有最低熵的分裂。

在这里，我们必须先计算父节点的熵。然后，我们需要计算被分割的每个节点的熵，并发布出来，包括所有子节点的加权平均值。

**方差减少**:对于连续的目标变量，使用方差减少。这里，我们使用方差来决定最佳分割。选择差异最小的拆分作为拆分标准:

3.  方差= ![](img/b83a3621-9aeb-480a-9a06-8df93b13d3d0.png)

这里，![](img/41238de0-7910-4753-b06d-e0aa03b19ed2.png)是所有值的平均值， *X，*是真实值， *n* 是值的个数。

首先计算每个节点的方差，然后加权平均每个节点的方差，使我们选择最佳节点。

树木分裂参数



# 我们需要调整或了解许多参数:

`Max_depth`:最重要的参数之一是`max_depth`。它抓住了树能有多深的本质。树中更深的深度意味着它能够从特征中提取更多的信息。然而，有时，过度的深度可能是一个令人担忧的原因，因为它往往会带来过度拟合。

*   `min_samples_split`:表示分割内部节点所需的最小样本数。这可以在考虑每个节点的至少一个样本和考虑每个节点的所有样本之间变化。当我们增加这个参数时，树变得更加受限，因为它必须在每个节点考虑更多的样本。`min_samples_split`值的增加倾向于欠配。
*   `min_samples_leaf`:这是一个叶节点所需的最小样本数。将该值增加到最大值可能会导致拟合不足。
*   `max_features`:这是最佳分割需要考虑的最大特征数量。当最大特征数量增加时，可能会导致过度拟合。
*   现在，我们已经很好地理解了随机森林算法。我们接下来会谈到这一点。

随机森林算法



# 随机森林算法与装袋技术一起工作。种植的树木数量和生长方式如下:

训练集中有 *N* 个观测值。从 *N* 个观察值中随机抽取样本，并进行替换。这些样本将作为不同树的训练集。

*   如果有 *M 个*输入特征(变量)，则 *m 个*特征被绘制为 *M 个*特征的子集，当然还有 *m 个< M 个*特征。这是在树的每个节点随机选择 m 个特征。
*   每棵树都长到了最大限度。
*   Every tree is grown to the largest extent possible.

预测是基于所有树产生的结果的集合而发生的。在分类的情况下，聚合的方法是投票，而在回归的情况下，它是所有结果的平均值:

*   ![](img/09fecda6-8be8-4b47-9b9c-51591cbcf909.png)

让我们做一个案例研究，因为这将帮助我们更详细地理解这个概念。让我们研究乳腺癌数据。

Let's work on a case study, since that will help us understand this concept more in detail. Let's work on breast cancer data.

个案研究



# 本案例研究中给出的数据是关于被检测出患有两种乳腺癌的患者的:

恶性的

*   温和的
*   这里给出了一些特征，这些特征具有从乳腺肿块的**细针抽吸** ( **FNA** )中计算出的关于细胞核的特性。基于这些特征，我们需要预测癌症是恶性的还是良性的。按照以下步骤开始:

导入所有需要的库:

1.  加载乳腺癌数据:

```
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
#importing our parameter tuning dependencies
from sklearn.model_selection import (cross_val_score, GridSearchCV,StratifiedKFold, ShuffleSplit )
#importing our dependencies for Feature Selection
from sklearn.feature_selection import (SelectKBest, RFE, RFECV)
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.cross_validation import ShuffleSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from collections import defaultdict
# Importing our sklearn dependencies for the modeling
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.cross_validation import KFold
from sklearn import metrics
from sklearn.metrics import (accuracy_score, confusion_matrix, 
 classification_report, roc_curve, auc)
```

2.  让我们来理解这些数据:

```
data= pd.read_csv("breastcancer.csv")
```

3.  我们得到以下输出:

```
data.info()
```

![](img/f9e6557f-a0cb-4eaa-9312-442ffb09c069.png)

让我们在这里考虑`data.head()`:

4.  由此，我们得到以下输出:

```
data.head()
```

![](img/819947a5-3c40-47d5-8342-71e05f58cd5f.png)

我们从下面的代码中得到数据诊断:

5.  以下是上述代码的输出:

```
data.diagnosis.unique()
```

![](img/7e288080-bf9d-41ea-b5bc-b43963d466ed.png)

数据描述如下:

6.  我们从前面的代码中得到以下输出:

```
data.describe()
```

![](img/b619a949-107e-455d-840b-3027dbbf095e.png)

![](img/4bd0bc77-162c-441f-b7cd-428b83e5e6f4.png)

```
data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})

datas = pd.DataFrame(preprocessing.scale(data.iloc[:,1:32]))
datas.columns = list(data.iloc[:,1:32].columns)
datas['diagnosis'] = data['diagnosis']

datas.diagnosis.value_counts().plot(kind='bar', alpha = 0.5, facecolor = 'b', figsize=(12,6))
plt.title("Diagnosis (M=1, B=0)", fontsize = '18')
plt.ylabel("Total Number of Patients")
plt.grid(b=True)
```

![](img/4bd0bc77-162c-441f-b7cd-428b83e5e6f4.png)

```
data_mean = data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]

plt.figure(figsize=(10,10))
foo = sns.heatmap(data_mean.corr(), vmax=1, square=True, annot=True)
```

![](img/83f6dc50-a460-45af-a92d-2b8ffc6995c3.png)

前面的输入为我们提供了以下输出:

```
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn import metrics
predictors = data_mean.columns[2:11]
target = "diagnosis"
X = data_mean.loc[:,predictors]
y = np.ravel(data.loc[:,[target]])
# Split the dataset in train and test:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print ('Shape of training set : %i & Shape of test set : %i' % (X_train.shape[0],X_test.shape[0]) )
print ('There are very few data points so 10-fold cross validation should give us a better estimate')
```

![](img/6d121f10-24e4-49d3-8832-e24d16f80548.png)

![](img/f63c06af-363d-4a1b-9699-bd534ae987c2.png)

```
param_grid = {
 'n_estimators': [ 25, 50, 100, 150, 300, 500], 
 "max_depth": [ 5, 8, 15, 25],
 "max_features": ['auto', 'sqrt', 'log2'] 
 } 
#use OOB samples ("oob_score= True") to estimate the generalization accuracy.
rfc = RandomForestClassifier(bootstrap= True, n_jobs= 1, oob_score= True)
#let's use cv=10 in the GridSearchCV call
#performance estimation
#initiate the grid 
grid = GridSearchCV(rfc, param_grid = param_grid, cv=10, scoring ='accuracy')
#fit your data before you can get the best parameter combination.
grid.fit(X,y)
grid.cv_results_
```

![](img/a1c90824-2c4a-4261-85a7-1fc5076b3613.png)

```
# Let's find out the best scores, parameter and the estimator from the gridsearchCV
print("GridSearhCV best model:\n ")
print('The best score: ', grid.best_score_)
print('The best parameter:', grid.best_params_)
print('The best model estimator:', grid.best_estimator_)
```

![](img/d7aa01a1-4ef0-4fe8-8c92-90eed38bba16.png)

```
# model = RandomForestClassifier() with optimal values
model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
 max_depth=8, max_features='sqrt', max_leaf_nodes=None,
 min_impurity_decrease=0.0, min_impurity_split=None,
 min_samples_leaf=1, min_samples_split=2,
 min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,
 oob_score=True, random_state=None, verbose=0, warm_start=False)
model.fit(X_train, y_train)
```

由此可见，测试数据上的表现精度为`95.0`:

```
print("Performance Accuracy on the Testing data:", round(model.score(X_test, y_test) *100))
```

在这里，总预言是`114`:

```
#Getting the predictions for X
y_pred = model.predict(X_test)
print('Total Predictions {}'.format(len(y_pred)))
```

![](img/e256f6d9-d551-4096-a773-babd33156f3a.png)

```
truth = pd.DataFrame(y_test, columns= ['Truth'])
predictions = pd.DataFrame(y_pred, columns= ['Predictions'])
frames = [truth, predictions]
_result = pd.concat(frames, axis=1)
print(_result.shape)
_result.head()
```

由此可见，10 k 倍交叉验证均值为`94.9661835749`:

```
# 10 fold cross-validation with a Tree classifier on the training dataset# 10 fold 
#splitting the data, fitting a model and computing the score 10 consecutive times
cv_scores = []
scores = cross_val_score(rfc, X_train, y_train, cv=10, scoring='accuracy')
cv_scores.append(scores.mean())
cv_scores.append(scores.std())

#cross validation mean score
print("10 k-fold cross validation mean score: ", scores.mean() *100)
```

这里我们可以看到分类精度是`95.0`:

```
# printing classification accuracy score rounded
print("Classification accuracy: ", round(accuracy_score(y_test, y_pred, normalize=True) * 100))
```

![](img/df04808f-0b0b-4ae4-ae11-5eaaea1d3c6f.png)

```
# Making the Confusion Matrix
cm = confusion_matrix(y_test, y_pred) 
plt.figure(figsize=(12,6))
ax = plt.axes()
ax.set_title('Confusion Matrix for both classes\n', size=21)
sns.heatmap(cm, cmap= 'plasma',annot=True, fmt='g') # cmap
plt.show()
```

![](img/ab332a99-7d5f-4df2-9ed3-4d8a540728df.png)

```
# The classification Report
target_names = ['Benign [Class 0]', 'Malignant[Class 1]']
print(classification_report(y_test, y_pred, target_names=target_names))
```

![](img/959ed7fc-e995-4594-8772-215c3369f34e.png)

```
y_pred_proba = model.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="curve, auc="+str(auc))
plt.legend(loc=4)
plt.show()
```

上图是一个**接收器操作特性** ( **ROC** )度量，用于使用交叉验证评估分类器输出质量。

前面的图显示了对我们选择的特征(`['compactness_mean', 'perimeter_mean', 'radius_mean', 'texture_mean', 'concavity_mean', 'smoothness_mean']`)的 ROC 反应和从 k 倍交叉验证中创建的诊断相关变量。

`0.99`的一个 ROC 区挺好的。

提升



# 说到装袋，它既适用于分类，也适用于回归。然而，还有一种技术也是合奏家族的一部分:增强。然而，这两者的基本原理是完全不同的。在 bagging 中，每个模型独立运行，然后在最后汇总结果。这是一个并行操作。增压的作用方式不同，因为它是按顺序流动的。这里的每个模型都运行并将重要特性传递给另一个模型:

![](img/d4f677a3-842b-47dc-be84-527d26d94f49.png)

梯度推进



# 为了解释梯度推进，我们将采用伟大的数据科学家 Ben Gorman 的路线。他已经能够用一种简单的数学方法来解释它。假设我们有九个训练示例，其中我们需要根据三个特征来预测一个人的年龄，例如他们是否喜欢园艺、玩视频游戏或上网。相关数据如下:

![](img/0bf3e5de-9aff-46ba-aaf6-830f589b6200.png)

为了建立这个模型，目标是最小化均方误差。

现在，我们将使用回归树构建模型。首先，如果我们希望在训练节点上至少有三个样本，树的第一次拆分可能如下所示:

![](img/928c8ffa-6ef3-4e38-8c74-2e1a1fbde65f.png)

这似乎没问题，但这不包括他们是否玩视频游戏或浏览互联网等信息。如果我们计划在训练节点有两个样本呢？

![](img/a16df436-cb99-466d-a364-c33c32f41c14.png)

通过前面的树，我们能够从特征中获得某些信息，例如 **SurfInternet** 和 **PlaysVideoGames** 。让我们弄清楚残差/误差是如何产生的:

![](img/53f43655-8f92-48e9-9912-97466e94b504.png)

![](img/53f43655-8f92-48e9-9912-97466e94b504.png)

现在，我们将处理第一个模型的残差:

![](img/aa17a869-d555-461e-be73-e76c2dc09eaf.png)

![](img/aa17a869-d555-461e-be73-e76c2dc09eaf.png)

一旦我们建立了残差模型，我们必须将以前的模型与当前的模型结合起来，如下表所示:

![](img/948cc81a-328c-4744-9dde-1a5ba19ab78f.png)

我们可以看到残差下降了，模型变得更好了。

让我们试着阐述一下到目前为止我们已经做了什么:

首先，我们在数据 *f [1] (x) = y.* 上建立一个模型

1.  接下来，我们计算残差并基于残差构建模型:
2.  *h[1](x)= y-f[1](x)*

接下来就是结合模型，即*f[2](x)= f[1](x)+h[1](x)。*

3.  增加更多的模型可以纠正以前模型的错误。前面的等式将变成如下:

*f3(x)= f2(x) + h2(x)*

该等式最终将如下所示:

*f[m](x)= f[m][-1](x)+h[m][-1](x)*

或者，我们可以这样写:

*h[m](x)= y-f[m](x)*

由于我们的任务是最小化平方误差， *f* 将用训练目标值的平均值初始化:

![](img/fa749feb-9490-4a4c-9790-fac085bc18f0.png)

4.  然后，我们可以找出 *f* [*m+1* ，]就像之前一样:

*f[m](x)= f[m]-1(x)+h[m][-1](x)*

5.  现在，我们可以对梯度推进模型使用梯度下降。我们要最小化的目标函数是 *L* 。我们的起点是 *f [o] (x)* 。对于迭代 *m=1* ，我们计算 *L* 相对于 *f [o] (x)* 的梯度。然后，我们将弱学习者与梯度分量相匹配。在回归树的情况下，叶节点在具有相似特征的样本中产生一个平均梯度**。对于每一片叶子，我们沿着平均梯度的方向前进。结果是 *f [1]* 并且这可以重复直到我们得到 *f [m]* 。**

我们修改了梯度推进算法，使其适用于任何可微损失函数。让我们清理前面的想法，并再次重新制定我们的梯度推进模型。

梯度推进参数

在对乳腺癌用例应用梯度增强之前，需要考虑不同的参数:



# `Min_samples_split`:在一个节点中被考虑用于分裂的最小样本数被称为`min_samples_split`。

`Min_samples_leaf`:终端或叶节点所需的最小样本数称为`min_samples_leaf`。

*   `Max_depth`:这是从一棵树的根到最远的叶子所允许的最大节点数。然而，更深的树可以模拟更复杂的关系，导致模型过度拟合。
*   `Max_leaf_nodes`:一棵树的叶子的最大节点数。由于二叉树被创建，深度`n`将产生最多*2^n 个叶子。因此，可以定义`max_depth`或`max_leaf_nodes`。*
*   `Max_depth`: This is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships, however, causing the model to overfit.
*   `Max_leaf_nodes`: The maximum number of nodes at the leaves in a tree. Since binary trees are created, a depth of `n` would produce a maximum of *2^(n )* leaves. Hence, either `max_depth` or `max_leaf_nodes` can be defined.

现在，我们将对乳腺癌用例应用梯度增强。这里，我们正在加载构建模型所需的库:

我们现在已经完成了执行随机森林时数据清理和探索的各个步骤。现在，我们将直接开始构建模型。

这里，我们将执行网格搜索，找出梯度推进算法的最佳参数:

```
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
```

我们得到以下输出:

![](img/f0781efe-1af7-400d-8397-3a9e5d860876.png)

```
param_grid = {
 'n_estimators': [ 25, 50, 100, 150, 300, 500], # the more parameters, the more computational expensive
 "max_depth": [ 5, 8, 15, 25],
 "max_features": ['auto', 'sqrt', 'log2'] 
 }
gbm = GradientBoostingClassifier(learning_rate=0.1,random_state=10,subsample=0.8)
#performance estimation
#initiate the grid 
grid = GridSearchCV(gbm, param_grid = param_grid, cv=10, scoring ='accuracy')
#fit your data before you can get the best parameter combination.
grid.fit(X,y)
grid.cv_results_     
```

We get the following output:

![](img/f0781efe-1af7-400d-8397-3a9e5d860876.png)

现在，让我们找出最佳参数:

输出如下所示:

![](img/588de5cb-de5b-4e24-ba89-04eb0c654304.png)

```
#Let's find out the best scores, parameter and the estimator from the gridsearchCV
print("GridSearhCV best model:\n ")
print('The best score: ', grid.best_score_)
print('The best parameter:', grid.best_params_)
print('The best model estimator:', grid.best_estimator_)
```

现在，我们将构建模型:

测试数据的表现精度为`96.0`:

预测总数为`114`:

```
model2 = GradientBoostingClassifier(criterion='friedman_mse', init=None,
 learning_rate=0.1, loss='deviance', max_depth=5,
 max_features='sqrt', max_leaf_nodes=None,
 min_impurity_decrease=0.0, min_impurity_split=None,
 min_samples_leaf=1, min_samples_split=2,
 min_weight_fraction_leaf=0.0, n_estimators=150,
 presort='auto', random_state=10, subsample=0.8, verbose=0,
 warm_start=False)
model2.fit(X_train, y_train) 

print("Performance Accuracy on the Testing data:", round(model2.score(X_test, y_test) *100))
```

![](img/ee427abe-70f1-4061-95f2-045ab887cb9e.png)

```
#getting the predictions for X
y_pred2 = model2.predict(X_test)
print('Total Predictions {}'.format(len(y_pred2)))
```

让我们进行交叉验证:

```
truth = pd.DataFrame(y_test, columns= ['Truth'])
predictions = pd.DataFrame(y_pred, columns= ['Predictions'])
frames = [truth, predictions]
_result = pd.concat(frames, axis=1)
print(_result.shape)
_result.head()
```

10 k 倍交叉验证平均得分为`94.9420289855`:

分类精度为`96.0`:

```
cv_scores = []

scores2 = cross_val_score(gbm, X_train, y_train, cv=10, scoring='accuracy')
cv_scores.append(scores2.mean())
cv_scores.append(scores2.std())

#cross validation mean score 
print("10 k-fold cross validation mean score: ", scores2.mean() *100)
```

The 10 k-fold cross-validation mean score is `94.9420289855`:

```
#printing classification accuracy score rounded
print("Classification accuracy: ", round(accuracy_score(y_test, y_pred2, normalize=True) * 100))
```

The classification accuracy is `96.0`:

```
# Making the Confusion Matrix
cm = confusion_matrix(y_test, y_pred2)
plt.figure(figsize=(12,6))
ax = plt.axes()
ax.set_title('Confusion Matrix for both classes\n', size=21)
sns.heatmap(cm, cmap= 'plasma',annot=True, fmt='g') # cmap
plt.show()
```

通过查看混淆矩阵，我们可以看到这个模型比前一个模型更好:

![](img/3bc63967-fbe6-41e9-b577-ad52bb8bcdf9.png)

摘要

在这一章中，我们学习了集成学习及其不同的方法，即打包、提升和堆叠。我们甚至看到了什么是 bootstrapping，这是 bagging 和 boosting 等集成学习方法的根源。我们还学习了决策树及其分而治之的方法，例如人们申请贷款。然后我们讨论了树分裂和分裂决策树的参数，接着讨论随机森林算法。我们使用所涵盖的概念对乳腺癌进行了案例研究。我们还发现了装袋增压和梯度增压的区别。我们还讨论了梯度增强的参数，以乳腺癌为例。



# 在下一章，我们将学习训练神经网络。

In this chapter, we studied ensemble learning and its different methods, namely bagging, boosting, and stacking. We even saw what is bootstrapping which is the root for ensemble learning methods such as bagging and boosting. We also learned about decision trees and its approach of divide and rule with example of people applying for loan. Then we covered tree splitting and the parameters to split a decision tree, moving on to the random forest algorithm. We worked on a case study of breast cancer using the concepts covered. We also discovered the difference between bagging and boosting and gradient boosting. We also discussed on parameters of gradient boosting to use it our example of breast cancer.

In the next chapter, we will learn about training neural networks.