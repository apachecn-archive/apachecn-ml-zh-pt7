

# 第六章。预测数字数据-回归方法

数学关系帮助我们理解日常生活的许多方面。例如，体重是一个人卡路里摄入量的函数，收入通常与教育和工作经验有关，民调数据有助于我们估计总统候选人连任的几率。

当这种关系用精确的数字来表达时，我们获得了额外的清晰度。例如，每天额外摄入 250 千卡热量，可能导致每月体重增加近 1 公斤；每一年的工作经验可能价值 1000 美元的额外年薪；当经济强劲时，总统更有可能连任。显然，这些等式并不完全适合每一种情况，但是我们期望它们平均起来是相当正确的。

本章扩展了我们的机器学习工具包，超越了之前讨论的分类方法，并介绍了估计数字数据之间关系的技术。在研究几个现实世界的数值预测任务时，您将了解到:

*   回归中使用的基本统计原理，这是一种对数值关系的大小和强度进行建模的技术
*   如何为回归分析准备数据，并估计和解释回归模型
*   一对称为回归树和模型树的混合技术，使决策树分类器适用于数值预测任务

基于统计领域的大量工作，这一章中使用的方法在数学上比以前涉及的方法更重一点，但是不要担心！即使你的代数技能有点生疏，R 也会照顾好你。

# 了解回归

回归涉及指定单个数字 **因变量**(要预测的值)和一个或多个数字**自变量**(预测值)之间的关系。顾名思义，因变量取决于自变量的值。最简单的回归形式假设自变量和因变量之间的关系遵循一条直线。

### 注意

描述数据拟合过程的术语“回归”起源于 19 世纪晚期弗朗西斯·高尔顿爵士对遗传学的一项研究。他发现，非常矮或非常高的父亲倾向于生出身高接近平均身高的儿子。他把这种现象称为“回归均值”。

你可能还记得基本的代数，直线可以用类似于 *y = a + bx* 的**斜率截距形式**来定义。在这种形式中，字母 *y* 表示因变量， *x* 表示自变量。 **斜率**项 *b* 规定了 *x* 每增加一次线上升多少。正值定义向上倾斜的线，而负值定义向下倾斜的线。术语 *a* 被称为**截距**，因为指定了直线与垂直 *y* 轴相交或相交的点。表示 *x = 0* 时 *y* 的值。

![Understanding regression](img/B03905_06_01.jpg)

回归方程使用相似的斜率截距格式模拟数据。机器的工作是识别 *a* 和 *b* 的值，以便指定的线能够最好地将提供的 *x* 值与 *y* 值相关联。不可能总有一个函数能完美地关联这些值，所以机器也必须有某种方法来量化误差幅度。我们稍后将深入讨论这一点。

回归分析通常用于模拟数据元素之间的复杂关系，估计治疗对结果的影响，以及推断未来。虽然它可以应用于几乎任何任务，但一些特定的用例包括:

*   研究人口和个体如何通过测量特征而变化，用于经济学、社会学、心理学、物理学和生态学等不同领域的科学研究
*   量化事件和响应之间的因果关系，如临床药物试验、工程安全测试或市场研究中的事件和响应
*   在给定已知标准的情况下，识别可用于预测未来行为的模式，例如预测保险索赔、自然灾害损失、选举结果和犯罪率

回归方法也用于**统计假设检验**，其根据观察到的数据确定一个前提可能是真还是假。回归模型对关系的强度和一致性的估计提供了可用于评估观察结果是否只是偶然的信息。

### 注意

假设检验极其微妙，不属于机器学习的范围。如果你对这个话题感兴趣，统计学入门教材是一个很好的开始。

回归分析不是单一算法的同义词。相反，它是大量方法的保护伞，这些方法可以适用于几乎任何机器学习任务。如果您只能选择一种方法，回归将是一个不错的选择。一个人可以把整个职业生涯都投入到别的事情上，也许还有很多东西要学。

在这一章中，我们将只关注最基本的**线性回归**模型——那些使用直线的模型。当只有一个独立变量时，它被称为简单线性回归的。在两个或更多独立变量的情况下，这被称为**多元线性回归**，或简单地称为“多元回归”。这两种技术都假设因变量是在连续的尺度上测量的。

回归也可以用于其他类型的因变量，甚至是一些分类任务。例如，**逻辑回归**用于模拟二元分类结果，而**泊松回归**——以法国数学家西蒙·泊松命名——模拟整数计数数据。被称为**多项式逻辑回归**的方法对分类结果进行建模；因此，它可以用于分类。相同的基本原则适用于所有的回归方法，所以在理解了线性情况之后，学习其他的就相当简单了。

### 提示

许多专门化的回归方法都属于一类**广义线性模型** ( **GLM** )。使用 GLM，可以通过使用**链接函数**将线性模型推广到其他模式，该链接函数为 *x* 和 *y* 之间的关系指定了更复杂的形式。这使得回归可以应用于几乎任何类型的数据。

我们将从简单线性回归的基本情况开始。尽管名字如此，这种方法在解决复杂问题时并不简单。在下一节中，我们将看到如何使用简单的线性回归模型来避免一场悲剧性的工程灾难。

## 简单线性回归

1986 年 1 月 28 日，美国航天飞机*挑战者号*的七名机组人员在火箭提升器故障时丧生，导致灾难性的解体。事后，专家们把发射温度作为潜在的罪魁祸首。负责密封火箭接头的橡胶 O 型圈从未在 40 华氏度(4 摄氏度)以下进行过测试，发射当天的天气异常寒冷，低于冰点。

事后看来，这起事故是数据分析和可视化重要性的一个案例研究。虽然不清楚火箭工程师和决策者在发射前获得了什么信息，但不可否认的是，更好的数据，如果仔细利用，很可能避免了这场灾难。

### 注意

本节的分析基于 Dalal SR，Fowlkes EB，Hoadley B. *航天飞机的风险分析:挑战者号前的故障预测*中的数据。美国统计协会杂志。1989;84:945-957.关于数据如何改变结果的一个观点，请看 Tufte ER。*视觉解释:图像和数量，证据和叙述*。图形出版社；1997.关于对比，参见 Robison W，Boisioly R，Hoeker D，Young，S. *陈述和错误陈述:Tufte 和 Morton Thiokol 工程师对挑战者*。科学与工程伦理。2002;8:59-81.

火箭工程师几乎肯定知道，低温会使组件变得更脆，更不容易密封，这将导致更高的危险燃料泄漏的可能性。然而，考虑到继续发射的政治压力，他们需要数据来支持这一假设。一个回归模型证明了温度和 O 型圈故障之间的联系，并可以预测发射时预期温度下的故障概率，这可能会非常有帮助。

为了建立回归模型，科学家们可能使用了前 23 次航天飞机成功发射的发射温度和部件损坏的数据。组件损坏表明两种问题之一。第一个问题叫做腐蚀，当过热烧毁 O 形圈时就会发生。第二个问题称为漏气，当热气体从密封不良的 O 型环中泄漏或“漏气”时，就会发生。由于航天飞机总共有六个主 O 型圈，每次飞行可能会出现多达六个损坏。尽管火箭可以在一次或多次遇险事件中幸存下来，或者只在一次遇险事件中失败，但每一次额外的遇险都会增加灾难性失败的可能性。

以下散点图显示了前 23 次发射中检测到的主要 O 形圈损坏情况，与发射时的温度进行了比较:

![Simple linear regression](img/B03905_06_02.jpg)

审视剧情，有一个明显的趋势。在较高温度下进行的发射往往较少发生 O 型圈损坏事件。

此外，最冷的一次发射(53 华氏度)发生了两起遇险事件，这一水平仅在另一次发射中达到过。根据手头的信息，挑战者号计划在温度低 20 多度的情况下发射似乎令人担忧。但是我们到底应该有多担心呢？要回答这个问题，我们可以求助于简单的线性回归。

一个简单的线性回归模型定义了一个因变量和一个独立预测变量之间的关系，使用一条由以下形式的方程定义的直线:

![Simple linear regression](img/B03905_06_03.jpg)

不要被希腊字符吓到，这个等式仍然可以用前面描述的斜率截距形式来理解。截距 *α* (alpha)描述了直线穿过 *y* 轴的位置，而斜率 *β* (beta)描述了在增加 *x* 的情况下 *y* 的变化。对于航天飞机的发射数据，斜率会告诉我们发射温度每升高一度，O 型圈失效次数的预期减少量。

### 提示

希腊字符常用于统计领域，表示作为统计函数参数的变量。因此，执行回归分析包括为 *α* 和 *β* 找到**参数估计值**。α和β的参数估计值通常用 *a* 和 *b* 来表示，尽管您可能会发现一些术语和符号可以互换使用。

假设我们知道航天飞机发射数据方程中的估计回归参数为: *a = 3.70* 和 *b = -0.048* 。

因此，全线性方程为*y = 3.70–0.048 x*。暂且忽略这些数字是如何获得的，我们可以在散点图上画出这样的线:

![Simple linear regression](img/B03905_06_04.jpg)

如这条线所示，在华氏 60 度的温度下，我们预测只有一个 O 型圈会损坏。在华氏 70 度时，我们预计大约有 0.3 次故障。如果我们外推我们的模型，一直到 31 度——挑战者号发射的预测温度——我们预计大约有 *3.70 - 0.048 * 31 = 2.21* O 形圈遇险事件。假设每个 O 形圈故障导致灾难性燃料泄漏的可能性相同，这意味着挑战者号在 31 度发射比典型的 60 度发射风险高近三倍，比 70 度发射风险高八倍以上。

请注意，直线并不完全通过每个数据点。相反，它稍微均匀地穿过数据，一些预测低于或高于该线。在下一节中，我们将了解为什么选择这条特定的线。

## 普通最小二乘估计

为了确定 *α* 和 *β* 的最优估计值，使用了一种称为**普通最小二乘法** ( **OLS** )的估计方法。在 OLS 回归中，选择斜率和截距，使其最小化误差平方和，即预测的 *y* 值和实际的 *y* 值之间的垂直距离。这些误差被称为**残差**，下图显示了几个点的:

![Ordinary least squares estimation](img/B03905_06_05.jpg)

在数学术语中，OLS 回归的目标可以表示为最小化以下等式的任务:

![Ordinary least squares estimation](img/B03905_06_06.jpg)

用通俗的语言来说，这个方程将 *e* (误差)定义为实际 *y* 值与预测 *y* 值之间的差值。对数据中所有点的误差值进行平方和求和。

### 提示

在 *y* 项上面的脱字符(`^`)是统计符号的一个常用特征。它表示该项是真实值 *y* 的估计值。这被称为*y*-帽子，发音和你戴在头上的帽子一模一样。

*a*的解取决于 *b* 的值。可以使用以下公式获得:

![Ordinary least squares estimation](img/B03905_06_07.jpg)

### 提示

为了理解这些方程，你需要知道另一点统计符号。出现在 *x* 和 *y* 项上的横条表示 *x* 或 *y* 的平均值。这被称为 *x* 酒吧或 *y* 酒吧，发音就像你去喝酒的地方。

虽然证明已经超出了本书的范围，但是可以使用微积分证明导致最小平方误差的值是:

![Ordinary least squares estimation](img/B03905_06_08.jpg)

如果我们把这个等式分解成几个部分，我们可以把它简化一点。 *b* 的分母应该看起来很熟悉；非常类似于 *x* 的方差，记为 *Var(x)* 。正如我们在[第 2 章](ch02.html "Chapter 2. Managing and Understanding Data")、*管理和理解数据*中所了解到的，方差包括从 *x* 的平均值中找出平均平方偏差。这可以表示为:

![Ordinary least squares estimation](img/B03905_06_09.jpg)

分子包括将每个数据点与平均值 *x* 的偏差之和乘以该点与平均值 *y* 的偏差。这类似于 *x* 和 *y* 的**协方差**函数，表示为 *Cov(x，y)* 。协方差公式为:

![Ordinary least squares estimation](img/B03905_06_10.jpg)

如果我们将协方差函数除以方差函数，则第 *n* 项被取消，我们可以将 *b* 的公式改写为:

![Ordinary least squares estimation](img/B03905_06_11.jpg)

鉴于这种重述，很容易使用内置的 R 函数来计算 *b* 的值。让我们将其应用于火箭发射数据，以估计回归线。

### 提示

如果您想了解这些示例，请从 Packt 发布网站下载`challenger.csv`文件，并使用`launch <- read.csv("challenger.csv")`命令加载到数据框。

假设我们的航天飞机发射数据存储在一个名为`launch`的数据帧中，自变量 *x* 为温度，因变量 *y* 为`distress_ct`。然后我们可以使用 R 的`cov()`和`var()`函数来估计 *b* :

```
> b <- cov(launch$temperature, launch$distress_ct) /
 var(launch$temperature)
> b
[1] -0.04753968

```

从这里我们可以使用`mean()`函数来估计 *a* :

```
> a <- mean(launch$distress_ct) - b * mean(launch$temperature)
> a
[1] 3.698413

```

手工估计回归方程并不理想，所以 R 提供了自动执行这种计算的函数。我们将很快使用这些方法。首先，我们将通过学习一种测量线性关系强度的方法来扩展我们对回归的理解，然后我们将了解如何将线性回归应用于具有多个独立变量的数据。

## 相关性

两个变量之间的**相关性** 是一个数字，表示它们的关系如何紧密地遵循一条直线。在没有额外限定的情况下，相关性通常指的是 20 世纪数学家卡尔·皮尔逊提出的**皮尔逊相关系数**。相关性的范围在-1 和+1 之间。极端值表示完美的线性关系，而接近零的相关性表示没有线性关系。

以下公式定义了皮尔逊相关性:

![Correlations](img/B03905_06_12.jpg)

### 提示

这里引入了更多的希腊符号。第一个符号(看起来像小写的 p)是 *rho* ，它用于表示皮尔逊相关统计量。看起来像 q 横着转的字符是希腊字母 *sigma* ，它们表示的是 *x* 或 *y* 的标准差。

利用这个公式，我们可以计算出发射温度和 O 型圈遇险事件数量之间的相关性。回想一下，协方差函数是`cov()`，标准差函数是`sd()`。我们将结果存储在`r`中，这是一个通常用来表示估计相关性的字母:

```
> r <- cov(launch$temperature, launch$distress_ct) /
 (sd(launch$temperature) * sd(launch$distress_ct))
> r
[1] -0.5111264

```

或者，我们可以使用 R 的相关函数，`cor()`:

```
> cor(launch$temperature, launch$distress_ct)
[1] -0.5111264

```

温度和损坏的 O 形圈数量之间的相关性为-0.51。负相关意味着温度的升高与损坏的 O 型圈数量的减少有关。对于研究 O 型环数据的美国宇航局工程师来说，这将是一个非常明确的指标，表明低温发射可能会有问题。这种相关性还告诉我们温度和 O 形圈损坏之间关系的相对强度。因为-0.51 是最大负相关-1 的中间值，这意味着存在中等强度的负线性关联。

有各种各样的经验法则用来解释相关性强度。一种方法将 0.1 至 0.3 之间的值指定为“弱”，0.3 至 0.5 之间的值指定为“中等”，0.5 以上的值指定为“强”(这些也适用于类似的负相关范围)。然而，对于某些目的来说，这些阈值可能过于宽松。通常，必须在上下文中解释这种相关性。对于涉及人类的数据，0.5 的相关性可被认为非常高，而对于由机械过程产生的数据，0.5 的相关性可能很弱。

### 提示

你可能听说过“相关性并不意味着因果关系”这句话这是基于这样一个事实，相关性只描述了一对变量之间的关联，但也可能有其他不可测量的解释。例如，死亡率和每天花在匹配电影上的时间可能有很大的关联，但在医生应该开始建议我们都看更多电影之前，我们需要排除另一种解释——年轻人看更多电影，死亡的可能性更小。

测量两个变量之间的相关性为我们提供了一种快速测量自变量和因变量之间关系的方法。随着我们开始定义具有大量预测因子的回归模型，这将变得越来越重要。

## 多元线性回归

大多数真实世界的分析都有多个独立变量。因此，对于大多数数字预测任务，您可能会使用**多元线性回归**。多元线性回归的优点和缺点如下表所示:

| 

强项

 | 

弱点

 |
| --- | --- |
| 

*   迄今为止最常见的数字数据建模方法
*   可适用于建模几乎任何建模任务
*   提供特征间关系的强度和大小以及结果的估计

 | 

*   对数据做出强有力的假设
*   模型的形式必须由用户事先指定
*   不处理缺失数据
*   只处理数字特征，因此分类数据需要额外处理
*   需要一些统计学知识来理解模型

 |

我们可以把多元回归理解为简单线性回归的延伸。这两种情况下的目标是相似的-找到使线性方程的预测误差最小的β系数的值。关键的区别在于，对于额外的自变量有额外的项。

多元回归方程一般遵循以下方程的形式。因变量 *y* 被指定为截距项 *α* 加上每个 *i* 特征的估计 *β* 值和 *x* 值的乘积之和。这里增加了一个误差项(用希腊字母*ε*表示)来提醒我们预测并不完美。这代表前面提到的**剩余**项:

![Multiple linear regression](img/B03905_06_13.jpg)

让我们考虑一下估计回归参数的解释。您会注意到，在前面的等式中，为每个特征提供了一个系数。这允许每个特征对 *y* 的值具有单独的估计影响。换句话说， *x [i]* 每增加一个单位， *y* 的变化量为*β[I]。当独立变量都为零时，截距 *α* 就是 *y* 的期望值。*

由于截距项 *α* 实际上与任何其他回归参数没有什么不同，因此它有时也表示为*β[0](读作β-零)，如以下等式所示:*

![Multiple linear regression](img/B03905_06_14.jpg)

就像之前一样，截距与任何独立的 *x* 变量无关。然而，出于稍后将变得清楚的原因，将 *β [0]* 想象成乘以一个项 *x [0]* 是一个值为 1 的常数是有帮助的:

![Multiple linear regression](img/B03905_06_15.jpg)

为了估计回归参数的值，应变量 *y* 的每个观察值必须与独立变量 *x* 的观察值相关，使用之前形式的回归方程。下图说明了这种结构:

![Multiple linear regression](img/B03905_06_16.jpg)

上图中所示的许多行和列数据可以用一个简洁的公式来描述，使用粗体**矩阵符号**来表示每个项代表多个值:

![Multiple linear regression](img/B03905_06_17.jpg)

因变量现在是一个向量， **Y** ，每个例子都有一行。独立变量已被组合成一个矩阵， **X** ，每个特征有一列，截距项还有一列“1”值。对于每个示例，每列都有一行。回归系数 **β** 和残差 **ε** 现在也是向量。

现在的目标是求解 **β** ，这是回归系数的向量，它使预测值和实际值之间的误差平方和最小。寻找最优解需要使用矩阵代数；因此，这个推导值得比本文所能提供的更仔细的注意。然而，如果你愿意相信其他人的工作，向量 **β** 的最佳估计可以计算为:

![Multiple linear regression](img/B03905_06_18.jpg)

这个方案使用了一对矩阵运算——其中 **T** 表示矩阵 **X** 的**转置**，而负指数表示**矩阵求逆**。使用 R 的内置矩阵运算，我们可以实现一个简单的多元回归学习器。让我们将这个公式应用于挑战者号的发射数据。

### 提示

如果你不熟悉前面的矩阵运算，维基百科上关于转置和矩阵求逆的页面提供了详尽的介绍，即使没有很强的数学背景也很容易理解。

使用下面的代码，我们可以创建一个名为`reg()`的基本回归函数，它带有一个参数`y`和一个参数`x`，并返回一个估计 beta 系数的向量:

```
reg <- function(y, x) {
 x <- as.matrix(x)
 x <- cbind(Intercept = 1, x)
 b <- solve(t(x) %*% x) %*% t(x) %*% y
 colnames(b) <- "estimate"
 print(b)
}

```

这里创建的`reg()`函数使用了几个我们以前没有使用过的 R 命令。首先，由于我们将对数据框中的列集使用函数，`as.matrix()`函数用于将数据框转换成矩阵形式。接下来，`cbind()`函数用于将一个附加列绑定到`x`矩阵上；命令`Intercept = 1`指示 R 将新列命名为`Intercept`，并用重复的 1 值填充该列。然后，对`x`和`y`对象执行若干矩阵运算:

*   `solve()`取矩阵的逆矩阵
*   `t()`用于转置矩阵
*   `%*%`将两个矩阵相乘

通过如图所示组合这些，我们的函数将返回一个向量`b`，它包含将`x`关联到`y`的线性模型的估计参数。函数中的最后两行给`b`向量一个名称，并在屏幕上打印结果。

让我们将我们的函数应用于航天飞机发射数据。如以下代码所示，数据集包括三个特征和苦恼计数(`distress_ct`)，这是感兴趣的结果:

```
> str(launch)
'data.frame':      23 obs. of  4 variables:
 $ distress_ct         : int  0 1 0 0 0 0 0 0 1 1 ...
 $ temperature         : int  66 70 69 68 67 72 73 70 57 63 ...
 $ field_check_pressure: int  50 50 50 50 50 50 100 100 200 ...
 $ flight_num          : int  1 2 3 4 5 6 7 8 9 10 ...

```

通过将其结果与 O 型圈故障与温度的简单线性回归模型进行比较，我们可以确认我们的功能工作正常，我们之前发现该模型的参数为 *a = 3.70* 和 *b = -0.048* 。由于温度在启动数据的第三列，我们可以如下运行`reg()`功能:

```
> reg(y = launch$distress_ct, x = launch[2])
 estimate
Intercept    3.69841270
temperature -0.04753968

```

这些值与我们之前的结果完全匹配，因此让我们使用该函数来构建一个多元回归模型。我们将像以前一样应用它，但这次指定三列数据，而不是一列:

```
> reg(y = launch$distress_ct, x = launch[2:4])
 estimate
Intercept             3.527093383
temperature          -0.051385940
field_check_pressure  0.001757009
flight_num            0.014292843

```

该模型预测了 O 形圈损坏事件的数量与温度、现场检查压力和发射 ID 号的关系。与简单的线性回归模型一样，温度变量的系数为负，这表明随着温度的升高，预期的 O 型圈事件的数量减少。现场检查压力指的是在启动前对 O 型环进行测试所施加的压力量。虽然检查压力最初为 50 psi，但对于一些发射，它被提高到 100 和 200 psi，这导致一些人认为它可能是 O 形环腐蚀的原因。系数是正的，但是很小。航班号包括在内，以说明航天飞机的年龄。随着年龄的增长，它的零件可能会变得更脆或更容易出故障。航班号和遇难人数之间微小的正相关可能反映了这一事实。

到目前为止，我们只是触及了线性回归建模的表面。虽然我们的工作有助于帮助我们准确理解回归模型是如何构建的，但 R 的函数还包括一些更复杂的建模任务所需的附加功能，以及帮助模型解释和评估拟合所需的诊断输出。让我们将我们的回归知识应用到更具挑战性的学习任务中。



# 示例–使用线性回归预测医疗费用

为了让一家健康保险公司赚钱，它需要每年收取的保费超过它在受益人医疗保健上的花费。因此，保险公司投入大量的时间和金钱来开发能够准确预测投保人群医疗费用的模型。

医疗费用很难估计，因为最昂贵的条件是罕见的，似乎是随机的。尽管如此，某些疾病在某些人群中更为普遍。例如，吸烟者比不吸烟者更容易患肺癌，肥胖人群更容易患心脏病。

这项分析的目的是利用患者数据来估计这些人群的平均医疗费用。这些估计可用于创建精算表，根据预期的治疗成本，将每年的保费价格设定得更高或更低。

## 第一步——收集数据

对于此分析，我们将使用一个包含美国患者假想医疗费用的模拟数据集。该数据是本书使用美国人口普查局的人口统计数据创建的，因此，大致反映了真实世界的情况。

### 提示

如果您希望以交互方式进行，请从 Packt Publishing 网站下载`insurance.csv`文件，并将其保存到您的 R 工作文件夹中。

`insurance.csv`文件包括 1，338 名目前参加保险计划的受益人，其特征表明了患者的特征以及该计划在该日历年的总医疗费用。其特点是:

*   `age`:整数，表示主要受益人的年龄(不包括 64 岁以上的人，因为他们通常由政府承保)。
*   `sex`:投保人性别，男性或女性。
*   `bmi`:体重指数(身体质量指数)，它提供了一个人相对于身高超重或体重不足的感觉。身体质量指数等于体重(公斤)除以身高(米)的平方。理想的身体质量指数在 18.5 到 24.9 之间。
*   `children`:表示保险计划覆盖的子女/家属人数的整数。
*   `smoker`:是或否分类变量，表示被保险人是否经常吸烟。
*   `region`:受益人在美国的居住地，分为四个地理区域:东北、东南、西南或西北。

重要的是要考虑这些变量与医疗费用的关系。例如，我们可能会认为老年人和吸烟者面临高额医疗费用的风险更高。与许多其他机器学习方法不同，在回归分析中，特征之间的关系通常由用户指定，而不是自动检测。我们将在下一节探讨这些潜在的关系。

## 步骤 2——探索和准备数据

正如我们之前对所做的那样，我们将使用`read.csv()`函数加载数据进行分析。我们可以安全地使用`stringsAsFactors = TRUE`，因为它适合将三个名义变量转换为因子:

```
> insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)

```

`str()`函数确认数据的格式符合我们的预期:

```
> str(insurance)
'data.frame':     1338 obs. of  7 variables:
 $ age     : int  19 18 28 33 32 31 46 37 37 60 ...
 $ sex     : Factor w/ 2 levels "female","male": 1 2 2 2 2 1 ...
 $ bmi     : num  27.9 33.8 33 22.7 28.9 25.7 33.4 27.7 ...
 $ children: int  0 1 3 0 0 0 1 3 2 0 ...
 $ smoker  : Factor w/ 2 levels "no","yes": 2 1 1 1 1 1 1 1 ...
 $ region  : Factor w/ 4 levels "northeast","northwest",..: ...
 $ expenses: num  16885 1726 4449 21984 3867 ...

```

我们的模型的因变量是`expenses`，它测量了一年中每个人在保险计划中支付的医疗费用。在建立回归模型之前，检查正态性通常是有帮助的。尽管线性回归并不严格要求正态分布的因变量，但当这是真的时，模型通常更适合。让我们来看看汇总统计数据:

```
> summary(insurance$expenses)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 1122    4740    9382   13270   16640   63770

```

因为平均值大于中位数，这意味着保险费用的分布是右偏的。我们可以使用直方图直观地证实这一点:

```
> hist(insurance$expenses)

```

输出如下所示:

![Step 2 – exploring and preparing the data](img/B03905_06_19.jpg)

正如所料，该图显示了右偏分布。它还表明，我们数据中的大多数人每年的医疗费用在 0 到 15，000 美元之间，尽管分布的尾部远远超过了这些峰值。虽然这种分布对于线性回归来说并不理想，但是提前了解这种弱点可以帮助我们在以后设计更好的模型。

在我们解决这个问题之前，还有一个问题摆在我们面前。回归模型要求每个特征都是数字，然而我们的数据框架中有三个因子类型的特征。例如，性别变量分为男性和女性级别，而吸烟者分为是和否。从`summary()`输出中，我们知道`region`变量有四个级别，但我们需要更仔细地看看它们是如何分布的:

```
> table(insurance$region)
northeast northwest southeast southwest
 324       325       364       325

```

在这里，我们看到数据几乎均匀地分布在四个地理区域。我们将很快看到 R 的线性回归函数如何处理这些因素变量。

### 探索特性之间的关系——关联矩阵

在将回归模型与数据拟合之前，确定自变量与因变量之间的关系以及自变量与因变量之间的关系会很有用。相关矩阵**提供了这些关系的快速概述。给定一组变量，它为每个成对的关系提供一个相关性。**

要为保险数据框中的四个数字变量创建相关矩阵，使用`cor()`命令:

```
> cor(insurance[c("age", "bmi", "children", "expenses")])
 age        bmi   children   expenses
age      1.0000000 0.10934101 0.04246900 0.29900819
bmi      0.1093410 1.00000000 0.01264471 0.19857626
children 0.0424690 0.01264471 1.00000000 0.06799823
expenses 0.2990082 0.19857626 0.06799823 1.00000000

```

在每个行和列对的相交处，为该行和列指示的变量列出相关性。对角线总是`1.0000000`，因为变量与其自身之间总是存在完美的相关性。对角线上方和下方的值是相同的，因为相关性是对称的。换句话说，`cor(x, y)`等于`cor(y, x)`。

矩阵中没有一个相关性被认为是强的，但是有一些值得注意的关联。例如，`age`和`bmi`似乎有微弱的正相关性，这意味着随着年龄的增长，他们的体重往往会增加。`age`与`expenses`、`bmi`与`expenses`、`children`与`expenses`之间也存在中度正相关。这些关联意味着随着年龄、体重和孩子数量的增加，保险的预期成本也会上升。当我们建立最终的回归模型时，我们将试图更清楚地梳理出这些关系。

### 可视化特征之间的关系——散点图矩阵

使用散点图也有助于可视化数字特征之间的关系。虽然我们可以为每种可能的关系创建一个散点图，但是对大量的特征这样做可能会变得很乏味。

另一种选择是创建一个**散点图矩阵**(有时缩写为 **SPLOM** )，它只是以网格形式排列的散点图的集合。它用于检测三个或更多变量之间的模式。散点图矩阵不是真正的多维可视化，因为一次只检查两个特征。尽管如此，它还是提供了数据如何相互关联的一般概念。

我们可以使用 R 的图形功能来创建四个数字特征的散点图矩阵:`age`、`bmi`、`children`和`expenses`。默认 R 安装中提供了`pairs()`功能，它提供了生成散点图矩阵的基本功能。要调用该函数，只需向它提供要呈现的数据框。这里，我们将把`insurance`数据框限制为四个感兴趣的数字变量:

```
> pairs(insurance[c("age", "bmi", "children", "expenses")])

```

这将生成下图:

![Visualizing relationships among features – the scatterplot matrix](img/B03905_06_20.jpg)

在散点图矩阵中，每行和每列的交叉点保存由行和列对指示的变量的散点图。对角线上下的图是由于 *x* 轴和 *y* 轴互换后的位置。

你注意到这些图中的模式了吗？虽然有些看起来像随机的点云，但有些似乎显示出一些趋势。`age`和`expenses`之间的关系显示了几条相对直线，而`bmi`对`expenses`的曲线有两组不同的点。很难在其他任何一幅图中发现趋势。

如果我们在情节中加入更多的信息，它会更加有用。增强的散点图矩阵可通过`psych`包中的`pairs.panels()`功能创建。如果您还没有安装这个包，请键入`install.packages("psych")`将其安装到您的系统上，并使用`library(psych)`命令加载它。然后，我们可以像之前一样创建一个散点图矩阵:

```
> pairs.panels(insurance[c("age", "bmi", "children", "expenses")])

```

这将生成一个信息量稍大的散点图矩阵，如下所示:

![Visualizing relationships among features – the scatterplot matrix](img/B03905_06_21.jpg)

在对角线上方，散点图已被相关矩阵取代。在对角线上，显示了描述每个特征的值的分布的直方图。最后，对角线下方的散点图现在显示了额外的视觉信息。

每个散点图上的椭圆形物体是一个**相关椭圆**。它提供了相关强度的可视化。椭圆中心的点表示 *x* 和 *y* 轴变量平均值处的点。这两个变量之间的相关性由椭圆的形状表示；拉伸得越多，相关性越强。与`bmi`和`children`一样，几乎完美的圆形椭圆表示非常弱的相关性(在这种情况下，为 0.01)。

在散点图上绘制的曲线称为**黄土曲线**。表示 *x* 和 *y* 轴变量之间的大致关系。最好通过例子来理解。`age`和`children`的曲线是一个倒 U 型，在中年左右达到顶峰。这意味着样本中年龄最大和最小的人比中年人有更少的孩子参加保险计划。因为这种趋势是非线性的，这一发现可能不能仅从相关性中推断出来。另一方面，`age`和`bmi`的黄土曲线是一条逐渐向上倾斜的线，这意味着体重随年龄增加，但我们已经从相关矩阵中推断出这一点。

## 第 3 步–根据数据训练模型

为了将线性回归模型拟合到 R 数据，可以使用`lm()`函数。这包含在`stats`包中，默认情况下，它应该包含在您的 R 安装中。`lm()`的语法如下:

![Step 3 – training a model on the data](img/B03905_06_22.jpg)

以下命令拟合了一个线性回归模型，该模型将六个独立变量与总医疗费用相关联。R 公式语法使用波浪号字符`~`来描述模型；因变量`expenses`位于波形符的左侧，而自变量位于右侧，由`+`符号分隔。不需要指定回归模型的截距项，因为它是默认假设的:

```
> ins_model <- lm(expenses ~ age + children + bmi + sex +
 smoker + region, data = insurance)

```

因为`.`字符可以用来指定所有的特征(不包括那些已经在公式中指定的)，所以下面的命令等同于前面的命令:

```
> ins_model <- lm(expenses ~ ., data = insurance)

```

构建模型后，只需输入模型对象的名称即可查看估计的 beta 系数:

```
> ins_model

Call:
lm(formula = expenses ~ ., data = insurance)

Coefficients:
 (Intercept)              age          sexmale 
 -11941.6            256.8           -131.4 
 bmi         children        smokeryes 
 339.3            475.7          23847.5 
regionnorthwest  regionsoutheast  regionsouthwest 
 -352.8          -1035.6           -959.3

```

理解回归系数相当简单。截距是自变量等于零时`expenses`的预测值。就像这里的情况一样，截距本身通常没有什么价值，因为不可能所有特征的值都为零。例如，由于不存在年龄为零和身体质量指数为零的人，截距没有真实世界的解释。由于这个原因，在实践中，截距常常被忽略。

假设所有其他值保持不变，贝塔系数表明每项功能增加一项的估计费用增加。例如，在其他条件相同的情况下，年龄每增加一岁，我们预计医疗费用平均会增加 256.80 美元。同样，在其他条件相同的情况下，每多生一个孩子，每年的医疗费用平均增加 475.70 美元，身体质量指数每增加一个单位，每年的医疗费用平均增加 339.30 美元。

您可能会注意到，虽然我们在模型公式中只指定了六个特征，但是除了截距之外，还报告了八个系数。这是因为`lm()`函数自动将一种称为**虚拟编码**的技术应用于我们包含在模型中的每个因子类型变量。

虚拟编码通过为特征的每个类别创建一个二进制变量，通常称为**虚拟** **变量**，允许将名义特征视为数字。如果观察值属于指定类别，虚拟变量设置为`1`，否则设置为`0`。例如，`sex`特征有两类:`male`和`female`。这将被拆分成两个二进制变量，R 分别命名为`sexmale`和`sexfemale`。对于`sex = male`处的观察值，则`sexmale = 1`和`sexfemale = 0`；反之，如果`sex = female`，则`sexmale = 0`和`sexfemale = 1`。同样的编码适用于具有三个或更多类别的变量。例如，R 将四类特征`region`拆分成四个虚拟变量:`regionnorthwest`、`regionsoutheast`、`regionsouthwest`和`regionnortheast`。

向回归模型中添加虚拟变量时，总会遗漏一个类别作为参考类别。然后相对于参考值解释估计值。在我们的模型中，R 自动给出了`sexfemale`、`smokerno`和`regionnortheast`变量，使东北地区的女性非吸烟者成为参照组。因此，男性每年的医疗费用比女性少 131.40 美元，吸烟者每年的医疗费用比不吸烟者平均多 23，847.50 美元。模型中三个地区的系数都是负的，这意味着参照组东北地区的平均费用最高。

### 提示

默认情况下，R 使用因子变量的第一级作为参考。如果您喜欢使用另一个级别，可以使用`relevel()`功能手动指定参考组。更多信息请使用 R 中的`?relevel`命令。

线性回归模型的结果合乎逻辑:老年、吸烟和肥胖往往与额外的健康问题有关，而额外的家庭成员依赖可能导致医生就诊和预防保健(如疫苗接种和年度体检)的增加。然而，我们目前还不知道模型与数据的吻合程度。我们将在下一节回答这个问题。

## 步骤 4–评估模型性能

我们通过键入`ins_model`获得的参数估计值告诉我们自变量与因变量之间的关系，但它们没有告诉我们模型与我们的数据的拟合程度。为了评估模型性能，我们可以对存储的模型使用`summary()`命令:

```
> summary(ins_model)

```

这会产生以下输出。请注意，出于说明目的，输出已标记为:

![Step 4 – evaluating model performance](img/B03905_06_23.jpg)

最初，`summary()`输出可能看起来令人困惑，但是基本原理很容易掌握。如前面输出中的编号标签所示，输出提供了三种评估模型性能或适合度的关键方法:

1.  **残差**部分提供了我们预测中误差的汇总统计，其中一些误差显然相当大。由于残差等于真实值减去预测值，最大误差 29981.7 表明该模型至少在一次观察中少预测了近 30，000 美元的费用。另一方面，50%的误差落在 1Q 和 3Q 值(第一和第三个四分位数)内，因此大多数预测在高于真实值 2，850.90 美元和低于真实值 1，383.90 美元之间。
2.  对于每个估计的回归系数， **p 值**，表示为`Pr(>|t|)`，提供了给定估计值时真实系数为零的概率估计。小 p 值表明真实系数极不可能为零，这意味着该特征极不可能与因变量没有关系。请注意，一些 p 值带有星号(`***`)，对应于脚注，表示估计值满足的**显著性水平**。这一水平是一个阈值，在建立模型之前选择，用于表示“真实”的发现，而不是仅仅由于偶然发现的结果；小于显著性水平的 p 值被视为**具有统计显著性**。如果模型中没有几个这样的术语，这可能会引起关注，因为这将表明所使用的特征并不能很好地预测结果。在这里，我们的模型有几个非常重要的变量，它们似乎以逻辑的方式与结果相关。
3.  **多个 R 平方值**(也称为决定系数)提供了一个度量，衡量我们的模型作为一个整体如何解释因变量的值。它类似于相关系数，因为该值越接近 1.0，模型就越能完美地解释数据。由于 R 平方值为 0.7494，我们知道该模型解释了因变量中近 75%的变化。因为具有更多特征的模型总是解释更多的变化，**调整的 R 平方值**通过惩罚具有大量独立变量的模型来校正 R 平方。这对于比较具有不同数量解释变量的模型的性能很有用。

鉴于前面三个性能指标，我们的模型表现相当不错。现实世界数据的回归模型具有相当低的 R 平方值并不罕见；0.75 的值其实已经很不错了。一些错误的大小有点令人担忧，但鉴于医疗费用数据的性质，这并不奇怪。然而，如下一节所示，我们可以通过稍微不同的方式来指定模型，从而提高模型的性能。

## 第 5 步——提高模型性能

正如前面提到的，回归建模和其他机器学习方法之间的一个关键区别是，回归通常将特征选择和模型规范留给用户。因此，如果我们有关于一个特性如何与结果相关的主题知识，我们可以使用这个信息来通知模型规范，并潜在地改进模型的性能。

### 模型规格–添加非线性关系

在线性回归中，自变量和因变量之间的关系被假定为线性，然而这不一定是真的。例如，年龄对医疗支出的影响在所有年龄值中可能不是恒定的；对于年龄最大的人群来说，治疗费用可能会高得不成比例。

如果您还记得，典型的回归方程遵循如下形式:

![Model specification – adding non-linear relationships](img/B03905_06_24.jpg)

为了说明非线性关系，我们可以将高阶项添加到回归模型中，将模型视为多项式。实际上，我们将建立这样的关系模型:

![Model specification – adding non-linear relationships](img/B03905_06_25.jpg)

这两个模型的不同之处在于将会估计一个额外的β，这是为了捕捉 *x* 平方项的影响。这使得年龄的影响可以作为年龄平方的函数来衡量。

要将非线性年龄添加到模型中，我们只需创建一个新变量:

```
> insurance$age2 <- insurance$age^2

```

然后，当我们生产改进的模型时，我们将使用`expenses ~ age + age2`形式将`age`和`age2`添加到`lm()`公式中。这将使模型能够区分年龄对医疗费用的线性和非线性影响。

### 转换–将数值变量转换为二进制指示器

假设我们有一种预感，某个特性的效果不是累积的，而是只有在达到特定阈值后才会产生效果。例如，身体质量指数可能对正常体重范围内的个人的医疗支出没有影响，但它可能与肥胖患者(即身体质量指数 30 或以上)的较高费用密切相关。

我们可以通过创建一个二元肥胖指标变量来模拟这种关系，如果身体质量指数至少为 30，则该变量为 1，如果小于 30，则为 0。该二元特征的估计β将指示相对于身体质量指数小于 30 的个体，身体质量指数为 30 或以上的个体对医疗费用的平均净影响。

为了创建这个特性，我们可以使用`ifelse()`函数，该函数为向量中的每个元素测试一个指定的条件，并根据条件是真还是假返回一个值。对于大于等于 30 的身体质量指数，我们将返回`1`，否则返回`0`:

```
> insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)

```

然后，我们可以在我们改进的模型中包括`bmi30`变量，或者替换原来的`bmi`变量，或者另外，这取决于我们是否认为肥胖的影响是在单独的线性身体质量指数效应之外发生的。如果没有好的理由，我们将在最终的模型中包含这两者。

### 提示

如果您在决定是否包含一个变量时有困难，通常的做法是包含它并检查 p 值。如果变量在统计上不显著，你就有证据支持将来排除它。

### 模型规格–添加交互效果

到目前为止，我们只考虑了每个特性对结果的贡献。如果某些特征对因变量有综合影响呢？例如，吸烟和肥胖可能分别产生有害影响，但有理由假设它们的综合影响可能比每一个单独因素的总和还要糟糕。

当两个特征具有组合效果时，这被称为**交互**。如果我们怀疑两个变量相互作用，我们可以通过将它们的相互作用添加到模型中来检验这一假设。使用 R 公式语法指定交互效果。为了让肥胖指标(`bmi30`)和吸烟指标(`smoker`)相互作用，我们将以`expenses ~ bmi30*smoker`的形式编写一个公式。

`*`操作符是指示 R 对`expenses ~ bmi30 + smokeryes + bmi30:smokeryes`建模的简写。展开形式的`:`(冒号)运算符表示`bmi30:smokeryes`是两个变量之间的相互作用。注意扩展的表单也自动包含了`bmi30`和`smoker`变量以及交互。

### 提示

在没有添加每个交互变量的情况下，交互永远不应该包含在模型中。如果您总是使用`*`操作符来创建交互，这将不是一个问题，因为 R 会自动添加所需的组件。

### 将所有这些放在一起——一个改进的回归模型

基于关于医疗成本如何与患者特征相关的一点主题知识，我们开发了一个我们认为更精确的回归公式。为了总结这些改进，我们:

*   为年龄增加了一个非线性项
*   创造了一个肥胖指标
*   详细说明了肥胖和吸烟之间的相互作用

我们将像以前一样使用`lm()`函数训练模型，但是这次我们将添加新构造的变量和交互项:

```
> ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
 bmi30*smoker + region, data = insurance)

```

接下来，我们总结结果:

```
> summary(ins_model2)

```

输出如下所示:

![Putting it all together – an improved regression model](img/B03905_06_26.jpg)

模型拟合统计有助于确定我们的更改是否提高了回归模型的性能。相对于我们的第一个模型，R 平方值从 0.75 提高到大约 0.87。类似地，考虑到模型复杂性增长的事实，调整后的 R 平方值也从 0.75 提高到 0.87。我们的模型现在可以解释 87%的医疗费用变化。此外，我们关于模型的函数形式的理论似乎是有效的。高阶的`age2`项具有统计学意义，肥胖指标`bmi30`也是如此。肥胖和吸烟之间的相互作用显示了巨大的影响；除了因吸烟而增加的超过 13，404 美元的成本外，肥胖吸烟者每年还要花费 19，810 美元。这可能表明吸烟会加剧与肥胖相关的疾病。

### 注

严格地说，回归建模对数据做了一些强有力的假设。这些假设对于数字预测来说并不重要，因为模型的价值并不基于它是否真正捕捉到了潜在的过程——我们只是关心其预测的准确性。但是，如果您想从回归模型系数中做出可靠的推断，则有必要运行诊断测试，以确保回归假设没有被违反。关于这个主题的精彩介绍，请参见 Allison PD *。多元回归:入门*。松树锻压机；1998.



# 了解回归树和模型树

如果你还记得《T4》第 5 章、*分而治之——使用决策树和规则分类*，决策树构建了一个很像流程图的模型，其中决策节点、叶节点和分支定义了一系列用于分类示例的决策。这种树也可以用于数值预测，只需对树生长算法做一些小的调整。在本节中，我们将只考虑用于数值预测的树与用于分类的树的不同之处。

用于数值预测的树分为两类。第一种被称为**回归树**，在 20 世纪 80 年代作为开创性的**分类和回归树** ( **CART** )算法的一部分被引入。尽管名称如此，回归树并不使用本章前面描述的线性回归方法，而是基于到达叶子的样本的平均值进行预测。

### 注意

CART 算法在 Breiman L，Friedman JH，Stone CJ，Olshen RA。*分类和回归树*。加利福尼亚州贝尔蒙特:查普曼和霍尔；1984.

用于数值预测的第二种树被称为**模型树**。比回归树晚推出几年，它们不太为人所知，但可能更强大。模型树的生长方式与回归树非常相似，但是在每一片叶子上，多重线性回归模型是从到达该节点的例子中构建的。根据叶节点的数量，一个模型树可以构建几十个甚至几百个这样的模型。这可能使模型树比等价的回归树更难理解，好处是它们可以产生更精确的模型。

### 注

最早的模型树算法， **M5** ，在小昆兰*用连续类学习*中有描述。第五届澳大利亚人工智能联合会议录。1992:343-348.

## 给树添加回归

能够执行数值预测的树提供了一个引人注目但经常被忽视的回归建模替代方案。下表列出了回归树和模型树相对于更常见的回归方法的优势和劣势:

| 

强项

 | 

弱点

 |
| --- | --- |
| 

*   结合了决策树的优势和对数字数据建模的能力
*   不需要用户预先指定模型
*   使用自动特征选择，这使得该方法可以用于大量特征
*   可能比线性回归更适合某些类型的数据
*   不需要统计学知识来解释模型

 | 

*   不如线性回归知名
*   需要大量的训练数据
*   难以确定个体特征对结果的整体净效应
*   大树会变得比回归模型更难解释

 |

虽然传统的回归方法通常是数字预测任务的首选，但在某些情况下，数字决策树具有明显的优势。例如，决策树可能更适合于具有许多特征或者特征和结果之间有许多复杂的非线性关系的任务。这些情况对回归提出了挑战。回归建模还对数字数据如何分布做出假设，这在现实世界的数据中经常被违反。对于树木来说，情况并非如此。

数值预测树的构建方式与分类树非常相似。从根节点开始，使用分而治之的策略，根据执行分割后将导致结果同质性最大增加的特征对数据进行分割。在分类树中，您会记得同质性是由熵来度量的，而熵对于数字数据是不确定的。相反，对于数字决策树，同质性是通过统计来衡量的，如方差、标准偏差或与平均值的绝对偏差。

一个常见的分割标准被称为**标准差缩减** ( **SDR** )。它由以下公式定义:

![Adding regression to trees](img/B03905_06_27.jpg)

在此公式中， *sd(T)* 函数是指集合 *T* 中值的标准差，而 *T [1]* ， *T [2]* ，...， *T [n]* 是对特征进行分割后得到的值的集合。 *|T|* 项是指集合 *T* 中个观测值的个数。本质上，该公式通过比较分割前的标准偏差与分割后的加权标准偏差来测量标准偏差的减少。

例如，考虑以下情况，其中树决定是否对二元特征 A 或 B 执行分割:

![Adding regression to trees](img/B03905_06_28.jpg)

使用建议拆分产生的组，我们可以如下计算 A 和 B 的 SDR。这里使用的`length()`函数返回向量中元素的数量。注意，整个组 T 被命名为`tee`以避免覆盖 R 的内置`T()`和`t()`函数:

```
> tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
> at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
> at2 <- c(6, 6, 7, 7, 7, 7)
> bt1 <- c(1, 1, 1, 2, 2, 3, 4)
> bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)
> sdr_a <- sd(tee) - (length(at1) / length(tee) * sd(at1) +
 length(at2) / length(tee) * sd(at2))
> sdr_b <- sd(tee) - (length(bt1) / length(tee) * sd(bt1) +
 length(bt2) / length(tee) * sd(bt2))

```

让我们比较一下 A 的 SDR 和 B 的 SDR:

```
> sdr_a
[1] 1.202815
> sdr_b
[1] 1.392751

```

特征 A 上的分割的 SDR 约为 1.2，而特征 B 上的分割的 SDR 约为 1.4。由于特征 B 上的分割的标准差减少得更多，所以决策树将首先使用 B。它产生的集合比。

假设这棵树在这里停止生长，只是裂开了。回归树的工作已经完成。它可以根据特征 B 上的示例值是将该示例放入组*T[1]还是*T[2]来对新示例进行预测。如果示例以*T[1]结束，模型将预测*平均值(bt1) = 2* ，否则将预测*平均值(bt2) = 6.25* 。***

相比之下，模型树会更进一步。使用属于组*T[1]的七个训练示例和属于组*T[2]的八个训练示例， 模型树可以构建结果与特征 a 的线性回归模型。请注意，特征 B 对构建回归模型没有帮助，因为叶子上的所有示例都具有相同的 B 值，它们根据 B 值被放入*T[1]或*T[2]中。然后，模型树可以使用两个线性模型中的任何一个对新示例进行预测。****

为了进一步说明这两种方法之间的区别，让我们来看一个真实的例子。



# 示例——用回归树和模型树评估葡萄酒的质量

酿酒业是一个充满挑战和竞争的行业，提供了巨大的利润潜力。然而，有许多因素有助于酒厂的盈利能力。作为一种农产品，天气和生长环境等各种变量都会影响品种的质量。装瓶和生产也能更好或更坏地影响风味。甚至产品的营销方式，从瓶子设计到价格点，都会影响顾客对味道的感知。

因此，酿酒业在数据收集和机器学习方法方面投入了大量资金，这些方法可能有助于酿酒决策科学。例如，机器学习已被用于发现不同地区葡萄酒化学成分的关键差异，或识别导致葡萄酒尝起来更甜的化学因素。

最近，机器学习被用来帮助评定葡萄酒的质量——这是一项众所周知的困难任务。一位著名的葡萄酒评论家撰写的评论通常会决定产品最终是放在货架的顶部还是底部，尽管事实上，在盲测中给葡萄酒评级时，即使是专家评委也不一致。

在这个案例研究中，我们将使用回归树和模型树来创建一个能够模仿葡萄酒专家评级的系统。因为树状图产生了一个易于理解的模型，这可以让酿酒师识别出有助于更好评价葡萄酒的关键因素。也许更重要的是，该系统不会受到品尝过程中人为因素的影响，如评分者的情绪或味觉疲劳。因此，计算机辅助葡萄酒测试可能会产生更好的产品，以及更客观、一致和公平的评级。

## 步骤 1–收集数据

为了开发葡萄酒评级模型，我们将使用由 P. Cortez、A. Cerdeira、F. Almeida、T. Matos 和 J. Reis 捐赠给 UCI 机器学习数据库([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))的数据。这些数据包括来自葡萄牙的红葡萄酒和白葡萄酒，葡萄牙是世界领先的葡萄酒生产国之一。因为红葡萄酒和白葡萄酒的高评级因素可能有所不同，所以在本次分析中，我们将只考察更受欢迎的白葡萄酒。

### 提示

按照这个例子，从 Packt Publishing 网站下载`whitewines.csv`文件，并保存到您的 R 工作目录中。如果您想自己探索这些数据，也可以使用`redwines.csv`文件。

白葡萄酒数据包括 4，898 个葡萄酒样本的 11 种化学特性的信息。对于每种葡萄酒，实验室分析会测量酸度、含糖量、氯化物、硫、酒精、pH 值和密度等特性。然后，由不少于三名评委组成的小组在盲品中对样品进行评级，质量等级从 0(非常差)到 10(非常好)。如果法官不同意评级，则使用中间值。

Cortez 的这项研究评估了三种机器学习方法对葡萄酒数据进行建模的能力:多元回归、人工神经网络和支持向量机。我们在本章早些时候讨论了多元回归，我们将在第 7 章、*黑盒方法-神经网络和支持向量机*中了解神经网络和支持向量机。研究发现，支持向量机提供的结果明显优于线性回归模型。然而，与回归不同，支持向量机模型很难解释。使用回归树和模型树，我们可能能够改进回归结果，同时仍然有一个易于理解的模型。

### 注意

要了解更多关于此处描述的葡萄酒研究的信息，请参考 Cortez P、Cerdeira A、Almeida F、Matos T、Reis J. *通过物理化学性质的数据挖掘建立葡萄酒偏好模型*。决策支持系统*。*2009；47:547-553.

## 第 2 步——探索和准备数据

像往常一样，我们将使用`read.csv()`函数将数据加载到 r 中。由于所有的特性都是数字的，我们可以安全地忽略`stringsAsFactors`参数:

```
> wine <- read.csv("whitewines.csv")

```

葡萄酒数据包括 11 个特征和质量结果，如下所示:

```
> str(wine)
'data.frame':  4898 obs. of  12 variables:
 $ fixed.acidity       : num  6.7 5.7 5.9 5.3 6.4 7 7.9 ...
 $ volatile.acidity    : num  0.62 0.22 0.19 0.47 0.29 0.12 ...
 $ citric.acid         : num  0.24 0.2 0.26 0.1 0.21 0.41 ...
 $ residual.sugar      : num  1.1 16 7.4 1.3 9.65 0.9 ...
 $ chlorides           : num  0.039 0.044 0.034 0.036 0.041 ...
 $ free.sulfur.dioxide : num  6 41 33 11 36 22 33 17 34 40 ...
 $ total.sulfur.dioxide: num  62 113 123 74 119 95 152 ...
 $ density             : num  0.993 0.999 0.995 0.991 0.993 ...
 $ pH                  : num  3.41 3.22 3.49 3.48 2.99 3.25 ...
 $ sulphates           : num  0.32 0.46 0.42 0.54 0.34 0.43 ...
 $ alcohol             : num  10.4 8.9 10.1 11.2 10.9 ...
 $ quality             : int  5 6 6 4 6 6 6 6 6 7 ...

```

与其他类型的机器学习模型相比，树的一个优点是可以处理多种类型的数据，无需预处理。这意味着我们不需要标准化或规范化这些特性。

然而，需要花点功夫来检查结果变量的分布，以告知我们对模型性能的评估。例如，假设不同葡萄酒的质量差异很小，或者葡萄酒呈双峰分布:要么很好，要么很差。为了检查这种极端情况，我们可以使用直方图来检查质量的分布:

```
> hist(wine$quality)

```

该产生如下图:

![Step 2 – exploring and preparing the data](img/B03905_06_29.jpg)

葡萄酒质量值似乎遵循一个相当正常的钟形分布，以 6 为中心。这在直觉上是有意义的，因为大多数葡萄酒质量一般；很少有特别差或者特别好的。虽然这里没有显示结果，但是检查`summary(wine)`输出中的异常值或其他潜在的数据问题也是有用的。尽管树对于杂乱的数据相当健壮，但是检查严重的问题总是谨慎的。现在，我们假设数据是可靠的。

然后，我们的最后一步是分成训练和测试数据集。由于`wine`数据集已经按随机顺序排序，我们可以按如下方式划分成两组连续的行:

```
> wine_train <- wine[1:3750, ]
> wine_test <- wine[3751:4898, ]

```

为了反映 Cortez 使用的条件，我们分别使用 75%和 25%的集合进行训练和测试。我们将根据测试数据评估我们基于树的模型的性能，看看我们是否可以获得与之前的研究相当的结果。

## 步骤 3–根据数据训练模型

我们将从训练一个回归树模型开始。尽管几乎任何决策树的实现都可以用来执行回归树建模，但是正如 CART 团队所描述的那样，`rpart`(递归划分)包提供了回归树的最忠实的实现。作为 CART 的经典 R 实现，`rpart`包也有很好的文档记录，并支持可视化和评估`rpart`模型的功能。

使用`install.packages("rpart")`命令安装`rpart`包。然后可以使用`library(rpart)`命令将它加载到 R 会话中。下面的语法将使用默认设置来训练一个树，这通常相当有效。如果您需要更精细的设置，请参考使用`?rpart.control`命令的控制参数文件。

![Step 3 – training a model on the data](img/B03905_06_30.jpg)

使用 R 公式接口，我们可以将`quality`指定为结果变量，并使用点符号允许`wine_train`数据框中的所有其他列用作预测值。生成的回归树模型对象被命名为`m.rpart`,以区别于我们稍后将训练的模型树:

```
> m.rpart <- rpart(quality ~ ., data = wine_train)

```

关于树的基本信息，只需输入模型对象的名称:

```
> m.rpart
n= 3750

node), split, n, deviance, yval
 * denotes terminal node

 1) root 3750 2945.53200 5.870933 
 2) alcohol< 10.85 2372 1418.86100 5.604975 
 4) volatile.acidity>=0.2275 1611  821.30730 5.432030 
 8) volatile.acidity>=0.3025 688  278.97670 5.255814 *
 9) volatile.acidity< 0.3025 923  505.04230 5.563380 *
 5) volatile.acidity< 0.2275 761  447.36400 5.971091 *
 3) alcohol>=10.85 1378 1070.08200 6.328737 
 6) free.sulfur.dioxide< 10.5 84   95.55952 5.369048 *
 7) free.sulfur.dioxide>=10.5 1294  892.13600 6.391036 
 14) alcohol< 11.76667 629  430.11130 6.173291 
 28) volatile.acidity>=0.465 11   10.72727 4.545455 *
 29) volatile.acidity< 0.465 618  389.71680 6.202265 *
 15) alcohol>=11.76667 665  403.99400 6.596992 *

```

对于树中的每个节点，列出了到达决策点的示例数量。例如，所有 3，750 个示例都从根节点开始，其中 2，372 个有`alcohol < 10.85`，1，378 个有`alcohol >= 10.85`。因为酒精是第一次出现在树上，所以它是葡萄酒质量最重要的预测指标。

由`*`指示的节点是终端或叶节点，这意味着它们产生预测(这里列为`yval`)。例如，节点 5 的`yval`为 5.971091。当该树用于预测时，任何带有`alcohol < 10.85`和`volatile.acidity < 0.2275`的葡萄酒样本将因此被预测为具有 5.97 的品质值。

使用`summary(m.rpart)`命令可以获得更详细的树拟合摘要，包括每个节点的均方误差和特征重要性的总体度量。

### 可视化决策树

虽然只使用前面的输出就可以理解树，但是使用可视化通常更容易理解。Stephen Milborrow 的`rpart.plot`包提供了一个易于使用的函数，可以生成出版物质量的决策树。

### 注

有关`rpart.plot`的更多信息，包括该功能可以生成的决策树形图类型的其他示例，请参考作者在[http://www.milbo.org/rpart-plot/](http://www.milbo.org/rpart-plot/)的网站。

在使用`install.packages("rpart.plot")`命令安装包之后，`rpart.plot()`函数从任何`rpart`模型对象生成一个树形图。以下命令绘制了我们之前构建的回归树:

```
> library(rpart.plot)
> rpart.plot(m.rpart, digits = 3)

```

生成的树形图如下:

![Visualizing decision trees](img/B03905_06_31.jpg)

除了控制图中包含的数字位数的参数`digits`之外，可视化的许多其他方面也可以调整。下面的命令显示了一些有用的选项:`fallen.leaves`参数强制叶节点在图的底部对齐，而`type`和`extra`参数影响决策和节点的标记方式:

```
> rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE,
 type = 3, extra = 101)

```

这些变化的结果是一个看起来非常不同的树形图:

![Visualizing decision trees](img/B03905_06_32.jpg)

像这样的可视化可能有助于回归树结果的传播，因为即使没有数学背景也很容易理解。在这两种情况下，叶节点中显示的数字是到达该节点的示例的预测值。向葡萄酒生产商展示该图表有助于识别预测高等级葡萄酒的关键因素。

## 步骤 4–评估模型性能

为了使用回归树模型对测试数据进行预测，我们使用了`predict()`函数。默认情况下，这会返回结果变量的估计数值，我们将把它保存在一个名为`p.rpart`的向量中:

```
> p.rpart <- predict(m.rpart, wine_test)

```

快速浏览一下我们预测的汇总统计数据，就会发现一个潜在的问题；预测值比真实值的范围小得多:

```
> summary(p.rpart)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 4.545   5.563   5.971   5.893   6.202   6.597
> summary(wine_test$quality)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 3.000   5.000   6.000   5.901   6.000   9.000

```

这一发现表明，该模型没有正确识别极端情况，特别是最好和最差的葡萄酒。另一方面，在第一和第三个四分位数之间，我们可能做得很好。

预测质量值和实际质量值之间的相关性提供了一种简单的方法来衡量模型的性能。回想一下，`cor()`函数可用于测量两个等长向量之间的关系。我们将用它来比较预测值与真实值的对应程度:

```
> cor(p.rpart, wine_test$quality)
[1] 0.5369525

```

0.54 的相关性当然是可以接受的。然而，相关性仅度量预测与真实值的相关程度；它并不能衡量预测值与真实值的差距。

### 用平均绝对误差测量性能

考虑模型性能的另一种方式是考虑平均而言，它的预测与真实值相差多远。这种测量被称为**平均绝对误差** ( **MAE** )。MAE 的等式如下，其中 *n* 表示预测的数量，而*e[I]表示预测 *i* 的误差:*

![Measuring performance with the mean absolute error](img/B03905_06_33.jpg)

顾名思义，该方程取误差绝对值的平均值。由于误差只是预测值和实际值之间的差异，我们可以创建一个简单的`MAE()`函数，如下所示:

```
> MAE <- function(actual, predicted) {
 mean(abs(actual - predicted))
}

```

我们预测的 MAE 是:

```
> MAE(p.rpart, wine_test$quality)
[1] 0.5872652

```

这意味着，平均而言，我们的模型预测和真实质量得分之间的差异约为 0.59。在从 0 到 10 的质量等级上，这似乎表明我们的模型做得相当好。

另一方面，回想一下，大多数葡萄酒既不是很好也不是很差；典型的质量分数大约是五到六分。因此，除了预测平均值之外什么都不做的分类器根据该度量仍然可以做得相当好。

训练数据中的平均质量等级如下:

```
> mean(wine_train$quality)
[1] 5.870933

```

如果我们预测每个葡萄酒样品的值为 5.87，我们将只有大约 0.67 的平均绝对误差:

```
> MAE(5.87, wine_test$quality)
[1] 0.6722474

```

我们的回归树( *MAE = 0.59* )平均来说比估算平均值( *MAE = 0.67* )更接近真实的质量分数，但并不是非常接近。相比之下，Cortez 报告了神经网络模型的 MAE 为 0.58，支持向量机的 MAE 为 0.45。这表明还有改进的余地。

## 第 5 步——提高模型性能

为了提高我们学习者的表现，让我们试着建立一个模型树。回想一下，模型树通过用回归模型替换叶节点来改进回归树。这通常会产生比回归树更准确的结果，回归树在叶节点仅使用单个值进行预测。

当前模型树的最新技术是 Y. Wang 和 I.H. Witten 的 **M5 算法** ( **M5 素数**)，它是 J.R. Quinlan 在 1992 年提出的原始 M5 模型树算法的变体。

### 注

关于 M5 算法的更多信息，参见王 Y，威滕。*预测连续类的模型树归纳*。欧洲机器学习会议海报论文汇刊。1997.

M5 算法通过`RWeka`包和`M5P()`函数在 R 中可用。下表显示了该函数的语法。如果你还没有安装`RWeka`包，一定要安装。由于其对 Java 的依赖，安装说明包含在[第 1 章](ch01.html "Chapter 1. Introducing Machine Learning")、*介绍机器学习*中。

![Step 5 – improving model performance](img/B03905_06_34.jpg)

我们将使用与回归树基本相同的语法来拟合模型树:

```
> library(RWeka)
> m.m5p <- M5P(quality ~ ., data = wine_train)

```

可以通过键入树的名称来检查树本身。在这种情况下，树非常大，只显示输出的前几行:

```
> m.m5p
M5 pruned model tree:
(using smoothed linear models)

alcohol <= 10.85 :
|   volatile.acidity <= 0.238 :
|   |   fixed.acidity <= 6.85 : LM1 (406/66.024%)
|   |   fixed.acidity >  6.85 :
|   |   |   free.sulfur.dioxide <= 24.5 : LM2 (113/87.697%)

```

您将会注意到，分割与我们之前构建的回归树非常相似。酒精是最重要的变量，其次是挥发性酸度和游离二氧化硫。然而，一个关键的区别是，节点不是终止于数字预测，而是终止于线性模型(这里显示为`LM1`和`LM2`)。

线性模型本身将在后面的输出中显示。例如，`LM1`的型号显示在即将到来的输出中。值可以被解释为与我们在本章前面构建的多元回归模型完全相同。每个数字都是相关特征对预测葡萄酒质量的净影响。固定酸度的系数`0.266`意味着酸度每增加 1 个单位，葡萄酒质量预计会增加`0.266`:

```
LM num: 1
quality =
 0.266 * fixed.acidity
 - 2.3082 * volatile.acidity
 - 0.012 * citric.acid
 + 0.0421 * residual.sugar
 + 0.1126 * chlorides
 + 0 * free.sulfur.dioxide
 - 0.0015 * total.sulfur.dioxide
 - 109.8813 * density
 + 0.035 * pH
 + 1.4122 * sulphates
 - 0.0046 * alcohol
 + 113.1021

```

值得注意的是，由`LM1`估计的影响仅适用于到达该节点的葡萄酒样本；在这个模型树中共建立了 36 个线性模型，每个模型对固定酸度和其他 10 个特征的影响有不同的估计。

对于模型与训练数据拟合程度的统计，可将`summary()`功能应用于 M5P 模型。但是，请注意，由于这些统计数据是基于训练数据的，因此它们只能用作粗略的诊断:

```
> summary(m.m5p)

=== Summary ===

Correlation coefficient                  0.6666
Mean absolute error                      0.5151
Root mean squared error                  0.6614
Relative absolute error                 76.4921 %
Root relative squared error             74.6259 %
Total Number of Instances             3750 

```

相反，我们将看看模型在看不见的测试数据上表现如何。`predict()`函数给我们一个预测值的向量:

```
> p.m5p <- predict(m.m5p, wine_test)

```

模型树似乎比回归树预测更大范围的值:

```
> summary(p.m5p)
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 4.389   5.430   5.863   5.874   6.305   7.437

```

相关性似乎也高得多:

```
> cor(p.m5p, wine_test$quality)
[1] 0.6272973

```

此外，该模型略微降低了平均绝对误差:

```
> MAE(wine_test$quality, p.m5p)
[1] 0.5463023

```

尽管我们在回归树之外没有提高太多，但我们超过了 Cortez 公布的神经网络模型的性能，并且我们越来越接近支持向量机模型公布的平均绝对误差值 0.45，这都是通过使用一种更简单的学习方法实现的。

### 提示

毫不奇怪，我们已经证实预测葡萄酒的质量是一个困难的问题；毕竟品酒本来就很主观。如果你想进行额外的练习，你可以在阅读完[第 11 章](ch11.html "Chapter 11. Improving Model Performance")、*提高模型性能*后尝试重新审视这个问题，其中涵盖了可能导致更好结果的额外技术。



# 总结

在这一章中，我们研究了两种数值数据建模的方法。第一种方法是线性回归，它是用直线来拟合数据。第二种方法使用决策树进行数值预测。后者有两种形式:回归树，使用叶节点上的样本平均值进行数值预测；以及模型树，它在每个叶节点上以混合的方式构建一个回归模型，这在某种程度上是两全其美的。

我们使用线性回归模型来计算不同人群的预期医疗费用。因为估计回归模型很好地描述了特征和目标变量之间的关系，所以我们能够识别某些人口统计数据，例如吸烟者和肥胖患者，他们可能需要支付更高的保险费率来支付高于平均水平的医疗费用。

回归树和模型树被用来从可测量的特征中模拟葡萄酒的主观质量。在此过程中，我们了解了回归树如何提供一种简单的方法来解释特征和数值结果之间的关系，但是越复杂的模型树可能越准确。在这个过程中，我们学习了几种评估数字模型性能的方法。

与本章形成鲜明对比的是，本章讲述的机器学习方法可以清楚地理解输入和输出之间的关系，而下一章讲述的方法可以产生几乎无法理解的模型。好处是它们是非常强大的技术——在最强大的股票分类器中——可以应用于分类和数值预测问题。