<html><head/><body>


	
		<title>B16060_06_Final_SMP_ePub</title>
		
	
	
		<div><div/>
		</div>
		<div><h1 id="_idParaDest-169">6.<a id="_idTextAnchor187"/>神经网络和深度学习</h1>
		</div>
		<div><p class="callout-heading">概观</p>
			<p class="callout">在这一章中，你将被介绍到关于神经网络和深度学习的最后一个主题。您将学习张量流、卷积神经网络(CNN)和递归神经网络(RNNs)。您将使用关键的深度学习概念来确定个人的信用度并预测附近的房价。稍后，您还将使用所学的技能实现一个图像分类程序。到本章结束时，你将对神经网络和深度学习的概念有一个牢固的掌握。</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor188"/>简介</h1>
			<p>在前一章中，我们学习了<a id="_idTextAnchor189"/>什么是聚类问题，并且看到了几种算法，比如k-means，它们可以自动地将数据点分组。在本章中，我们将了解神经网络和深度学习网络。</p>
			<p>神经网络和深度学习网络的区别在于网络的复杂性和深度。传统上，神经网络只有一个隐藏层，而深度学习网络的隐藏层不止一个。</p>
			<p>虽然我们将使用神经网络和深度学习进行监督学习，但请注意，神经网络也可以对无监督学习技术进行建模。这种模型实际上在20世纪80年代相当流行，但由于当时所需的计算能力有限，直到最近这种模型才被广泛采用。随着图形处理单元(GPU)和云计算的民主化，我们现在可以获得巨大的计算能力。这是神经网络，尤其是深度学习再次成为热门话题的主要原因。</p>
			<p>深度学习可以模拟比传统神经网络更复杂的模式，因此深度学习现在在计算机视觉(人脸检测和图像识别等应用)和自然语言处理(聊天机器人和文本生成等应用)中得到了更广泛的应用。</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor190"/>人工神经元</h1>
			<p><strong class="bold">人工神经网络</strong> ( <strong class="bold"> ANNs </strong>)，顾名思义，试图复制人类大脑的工作方式，更具体地说是神经元的工作方式。</p>
			<p>神经元是大脑中通过电信号与其他细胞交流的细胞。神经元可以对声音、光和触摸等刺激做出反应。它们也可以触发肌肉收缩等动作。平均而言，人类大脑包含100亿到200亿个神经元。这是一个相当大的网络，对不对？这就是人类能够取得如此多惊人成就的原因。这也是为什么研究人员试图模仿大脑如何运作，并在这样做创造了人工神经网络。</p>
			<p>人工神经网络由多个人工神经元组成，这些神经元相互连接并形成一个网络。人工神经元只是一个处理单元，它对一些输入(<code>x1</code>、<code>x2</code>、…、<code>xn</code>)执行数学运算，并将最终结果(<code>y</code>)返回给下一个单元，如下所示:</p>
			<div><div><img src="img/B16060_06_01.jpg" alt="Figure 6.1: Representation of an artificial neuron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.1:人工神经元的表示</p>
			<p>我们将在接下来的章节中更详细地了解人工神经元是如何工作的。</p>
			<h1 id="_idParaDest-172">张量流中的欧元</h1>
			<p>TensorFlow是目前最流行的神经网络和深度学习框架。它是由谷歌创建和维护的。TensorFlow用于语音识别和语音搜索，也是<a href="http://translate.google.com">translate.google.com</a>背后的大脑。在本章的后面，我们将使用TensorFlow来识别手写字符。</p>
			<p>TensorFlow API有多种语言版本，包括Python、JavaScript、Java和c。tensor flow可与<strong class="bold"> tensors </strong>配合使用。你可以把张量想象成一个容器，由一个矩阵(通常是高维的)和与它将要执行的操作相关的附加信息(比如权重和偏差，你将在本章后面看到)组成。没有维度(没有秩)的张量是标量。秩为1的张量是向量，秩为2的张量是矩阵，秩为3的张量是三维矩阵。秩表示张量的维数。在这一章中，我们将研究秩为2和3的张量。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">数学家使用矩阵和维度这两个术语，而深度学习程序员使用张量和秩来代替。</p>
			<p>TensorFlow还附带数学函数来转换张量，如下所示:</p>
			<ul>
				<li><code>add</code>和<code>multiply</code></li>
				<li><code>exp</code>和<code>log</code></li>
				<li><code>greater</code>、<code>less</code>和<code>equal</code></li>
				<li><code>concat</code>、<code>slice</code>和<code>split</code></li>
				<li><code>matrix_inverse</code>、<code>matrix_determinant</code>和<code>matmul</code></li>
				<li><code>sigmoid</code>、<code>relu</code>和<code>softmax</code></li>
			</ul>
			<p>我们将在本章后面更详细地讨论它们。</p>
			<p>在下一个练习中，我们将使用TensorFlow来计算一个人工神经元。</p>
			<h2 id="_idParaDest-173">练习6.01:使用基本运算和张量流常数</h2>
			<p>在本练习中，我们将使用TensorFlow中的算术运算，通过执行矩阵乘法和加法以及应用非线性函数<code>sigmoid</code>来模拟人工神经元。</p>
			<p>以下步骤将帮助您完成练习:</p>
			<ol>
				<li>打开新的Jupyter笔记本文件。</li>
				<li>将<code>tensorflow</code>包作为<code>tf</code> : <pre>import tensorflow as tf</pre>导入</li>
				<li>Create a tensor called <code>W</code> of shape <code>[1,6]</code> (that is, with 1 row and 6 columns), using <code>tf.constant()</code>, that contains the matrix <code>[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]</code>. Print its value:<pre>W = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[1, 6]) 
W</pre><p>预期输出如下:</p><pre>&lt;tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[1., 2., 3., 4., 5., 6.]], dtype=float32)&gt;</pre></li>
				<li>Create a tensor called <code>X</code> of shape <code>[6,1]</code> (that is, with 6 rows and 1 column), using <code>tf.constant()</code>, that contains <code>[7.0, 8.0, 9.0, 10.0, 11.0, 12.0]</code>. Print its value:<pre>X = tf.constant([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], \
                shape=[6, 1]) 
X</pre><p>预期输出如下:</p><pre>&lt;tf.Tensor: shape=(6, 1), dtype=float32, numpy= 
array([[ 7.],
       [ 8.],
       [ 9.],
       [10.],
       [11.],
       [12.]], dtype=float32)&gt;</pre></li>
				<li>Now, create a tensor called <code>b</code>, using <code>tf.constant()</code>, that contains <code>-88</code>. Print its value:<pre>b = tf.constant(-88.0)
b</pre><p>预期输出如下:</p><pre>&lt;tf.Tensor: shape=(), dtype=float32, numpy=-88.0&gt;</pre></li>
				<li>Perform a matrix multiplication between <code>W</code> and <code>X</code> using <code>tf.matmul</code>, save its results in the <code>mult</code> variable, and print its value:<pre>mult = tf.matmul(W, X)
mult</pre><p>预期输出如下:</p><pre>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[217.]], dtype=float32)&gt;</pre></li>
				<li>Perform a matrix addition between <code>mult</code> and <code>b</code>, save its results in a variable called <code>Z</code>, and print its value:<pre>Z = mult + b
Z</pre><p>预期输出如下:</p><pre>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[129.]], dtype=float32)&gt;</pre></li>
				<li>Apply the <code>sigmoid</code> function to <code>Z</code> using <code>tf.math.sigmoid</code>, save its results in a variable called <code>a</code>, and print its value. The <code>sigmoid </code>function transforms any numerical value within the range <strong class="bold">0</strong> to <strong class="bold">1</strong> (we will learn more about this in the following sections):<pre>a = tf.math.sigmoid(Z)
a</pre><p>预期输出如下:</p><pre>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)&gt;</pre></li>
			</ol>
			<p><code>sigmoid</code>功能将<code>Z</code>的原始值<code>129</code>转换为<code>1</code>。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">要访问该特定部分的源代码，请参考<a href="https://packt.live/31ekGLM">https://packt.live/31ekGLM</a>。</p>
			<p class="callout">你也可以在https://packt.live/3evuKnC在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。</p>
			<p>在本练习中，您使用TensorFlow成功实现了一个人工神经元。这是任何神经网络模型的基础。</p>
			<p>在下一节中，我们将着眼于神经网络的架构。</p>
			<h1 id="_idParaDest-174">欧洲网络体系结构</h1>
			<p>神经网络是<strong class="bold">人工智能</strong> ( <strong class="bold"> AI </strong>)的最新分支。神经网络的灵感来自人脑的工作方式。它们是由沃伦麦卡洛克和沃尔特皮茨在20世纪40年代发明的。神经网络是一种数学模型，用于描述人脑如何解决问题。</p>
			<p>当谈到人脑时，我们将使用ANN来指代数学模型和生物神经网络。</p>
			<p>与其他分类或回归模型相比，神经网络的学习方式更加复杂。神经网络模型具有大量的内部变量，并且输入和输出变量之间的关系可能涉及多个内部层。神经网络比其他监督学习算法具有更高的准确性。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">用TensorFlow掌握神经网络是一个复杂的过程。本节的目的是为您提供入门资源。</p>
			<p>在这一章中，我们将要使用的主要例子是从图像中识别数字。我们正在考虑这种格式，因为每个图像都很小，我们有大约70，000张图像可用。处理这些图像所需的处理能力与普通计算机相似。</p>
			<p>人工神经网络的工作方式类似于人脑的工作方式。人脑中的一个树突连接着一个细胞核，细胞核连接着一个轴突。在人工神经网络中，输入是树状结构，计算发生的地方是细胞核，输出是轴突。</p>
			<p>人造神经元被设计用来复制细胞核的工作方式。它将通过计算矩阵乘法和激活函数来转换输入信号。如果这个函数决定一个神经元必须被激发，一个信号出现在输出中。这个信号可以是网络中其他神经元的输入:</p>
			<div><div><img src="img/B16060_06_02.jpg" alt="Figure 6.2: Figure showing how an ANN works&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.2:显示人工神经网络如何工作的图表</p>
			<p>让我们以<code>n=4</code>为例进一步理解上图。在这种情况下，以下内容适用:</p>
			<ul>
				<li><code>X</code>是输入矩阵，由<code>x1</code>、<code>x2</code>、<code>x3</code>和<code>x4</code>组成。</li>
				<li>权重矩阵<code>W</code>将由<code>w1</code>、<code>w2</code>、<code>w3</code>和<code>w4</code>组成。</li>
				<li><code>b</code>是偏向。</li>
				<li><code>f</code>是激活功能。</li>
			</ul>
			<p>我们将首先用矩阵乘法和偏差计算<code>Z</code>(神经元的左侧):</p>
			<pre>Z = W * X + b = x1*w1 + x2*w2 + x3*w3 + x4*w4 + b</pre>
			<p>然后，将通过应用函数<code>f</code>计算输出<code>y</code>:</p>
			<pre>y = f(Z) = f(x1*w1 + x2*w2 + x3*w3 + x4*w4 + b)</pre>
			<p>太好了——这就是人工神经元在引擎盖下的工作方式。它是两个矩阵运算，一个乘积后跟一个和，以及一个函数变换。</p>
			<p>我们现在进入下一部分——重量。</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor194"/>重量</h2>
			<p><code>y</code>。</p>
			<p>单个神经元是加权和与激活函数的组合，并且可以被称为隐藏层。具有一个隐藏层的神经网络称为<strong class="bold">常规神经网络</strong>:</p>
			<div><div><img src="img/B16060_06_03.jpg" alt="Figure 6.3: Neurons 1, 2, and 3 form the hidden layer of this sample network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.3:神经元1、2和3构成了这个样本网络的隐藏层</p>
			<p>当连接输入和输出时，我们可能有多个隐藏层。具有多个层的神经网络称为<strong class="bold">深度神经网络</strong>。</p>
			<p>深度学习这个术语来源于多层的存在。当创建一个<strong class="bold">人工神经网络</strong> ( <strong class="bold">安</strong>)时，我们可以指定隐含层数。</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor195"/>偏见</h2>
			<p>之前，我们看到神经元的方程式如下:</p>
			<pre>y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4)</pre>
			<p>这个等式的问题是没有依赖于输入<code>x1</code>、<code>x2</code>、<code>x3</code>和<code>x4</code>的常数因子。前面的等式可以模拟将通过点0的任何线性函数:如果所有的<code>w</code>值都等于0，那么<code>y</code>也将等于0。但是其他不经过点0的函数呢？例如，假设我们正在预测一名员工在任职月份的流动概率。即使他们还没有工作满一个月，流失的概率也不是零。</p>
			<p>为了适应这种情况，我们需要引入一个名为<code>b</code>的新参数，它等于0.5，因此新雇主在第一个月的流失率为50%。</p>
			<p>因此，我们在等式中加入偏差:</p>
			<pre>y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4 + b)
y = f(x  w + b)</pre>
			<p>第一个等式是详细的形式，描述了每个坐标、权重系数和偏差的作用。第二个方程是向量形式，其中<code>x = (x1, x2, x3, x4)</code>和<code>w = (w1, w2, w3, w4)</code>。向量之间的点运算符表示两个向量的点或标量积。这两个方程是等价的。我们将在实践中使用第二种形式，因为使用TensorFlow定义变量向量比逐个定义每个变量更容易。</p>
			<p>同样，对于<code>w1</code>、<code>w2</code>、<code>w3</code>和<code>w4</code>，偏差<code>b</code>是一个变量，这意味着它的值可以在学习过程中改变。</p>
			<p>有了每个神经元内置的常数因子，神经网络模型在更好地拟合特定训练数据集方面变得更加灵活。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">由于一些负重量的存在，可能会出现乘积<code>p = x1*w1 + x2*w2 + x3*w3 + x4*w4</code>为负的情况。我们可能仍然希望给模型灵活性来执行(<em class="italic">或触发</em>)一个值大于给定负数的神经元。因此，增加一个恒定的偏置，例如<code>b = 5</code>，可以确保神经元也为<code>-5</code>和<code>0</code>之间的值触发。</p>
			<p>TensorFlow提供了<code>Dense()</code>类来模拟神经网络的隐藏层(<em class="italic">也称为全连接层</em>):</p>
			<pre>from tensorflow.keras import layers
layer1 = layers.Dense(units=128, input_shape=[200])</pre>
			<p>在这个例子中，我们创建了一个完全连接的<code>128</code>神经元层，它将形状张量<code>200</code>作为输入。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">您可以在<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">https://www . TensorFlow . org/API _ docs/python/TF/keras/layers/Dense</a>找到关于这个tensor flow类的更多信息。</p>
			<p><code>Dense()</code>类应该有一个扁平的输入(只有一行)。例如，如果您的输入是由<code>28</code>形成的<code>28</code>形状，那么您必须事先用<code>Flatten()</code>类将其展平，以便得到一个有784个神经元的单行(<code>28 * 28</code>):</p>
			<pre>from tensorflow.keras import layers
input_layer = layers.Flatten(input_shape=(28, 28))
layer1 = layers.Dense(units=128)</pre>
			<p class="callout-heading">注意</p>
			<p class="callout">您可以在<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten">https://www . TensorFlow . org/API _ docs/python/TF/keras/layers/Flatten</a>找到关于这个tensor flow类的更多信息。</p>
			<p>在接下来的章节中，我们将学习如何用额外的参数来扩展这一层神经元。</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor196"/>人工神经网络的用例</h2>
			<p>人工神经网络在监督学习技术中占有一席之地。他们可以模拟分类和回归问题。分类器神经网络寻找特征和标签之间的关系。特征是输入变量，而分类器可以选择作为返回值的每个类是单独的输出。在回归的情况下，输入变量是要素，而只有一个输出:预测值。虽然传统的分类和回归技术在人工智能中有其用例，但人工神经网络通常更善于发现输入和输出之间的复杂关系。</p>
			<p>在下一节中，我们将看看激活函数及其不同的类型。</p>
			<h1 id="_idParaDest-178">动作<a id="_idTextAnchor197"/>激活功能</h1>
			<p>如前所述，单个神经元需要通过应用激活函数来执行转换。在神经网络中可以使用不同的激活函数。如果没有这些功能，神经网络将只是一个线性模型，可以很容易地用矩阵乘法来描述。</p>
			<p>神经网络的激活函数提供非线性，因此可以模拟更复杂的模式。两个非常常见的激活函数是<code>sigmoid</code>和<code>tanh</code>(双曲正切函数)。</p>
			<h2 id="_idParaDest-179">签名<a id="_idTextAnchor198"/> moid</h2>
			<p><code>sigmoid</code>的公式如下:</p>
			<div><div><img src="img/B16060_06_04.jpg" alt="Figure 6.4: The sigmoid formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.4:sigmoid公式</p>
			<p><code>sigmoid</code>功能的输出值范围从<strong class="bold"> 0 </strong>到<strong class="bold"> 1 </strong>。该激活函数通常用于二进制分类问题的神经网络的最后一层。</p>
			<h2 id="_idParaDest-180">Tanh <a id="_idTextAnchor199"/></h2>
			<p>双曲正切的公式如下:</p>
			<div><div><img src="img/B16060_06_05.jpg" alt="Figure 6.5: The tanh formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.5:双曲正切公式</p>
			<p><code>tanh</code>激活功能与<code>sigmoid</code>功能非常相似，直到最近还很流行。它通常用于神经网络的隐藏层。其数值范围在<strong class="bold"> -1 </strong>和<strong class="bold"> 1 </strong>之间。</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor200"/>热卢</h2>
			<p><a id="_idTextAnchor201"/>另一个重要的激活功能是<code>relu</code>。<strong class="bold"> ReLU </strong>代表<strong class="bold">整流线性单元</strong>。这是目前最广泛使用的隐藏层激活功能。其公式如下:</p>
			<div><div><img src="img/B16060_06_06.jpg" alt="Figure 6.6: The ReLU formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.6:ReLU公式</p>
			<p>现在<code>relu</code>功能有不同的变体，比如<code>leaky ReLU</code>和<code>PReLU</code>。</p>
			<h2 id="_idParaDest-182">软马<a id="_idTextAnchor202"/> x</h2>
			<p>该函数将列表中的值收缩到<code>softmax</code>之间，函数如下:</p>
			<div><div><img src="img/B16060_06_07.jpg" alt="Figure 6.7: The softmax formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.7:soft max公式</p>
			<p><code>softmax</code>函数通常用作多类分类问题的神经网络的最后一层，因为它可以为每个不同的输出类生成概率。</p>
			<p>记住，在TensorFlow中，我们可以用激活函数扩展一个<code>Dense()</code>层；我们只需要设置<code>activation</code>参数。在下面的例子中，我们将添加<code>relu</code>激活功能:</p>
			<pre>from tensorflow.keras import layers
layer1 = layers.Dense(units=128, input_shape=[200], \
                      activation='relu')</pre>
			<p>让我们使用这些不同的激活函数，并通过解决下面的练习来观察这些函数如何抑制加权输入。</p>
			<h2 id="_idParaDest-183">练习<a id="_idTextAnchor203"/> e 6.02:激活功能</h2>
			<p>在本练习中，我们将使用<code>numpy</code>包实现以下激活功能:<code>sigmoid</code>、<code>tanh</code>、<code>relu</code>和<code>softmax</code>。</p>
			<p>以下步骤将帮助您完成练习:</p>
			<ol>
				<li value="1">打开新的Jupyter笔记本文件。</li>
				<li>将<code>numpy</code>包导入为<code>np</code> : <pre>import numpy as np</pre></li>
				<li>创建一个<code>sigmoid</code>函数，如下面的代码片段所示，它使用<code>np.exp()</code>方法:<pre>def sigmoid(x):      return 1 / (1 + np.exp(-x))</pre>实现sigmoid公式(如前一节所示)</li>
				<li>Calculate the result of <code>sigmoid</code> function on the value <code>-1</code>:<pre>sigmoid(-1)</pre><p>预期输出如下:</p><pre>0.2689414213699951</pre><p>这是对值<code>-1</code>执行sigmoid变换的结果。</p></li>
				<li>将<code>matplotlib.pyplot</code>包作为<code>plt</code> : <pre>import matplotlib.pyplot as plt</pre>导入</li>
				<li>Create a <code>numpy</code> array called <code>x</code> that contains values from <code>-10</code> to <code>10</code> evenly spaced by an increment of <code>0.1</code>, using the <code>np.arange()</code> method. Print its value:<pre>x = np.arange(-10, 10, 0.1)
x</pre><p>预期输出如下:</p><pre>array([-1.00000000e+01, -9.90000000e+00, -9.80000000e+00,
       -9.70000000e+00, -9.60000000e+00, -9.50000000e+00,
       -9.40000000e+00, -9.30000000e+00, -9.20000000e+00,
       -9.10000000e+00, -9.00000000e+00, -8.90000000e+00,
       -8.80000000e+00, -8.70000000e+00, -8.60000000e+00,
       -8.50000000e+00, -8.40000000e+00, -8.30000000e+00,
       -8.20000000e+00, -8.10000000e+00, -8.00000000e+00,
       -7.90000000e+00, -7.80000000e+00, -7.70000000e+00,
       -7.60000000e+00, -7.50000000e+00, -7.40000000e+00,
       -7.30000000e+00, -7.20000000e+00, -7.10000000e+00,
       -7.00000000e+00, -6.90000000e+00,</pre><p>很好——我们生成了一个包含在<code>-10</code>和<code>10</code>之间的值的<code>numpy</code>数组。</p><p class="callout-heading">注意</p><p class="callout">前面的输出被截断。</p></li>
				<li>Plot a line chart with <code>x</code> and <code>sigmoid(x)</code> using <code>plt.plot()</code> and <code>plt.show()</code>:<pre>plt.plot(x, sigmoid(x))
plt.show()</pre><p>预期输出如下:</p><div><img src="img/B16060_06_08.jpg" alt="Figure 6.8: Line chart using the sigmoid function&#13;&#10;"/></div><p class="figure-caption">图6.8:使用sigmoid函数的折线图</p><p>我们可以看到<code>sigmoid</code>功能的输出范围在<code>0</code>和<code>1</code>之间。对于<code>0</code>附近的值，斜率相当陡。</p></li>
				<li>创建一个<code>tanh()</code>函数，使用<code>np.exp()</code>方法:<pre>def tanh(x):      return 2 / (1 + np.exp(-2*x)) - 1</pre>实现Tanh公式(如前一节所示)</li>
				<li>Plot a line chart with <code>x</code> and <code>tanh(x)</code> using <code>plt.plot()</code> and <code>plt.show()</code>:<pre>plt.plot(x, tanh(x))
plt.show()</pre><p>预期输出如下:</p><div><img src="img/B16060_06_09.jpg" alt="Figure 6.9: Line chart using the tanh function&#13;&#10;"/></div><p class="figure-caption">图6.9:使用双曲正切函数的折线图</p><p><code>tanh</code>函数的形状与<code>sigmoid</code>非常相似，但对于接近<code>0</code>的值，其斜率更陡。记住，它的范围在<strong class="bold"> -1 </strong>和<strong class="bold"> 1 </strong>之间。</p></li>
				<li>创建一个<code>relu</code>函数，使用<code>np.maximum()</code>方法:<pre>def relu(x):     return np.maximum(0, x)</pre>实现ReLU公式(如前一节所示)</li>
				<li>Plot a line chart with <code>x</code> and <code>relu(x)</code> using <code>plt.plot()</code> and <code>plt.show()</code>:<pre>plt.plot(x, relu(x))
plt.show()</pre><p>预期输出如下:</p><div><img src="img/B16060_06_10.jpg" alt="Figure 6.10: Line chart using the relu function&#13;&#10;"/></div><p class="figure-caption">图6.10:使用relu函数的折线图</p><p>当值为负值时，ReLU函数等于<code>0</code>，对于正值，ReLU函数等于恒等函数<code>f(x)=x</code>。</p></li>
				<li>使用<code>np.exp()</code>方法:<pre>def softmax(list):      return np.exp(list) / np.sum(np.exp(list))</pre>创建一个<code>softmax</code>函数，实现softmax公式(如前一节所示)</li>
				<li>Calculate the output of <code>softmax</code> on the list of values, <code>[0, 1, 168, 8, 2]</code>:<pre>result = softmax( [0, 1, 168, 8, 2]) 
result</pre><p>预期输出如下:</p><pre>array([1.09276566e-73, 2.97044505e-73, 1.00000000e+00, 
       3.25748853e-70, 8.07450679e-73])</pre></li>
			</ol>
			<p>正如所料，位于第三个位置的项目具有最高的softmax概率，因为其原始值最高。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">要访问该特定部分的源代码，请参考<a href="https://packt.live/3fJzoOU">https://packt.live/3fJzoOU</a>。</p>
			<p class="callout">你也可以在<a href="https://packt.live/3188pZi">https://packt.live/3188pZi</a>在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。</p>
			<p>通过完成这个练习，我们实现了神经网络的一些最重要的激活功能。</p>
			<h1 id="_idParaDest-184">正向Pr <a id="_idTextAnchor204"/>运算和损失函数</h1>
			<p>到目前为止，我们已经看到了神经元如何接受输入，并对其执行一些数学运算，然后获得输出。我们了解到神经网络是多层神经元的组合。</p>
			<p>将神经网络的输入转换成结果的过程被称为<strong class="bold">前向传播</strong>(或前向传递)。我们要求神经网络做的是通过将多个神经元应用于输入数据来进行预测(神经网络的最终输出):</p>
			<div><div><img src="img/B16060_06_11.jpg" alt="Figure 6.11: Figure showing forward propagation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.11:显示正向传播的图</p>
			<p>神经网络依靠每个神经元的权重矩阵、偏差和激活函数来计算预测输出值<img src="img/B16060_06_11a.png" alt="b"/>。现在，让我们假设权重矩阵和偏差的值是预先设定的。激活函数是在设计神经网络的结构时定义的。</p>
			<p>至于任何有监督的机器学习算法，目标都是做出准确的预测。这意味着我们需要评估预测与真实值相比有多准确。对于传统的机器学习算法，我们使用均方差、准确度或F1分数等评分指标。这也可以应用于神经网络，但唯一的区别是这样的分数以两种不同的方式使用:</p>
			<ul>
				<li>数据科学家使用它们来评估模型在训练集和测试集上的性能，然后根据需要调整超参数。这也适用于神经网络，所以这里没有什么新鲜的。</li>
				<li>它们被神经网络用来从错误中自动学习，并更新权重矩阵和偏差。这将在关于反向传播的下一节中更详细地解释。因此，神经网络将使用一个度量(也称为<strong class="bold">损失函数</strong>)来比较其预测值<img src="img/B16060_06_11b.png" alt="38"/>和真实标签(y)，然后学习如何自动做出更好的预测。</li>
			</ul>
			<p>损失函数对于神经网络学习做出好的预测是至关重要的。这是一个超参数，需要由数据科学家在设计神经网络的架构时定义。选择使用哪个损失函数完全是任意的，取决于数据集或您想要解决的问题，您会选择一个或另一个。幸运的是，我们有一些在大多数情况下都有效的基本经验法则:</p>
			<ul>
				<li>如果您正在处理回归问题，可以使用均方误差。</li>
				<li>如果是二元分类，损失函数应该是二元交叉熵。</li>
				<li>如果是多类分类，那么分类交叉熵应该是您的首选。</li>
			</ul>
			<p>最后一点，损失函数的选择也将决定在神经网络的最后一层使用哪一个激活函数。每个损失函数都需要某种类型的数据，以便正确评估预测性能。</p>
			<p>以下是根据损失函数和项目/问题类型的激活函数列表:</p>
			<div><div><img src="img/B16060_06_12.jpg" alt="Figure 6.12: Overview of the different activation functions and their applications&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.12:不同激活功能及其应用概述</p>
			<p>使用TensorFlow，为了构建您的自定义架构，您可以实例化<code>Sequential()</code>类并添加您的完全连接的神经元层，如以下代码片段所示:</p>
			<pre>import tensorflow as tf
from tensorflow.keras import layers
model = tf.keras.Sequential()
input_layer = layers.Flatten(input_shape=(28,28))
layer1 = layers.Dense(128, activation='relu')
model.add(input_layer)
model.add(layer1)</pre>
			<p>现在是时候看看神经网络如何通过反向传播来改善其预测了。</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor205"/>反向传播</h1>
			<p>之前，我们学习了神经网络如何通过使用权重矩阵和来自其神经元的偏差(我们可以将它们组合成单个矩阵)来进行预测。使用损失函数，网络确定预测的好坏。如果它可以使用这些信息并相应地更新参数，那就太好了。这正是反向传播的目的:优化神经网络的参数。</p>
			<p>训练神经网络包括多次执行前向传播和后向传播，以便根据误差进行预测和更新参数。在第一遍(或传播)中，我们首先初始化神经网络的所有权重。然后，我们应用前向传播，接着是反向传播，后者更新权重。</p>
			<p>我们多次应用该过程，神经网络将迭代地优化其参数。您可以通过设置神经网络遍历整个数据集的最大次数(也称为时期)来决定停止此学习过程，或者如果神经网络的分数在几个时期后不再提高，则定义一个提前停止阈值。</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor206"/>优化者和学习率</h1>
			<p>在上一节中，我们看到神经网络遵循迭代过程来为任何输入数据集找到最佳解决方案。它的学习过程是一个优化过程。您可以使用不同的优化算法(也称为<code>Adam</code>、<code>SGD</code>和<code>RMSprop</code>)。</p>
			<p>神经网络优化器的一个重要参数是学习速率。该值定义了神经网络更新其权重的速度。定义太低的学习速率会减慢学习过程，并且神经网络在找到正确的参数之前会花费很长时间。另一方面，学习率太高会使神经网络无法学习到解决方案，因为它的权重变化比要求的要大。一个好的做法是以不太小的学习速率开始(例如<strong class="bold"> 0.01 </strong>或<strong class="bold"> 0.001 </strong>)，然后一旦其分数开始稳定或变得更差，就停止神经网络训练，并降低学习速率(例如，降低一个数量级)并继续训练网络。</p>
			<p>使用TensorFlow，您可以从<code>tf.keras.optimizers</code>实例化一个优化器。例如，下面的代码片段向我们展示了如何使用<code>0.001</code>作为学习率来创建一个<code>Adam</code>优化器，然后通过指定损失函数(<code>'sparse_categorical_crossentropy'</code>)和要显示的度量(<code>'accuracy'</code>)来编译我们的神经网络:</p>
			<pre>import tensorflow as tf
optimizer = tf.keras.optimizers.Adam(0.001)
model.compile(loss='sparse_categorical_crossentropy', \
              optimizer=optimizer, metrics=['accuracy'])</pre>
			<p>一旦模型编译完成，我们就可以用<code>.fit()</code>方法训练神经网络，如下所示:</p>
			<pre>model.fit(features_train, label_train, epochs=5)</pre>
			<p>这里，我们在<code>5</code>时期的训练集上训练神经网络。一旦经过训练，我们就可以在测试集上使用该模型，并使用<code>.evaluate()</code>方法评估其性能:</p>
			<pre>model.evaluate(features_test, label_test)</pre>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在<a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">https://www . TensorFlow . org/API _ docs/python/TF/keras/optimizer</a>找到更多关于这个tensor flow优化器的信息。</p>
			<p>在下一个练习中，我们将在数据集上训练神经网络。</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor208"/>练习6.03:信贷审批分类</h2>
			<p>在本练习中，我们将使用德国信用审批数据集，并训练一个神经网络来对个人是否有信用进行分类。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">数据集文件也可以在我们的GitHub资源库中找到:</p>
			<p class="callout"><a href="https://packt.live/2V7uiV5">https://packt.live/2V7uiV5</a>。</p>
			<p>以下步骤将帮助您完成练习:</p>
			<ol>
				<li value="1">打开新的Jupyter笔记本文件。</li>
				<li>从<code>numpy</code> : <pre>from numpy import loadtxt</pre>导入<code>loadtxt</code>方法</li>
				<li>创建一个名为<code>file_url</code>的变量，包含到原始数据集的链接:<pre>file_url = 'https://raw.githubusercontent.com/'\            'PacktWorkshops/'\            'The-Applied-Artificial-Intelligence-Workshop'\            '/master/Datasets/german_scaled.csv'</pre></li>
				<li>Load the data into a variable called <code>data</code> using <code>loadtxt()</code> and specify the <code>delimiter=','</code> parameter. Print its content:<pre>data = loadtxt(file_url, delimiter=',')
data</pre><p>预期输出如下:</p><pre>array([[0.        , 0.33333333, 0.02941176, ..., 0.      , 1.      ,
        1.        ],
       [1.        , 0.        , 0.64705882, ..., 0.      , 0.      ,
        1.        ],
       [0.        , 1.        , 0.11764706, ..., 1.      , 0.      ,
        1.        ],
       ...,
       [0.        , 1.        , 0.11764706, ..., 0.      , 0.      ,
        1.        ],
       [1.        , 0.33333333, 0.60294118, ..., 0.      , 1.      ,
        1.        ],
       [0.        , 0.        , 0.60294118, ..., 0.      , 0.      ,
        1.        ]])</pre></li>
				<li>创建一个名为<code>label</code>的变量，只包含第一列的数据(这将是我们的响应变量):<pre>label = data[:, 0]</pre></li>
				<li>创建一个名为<code>features</code>的变量，它包含除第一列(对应于响应变量)之外的所有数据:<pre>features = data[:, 1:]</pre></li>
				<li>从<code>sklearn.model_selection</code> : <pre>from sklearn.model_selection import train_test_split</pre>导入<code>train_test_split</code>方法</li>
				<li>将数据分成训练集和测试集，并将结果保存到四个变量中，分别称为<code>features_train</code>、<code>features_test</code>、<code>label_train</code>和<code>label_test</code>。使用20%的数据进行测试，并指定<code>random_state=7</code> : <pre>features_train, features_test, \ label_train, label_test = train_test_split(features, \                                            label, \                                            test_size=0.2, \                                            random_state=7)</pre></li>
				<li>将<code>numpy</code>导入为<code>np</code>，将<code>tensorflow</code>导入为<code>tf</code>，将<code>layers</code>从<code>tensorflow.keras</code> : <pre>import numpy as np import tensorflow as tf from tensorflow.keras import layers</pre>导入</li>
				<li>使用<code>np.random_seed()</code>和<code>tf.random.set_seed()</code> : <pre>np.random.seed(1) tf.random.set_seed(1)</pre>将<code>1</code>设置为<code>numpy</code>和<code>tensorflow</code>的种子</li>
				<li>实例化一个<code>tf.keras.Sequential()</code>类并保存到一个名为<code>model</code> : <pre>model = tf.keras.Sequential()</pre>的变量中</li>
				<li>用<code>16</code>神经元、<code>activation='relu'</code>和<code>input_shape=[19]</code>实例化一个<code>layers.Dense()</code>类，然后保存到一个名为<code>layer1</code> : <pre>layer1 = layers.Dense(16, activation='relu', \                       input_shape=[19])</pre>的变量中</li>
				<li>用<code>1</code>神经元和<code>activation='sigmoid'</code>实例化第二个<code>layers.Dense()</code>类，然后保存到一个名为<code>final_layer</code> : <pre>final_layer = layers.Dense(1, activation='sigmoid')</pre>的变量中</li>
				<li>使用<code>.add()</code> : <pre>model.add(layer1) model.add(final_layer)</pre>将刚刚定义的两个图层添加到模型中</li>
				<li>用<code>0.001</code>作为学习率来实例化一个<code>tf.keras.optimizers.Adam()</code>类，并保存到一个名为<code>optimizer</code> : <pre>optimizer = tf.keras.optimizers.Adam(0.001)</pre>的变量中</li>
				<li>使用<code>.compile()</code>和<code>loss='binary_crossentropy'</code>、<code>optimizer=optimizer, metrics=['accuracy']</code>编译神经网络，如以下代码片段所示:<pre>model.compile(loss='binary_crossentropy', \               optimizer=optimizer, metrics=['accuracy'])</pre></li>
				<li>Print a summary of the model using <code>.summary()</code>:<pre>model.summary()</pre><p>预期输出如下:</p><div><img src="img/B16060_06_13.jpg" alt="Figure 6.13: Summary of the sequential model&#13;&#10;"/></div><p class="figure-caption">图6.13:顺序模型总结</p><p>这个输出总结了我们的神经网络的架构。正如所料，我们可以看到它由三层组成，我们知道每层的输出大小和参数数量，它们对应于权重和偏差。例如，第一层具有要学习的<code>16</code>神经元和<code>320</code>参数(权重和偏差)。</p></li>
				<li>Next, fit the neural networks with the training set and specify <code>epochs=10</code>:<pre>model.fit(features_train, label_train, epochs=10)</pre><p>预期输出如下:</p><div><img src="img/B16060_06_14.jpg" alt="Figure 6.14: Fitting the neural network with the training set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图6.14:用训练集拟合神经网络</p>
			<p>输出提供了有关神经网络训练的大量信息。第一行告诉我们训练集是由<code>800</code>观察值组成的。然后我们可以看到每个时代的结果:</p>
			<p>总处理时间(秒)</p>
			<p>按美国/样本中的数据样本列出的处理时间</p>
			<p>损失值和准确度得分</p>
			<p>这个神经网络的最终结果是最后一个时期(<code>epoch=10</code>)，其中我们获得了<code>0.6888</code>的准确度分数。但是我们可以看到趋势在改善:在每个时期之后，准确度分数仍然在增加。因此，如果我们通过增加历元数或降低学习速率来训练神经网络更长时间，我们可能会得到更好的结果。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">要访问该特定部分的源代码，请参考<a href="https://packt.live/3fMhyLk">https://packt.live/3fMhyLk</a>。</p>
			<p class="callout">你也可以在<a href="https://packt.live/2Njghza">https://packt.live/2Njghza</a>在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。</p>
			<p>通过完成本练习，您已经训练了第一个分类器。在传统的机器学习算法中，你需要使用更多的代码来实现这一点，因为你必须定义神经网络的整个架构。在这里，神经网络在<code>10</code>时代之后得到了<code>0.6888</code>，但是如果我们让它训练更长时间，它仍然可以改进。你可以自己试试这个。</p>
			<p>接下来，我们将研究正规化。</p>
			<h1 id="_idParaDest-188">调整</h1>
			<p>与任何机器学习算法一样，当神经网络学习仅与训练集相关的模式时，它们可能会面临过度拟合的问题。在这种情况下，模型将无法概括看不见的数据。</p>
			<p>幸运的是，有多种技术可以帮助降低过度拟合的风险:</p>
			<ul>
				<li>L1正则化，其向损失函数添加惩罚参数(权重的绝对值)</li>
				<li>L2正则化，将惩罚参数(权重的平方值)添加到损失函数中</li>
				<li>早期停止，如果验证集的误差增加，而训练集的误差减少，则停止训练</li>
				<li>Dropout，这将在训练过程中随机删除一些神经元</li>
			</ul>
			<p>所有这些技术都可以添加到我们创建的神经网络的每一层。我们将在下一个练习中讨论这个问题。</p>
			<h2 id="_idParaDest-189">练习6.04:用正则化方法预测波士顿房价</h2>
			<p>在本练习中，您将构建一个神经网络来预测波士顿郊区的中值房价，并了解如何向网络中添加正则项。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">数据集文件也可以在我们的GitHub资源库中找到:<a href="https://packt.live/2V9kRUU">https://packt.live/2V9kRUU</a>。</p>
			<p class="callout">引用:该数据最初由J. Environ的Harrison d .和Rubin feld d . l .发表。经济学&amp;管理学，第5卷，81-102，1978 。</p>
			<p>数据集由<code>12</code>不同的要素组成，这些要素提供了关于郊区和目标变量的信息(<code>MEDV</code>)。目标变量是数值型的，代表业主自住房屋的中值，单位为1000美元。</p>
			<p>以下步骤将帮助您完成练习:</p>
			<ol>
				<li value="1">打开新的Jupyter笔记本文件。</li>
				<li>将<code>pandas</code>包作为<code>pd</code> : <pre>import pandas as pd</pre>导入</li>
				<li>创建一个包含原始数据集链接的<code>file_url</code>变量:<pre>file_url = 'https://raw.githubusercontent.com/'\            'PacktWorkshops/'\            'The-Applied-Artificial-Intelligence-Workshop'\            '/master/Datasets/boston_house_price.csv'</pre></li>
				<li>使用<code>pd.read_csv()</code> : <pre>df = pd.read_csv(file_url)</pre>将数据集加载到名为<code>df</code>的变量中</li>
				<li>Display the first five rows using <code>.head()</code>:<pre>df.head()</pre><p>预期输出如下:</p><div><img src="img/B16060_06_15.jpg" alt="Figure 6.15: Output showing the first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">图6.15:显示数据集前五行的输出</p></li>
				<li>使用<code>.pop()</code>提取目标变量，并保存到名为<code>label</code> : <pre>label = df.pop('MEDV')</pre>的变量中</li>
				<li>从<code>sklearn.preprocessing</code> : <pre>from sklearn.preprocessing import scale</pre>导入<code>scale</code>功能</li>
				<li>Scale the DataFrame, <code>df</code>, and save the results into a variable called <code>scaled_features</code>. Print its content:<pre>scaled_features = scale(df)
scaled_features</pre><p>预期输出如下:</p><pre>array([[-0.41978194,  0.28482986, -1.2879095 , ..., -0.66660821,
        -1.45900038, -1.0755623 ],
       [-0.41733926, -0.48772236, -0.59338101, ..., -0.98732948,
        -0.30309415, -0.49243937],
       [-0.41734159, -0.48772236, -0.59338101, ..., -0.98732948,
        -0.30309415, -1.2087274 ],
       ...,
       [-0.41344658, -0.48772236,  0.11573841, ..., -0.80321172,
         1.17646583, -0.98304761],
       [-0.40776407, -0.48772236,  0.11573841, ..., -0.80321172,
         1.17646583, -0.86530163],
       [-0.41500016, -0.48772236,  0.11573841, ..., -0.80321172,</pre><p>在输出中，您可以看到我们所有的功能现在都是标准化的。</p></li>
				<li>从<code>sklearn.model_selection</code> : <pre>from sklearn.model_selection import train_test_split</pre>导入<code>train_test_split</code></li>
				<li>将数据分成训练集和测试集，并将结果保存到四个变量中，分别称为<code>features_train</code>、<code>features_test</code>、<code>label_train</code>和<code>label_test</code>。使用10%的数据进行测试，并指定<code>random_state=8</code> : <pre>features_train, features_test, \ label_train, label_test = train_test_split(scaled_features, \                                            label, \                                            test_size=0.1, \                                            random_state=8)</pre></li>
				<li>导入<code>numpy</code>为<code>np</code>，<code>tensorflow</code>为<code>tf</code>，从<code>tensorflow.keras</code> : <pre>import numpy as np import tensorflow as tf from tensorflow.keras import layers</pre>导入<code>layers</code></li>
				<li>使用<code>np.random_seed()</code>和<code>tf.random.set_seed()</code> : <pre>np.random.seed(8) tf.random.set_seed(8)</pre>将<code>8</code>设置为NumPy和TensorFlow的种子</li>
				<li>实例化一个<code>tf.keras.Sequential()</code>类并保存到一个名为<code>model</code> : <pre>model = tf.keras.Sequential()</pre>的变量中</li>
				<li>接下来，使用带有<code>l1=0.01 </code>和<code>l2=0.01</code>的<code>tf.keras.regularizers.l1_l2</code>创建一个组合的<code>l1</code>和<code>l2</code>正则化器。保存到一个名为<code>regularizer</code> : <pre>regularizer = tf.keras.regularizers.l1_l2(l1=0.1, l2=0.01)</pre>的变量中</li>
				<li>用<code>10</code>神经元、<code>activation='relu'</code>、<code> input_shape=[12]</code>和<code>kernel_regularizer=regularizer</code>实例化一个<code>layers.Dense()</code>类，并保存到一个名为<code>layer1</code> : <pre>layer1 = layers.Dense(10, activation='relu', \          input_shape=[12], kernel_regularizer=regularizer)</pre>的变量中</li>
				<li>用<code>1</code>神经元实例化第二个<code>layers.Dense()</code>类，并保存到一个名为<code>final_layer</code> : <pre>final_layer = layers.Dense(1)</pre>的变量中</li>
				<li>Add the two layers you just defined to the model using <code>.add()</code> and add a layer in between each of them with <code>layers.Dropout(0.25)</code>:<pre>model.add(layer1)
model.add(layers.Dropout(0.25))
model.add(final_layer)</pre><p>我们在每个致密层之间添加了一个脱落层，随机移除25%的神经元。</p></li>
				<li>用<code>0.001</code>作为学习率来实例化一个<code>tf.keras.optimizers.SGD()</code>类，并保存到一个名为<code>optimizer</code> : <pre>optimizer = tf.keras.optimizers.SGD(0.001)</pre>的变量中</li>
				<li>使用<code>.compile()</code>和<code>loss='mse', optimizer=optimizer, metrics=['mse']</code> : <pre>model.compile(loss='mse', optimizer=optimizer, \               metrics=['mse'])</pre>编译神经网络</li>
				<li>Print a summary of the model using <code>.summary()</code>:<pre>model.summary()</pre><p>预期输出如下:</p><div><img src="img/B16060_06_16.jpg" alt="Figure 6.16: Summary of the model&#13;&#10;"/></div><p class="figure-caption">图6.16:模型总结</p><p>这个输出总结了我们的神经网络的架构。我们可以看到它由三层组成，其中两层是致密层，一层是脱落层。</p></li>
				<li>Instantiate a <code>tf.keras.callbacks.EarlyStopping()</code> class with <code>monitor='val_loss'</code> and <code>patience=2</code> as the learning rate and save it into a variable called <code>callback</code>:<pre>callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \
                                            patience=2)</pre><p>我们刚刚定义了一个回调，声明如果验证损失(<code>monitor='val_loss'</code>)在<code>2</code>时期(<code>patience=2</code>)后没有改善，神经网络将停止其训练。</p></li>
				<li>Fit the neural networks with the training set and specify <code>epochs=50</code>, <code>validation_split=0.2</code>, <code>callbacks=[callback]</code>, and <code>verbose=2</code>:<pre>model.fit(features_train, label_train, \
          epochs=50, validation_split = 0.2, \
          callbacks=[callback], verbose=2)</pre><p>预期输出如下:</p><div><img src="img/B16060_06_17.jpg" alt="Figure 6.17: Fitting the neural network with the training set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">图6.17:用训练集拟合神经网络</p>
			<p>在输出中，我们看到神经网络在第22个时期后停止了训练。它在最大历元数<code>50</code>之前停止。这是由于我们之前设置的回调:如果验证损失在两个时期后没有改善，训练应该停止。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">要访问该特定部分的源代码，请参考<a href="https://packt.live/2Yobbba">https://packt.live/2Yobbba</a>。</p>
			<p class="callout">你也可以在<a href="https://packt.live/37SVSu6">https://packt.live/37SVSu6</a>在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。</p>
			<p>您刚刚应用了多种正则化技术，并训练了一个神经网络来预测波士顿郊区住房的中值。</p>
			<h2 id="_idParaDest-190">activity<a id="_idTextAnchor211"/>ity 6.01:寻找数字数据集的最佳准确度分数</h2>
			<p>在本活动中，您将训练和评估一个神经网络，该网络将从MNIST数据集提供的图像中识别手写数字。你将专注于获得最佳准确度分数。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在TensorFlow的网站<a href="https://www.tensorflow.org/datasets/catalog/mnist">https://www.tensorflow.org/datasets/catalog/mnist</a>了解更多关于这个数据集的信息。</p>
			<p class="callout">引用:该数据集最初由<em class="italic"> Yann Lecun </em>分享。</p>
			<p>以下步骤将帮助您完成活动:</p>
			<ol>
				<li value="1">导入MNIST数据集。</li>
				<li>通过应用除以<code>255</code>来标准化数据。</li>
				<li>Create a neural network architecture with the following layers:<p>使用<code>layers.Flatten(input_shape=(28,28))</code>展平输入层</p><p>与<code>layers.Dense(128, activation='relu')</code>完全连接的层</p><p>带有<code>layers.Dropout(0.25)</code>的脱落层</p><p>与<code>layers.Dense(10, activation='softmax')</code>完全连接的层</p></li>
				<li>指定一个学习率为<code>0.001</code>的<code>Adam</code>优化器。</li>
				<li>定义对<code>5</code>的验证损失和耐心的提前停止。</li>
				<li>训练模型。</li>
				<li>评估模型并找到准确度分数。</li>
			</ol>
			<p>预期输出如下:</p>
			<div><div><img src="img/B16060_06_18.jpg" alt="Figure 6.18: Expected accuracy score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.18:预期的准确度分数</p>
			<p class="callout-heading">注意</p>
			<p class="callout">这项活动的解决方案可以在第378页找到</p>
			<p>在下一部分，我们将深入研究深度学习主题。</p>
			<h1 id="_idParaDest-191">深L <a id="_idTextAnchor212"/>收益</h1>
			<p>现在，我们可以轻松地构建和训练具有一个隐藏层的神经网络，我们可以通过深度学习来研究更复杂的架构。</p>
			<p>深度学习只是传统神经网络的扩展，但具有更深更复杂的架构。深度学习可以模拟非常复杂的模式，应用于检测图像中的对象和将文本翻译成不同语言等任务。</p>
			<h2 id="_idParaDest-192">Shallo <a id="_idTextAnchor213"/> w与深度网络</h2>
			<p>现在，我们可以轻松地构建和训练具有一个隐藏层的神经网络，我们可以通过深度学习来研究更复杂的架构。</p>
			<p>如前所述，我们可以向神经网络添加更多的隐藏层。这将增加要学习的参数的数量，但可能有助于模拟更复杂的模式。这就是深度学习的意义所在:增加神经网络的深度，以解决更复杂的问题。</p>
			<p>例如，我们可以在前面关于正向传播和损失函数的章节中介绍的神经网络中添加第二层:</p>
			<div><div><img src="img/B16060_06_19.jpg" alt="Figure 6.19: Figure showing two hidden layers in a neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.19:显示神经网络中两个隐藏层的图形</p>
			<p>理论上，我们可以添加无限数量的隐藏层。但是深层网络有一个缺点。增加深度也将增加要优化的参数的数量。因此，神经网络将不得不训练更长时间。因此，作为良好的实践，最好从一个简单的架构开始，然后稳步增加其深度。</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor214"/>计算机视觉和图像分类</h1>
			<p>深度学习在计算机视觉和自然语言处理方面取得了惊人的成果。计算机视觉是一个涉及分析数字图像的领域。数字图像是由<strong class="bold">个像素</strong>组成的矩阵。每个像素都有一个在<strong class="bold"> 0 </strong>和<strong class="bold"> 255 </strong>之间的值，这个值代表像素的亮度。图像可以是黑白的，并且只有一个通道。但它也可以有颜色，在这种情况下，它将有三个通道用于红色、绿色和蓝色。这种数字版本的图像可以输入到深度学习模型中。</p>
			<p>计算机视觉有多种应用，例如图像分类(识别图像中的主要对象)、对象检测(定位图像中的不同对象)和图像分割(找到图像中对象的边缘)。在本书中，我们将只看图像分类。</p>
			<p>在下一节中，我们将研究一种特定类型的架构:CNN。</p>
			<h2 id="_idParaDest-194">卷积<a id="_idTextAnchor215"/>神经网络</h2>
			<p><strong class="bold">CNN</strong>是为图像相关模式识别而优化的人工神经网络。CNN基于卷积层，而不是全连接层。</p>
			<p>卷积层用于通过过滤器检测图像中的图案。过滤器只是一个矩阵，通过卷积运算应用于输入图像的一部分，输出将是另一个图像(也称为特征图)，其中突出显示了过滤器找到的模式。例如，一个简单的过滤器可以识别花朵上的垂直线，如下图所示:</p>
			<div><div><img src="img/B16060_06_20.jpg" alt="Figure 6.20: Convolution detecting patterns in an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.20:卷积检测图像中的图案</p>
			<p>这些过滤器不是预先设置的，而是由CNN自动学习的。训练结束后，CNN可以识别图像中的不同形状。这些形状可以在图像上的任何位置，卷积运算符可以识别类似的图像信息，而不管其确切位置和方向。</p>
			<h2 id="_idParaDest-195">召集作战</h2>
			<p>卷积是一种特殊类型的矩阵运算。对于输入图像，大小为<code>n*n</code>的过滤器将通过图像的特定区域，应用元素乘积和求和，并返回计算值:</p>
			<div><div><img src="img/B16060_06_21.jpg" alt="Figure 6.21: Convolutional operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.21:卷积运算</p>
			<p>在前面的示例中，我们对图像的左上角部分应用了滤镜。然后，我们应用了一个元素乘积，它只是将输入图像中的一个元素乘以滤波器上的相应值。在示例中，我们计算了以下内容:</p>
			<ul>
				<li>第一行，第一列:<code>5</code> * <code>2</code> = <code>10</code></li>
				<li>第一行，第二列:<code>10</code> * <code>0</code> = <code>0</code></li>
				<li>第一行，第三列:<code>15</code> * <code>(-1)</code> = <code>-15</code></li>
				<li>第二行，第一列:<code>10</code> * <code>2</code> = <code>20</code></li>
				<li>第二行，第二列:<code>20</code> * <code>0</code> = <code>0</code></li>
				<li>第2行，第3列:<code>30</code> * <code>(-1)</code> = <code>-30</code></li>
				<li>第3行，第1列:<code>100</code> * <code>2</code> = <code>200</code></li>
				<li>第3行，第2列:<code>150</code> * <code>0</code> = <code>0</code></li>
				<li>第3行，第3列:<code>200</code> * <code>(-1)</code> = <code>-200</code></li>
			</ul>
			<p>最后我们进行这些值的求和:<code>10</code>+<code>0</code>-<code>15</code>+<code>20</code>+<code>0</code>-<code>30</code>+<code>200</code>+<code>0</code>-<code>200</code>=<code>-15</code>。</p>
			<p>然后，我们将通过将过滤器从输入图像向右滑动一列来执行相同的操作。我们不断滑动滤镜，直到覆盖整个图像:</p>
			<div><div><img src="img/B16060_06_22.jpg" alt="Figure 6.22: Convolutional operations on different rows and columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.22:不同行和列上的卷积运算</p>
			<p>除了逐列滑动，我们还可以滑动两列、三列或更多列。定义该滑动操作长度的参数称为<strong class="bold">步幅</strong>。</p>
			<p>您可能已经注意到卷积运算的结果是一个比输入图像尺寸更小的图像(或特征图)。如果想要保持完全相同的尺寸，可以在输入图像的边框周围添加值为0的附加行和列。这个操作叫做<strong class="bold">填充</strong>。</p>
			<p>这就是卷积运算背后的内容。卷积层只是这种操作与多个滤波器的应用。</p>
			<p>我们可以用下面的代码片段在TensorFlow中声明一个卷积层:</p>
			<pre>from tensorflow.keras import layers
layers.Conv2D(32, kernel_size=(3, 3), strides=(1,1), \
              padding="valid", activation="relu")</pre>
			<p>在前面的例子中，我们已经用<code>32</code>过滤器(也称为<code>(3, 3)</code>，步幅为<code>1</code>(一次滑动一列或一行窗口)实例化了一个卷积层，并且没有填充(<code>padding="valid"</code>)。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在TensorFlow的网站上阅读关于这个Conv2D类的更多信息，网址是<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">https://www . tensor flow . org/API _ docs/python/TF/keras/layers/conv 2d</a>。</p>
			<p>在TensorFlow中，卷积层期望输入是以下格式的张量:(<strong class="bold">行</strong>、<strong class="bold">高</strong>、<strong class="bold">宽</strong>、<strong class="bold">通道</strong>)。根据数据集的不同，您可能需要调整图像的形状以符合这一要求。TensorFlow为此提供了一个函数，如以下代码片段所示:</p>
			<pre>features_train.reshape(60000, 28, 28, 1)</pre>
			<h2 id="_idParaDest-196">联营公司</h2>
			<p>CNN架构中另一个常见的层是池层。我们之前已经看到，如果不添加填充，卷积层会减小图像的大小。这种行为是意料之中的吗？为什么我们不保持与输入图像完全相同的大小呢？一般来说，使用CNN，我们倾向于随着我们在不同层的进展而减小特征地图的大小。这样做的主要原因是，我们希望越来越多的特定模式检测器靠近网络的末端。</p>
			<p>更接近网络的开始，CNN将倾向于使用更通用的过滤器，如垂直或水平线检测器，但随着深入，例如，如果我们训练CNN识别猫和狗，我们将使用能够检测狗的尾巴或猫的胡须的过滤器，或者如果我们对水果图像进行分类，则使用能够检测物体纹理的过滤器。此外，具有较小的特征图降低了检测到错误模式的风险。</p>
			<p>通过增加步幅，我们可以进一步减小输出特征图的大小。但还有另一种方法:在卷积层之后添加一个池层。池化图层是给定大小的矩阵，并将聚合函数应用于要素地图的每个区域。最常用的聚合方法是找到一组像素的最大值:</p>
			<div><div><img src="img/B16060_06_23.jpg" alt="Figure 6.23: Workings of the pooling layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.23:池层的工作方式</p>
			<p>在前面的例子中，我们使用了大小为(<code>2, 2</code>)和<code>stride=2</code>的最大池。我们查看特征图的左上角，找到像素<code>6</code>、<code>8</code>、<code>1</code>和<code>2</code>中的最大值，得到结果<code>8</code>。然后，我们将最大池滑动一个步长<code>2</code>，并对像素<code>6</code>、<code>1</code>、<code>7</code>和<code>4</code>执行相同的操作。我们对底部组重复相同的操作，并获得新的尺寸特征图(<code>2,2</code>)。</p>
			<p>在TensorFlow中，我们可以使用<code>MaxPool2D()</code>类来声明一个max-pooling层:</p>
			<pre>from tensorflow.keras import layers
layers.MaxPool2D(pool_size=(2, 2), strides=2)</pre>
			<p class="callout-heading">注意</p>
			<p class="callout">您可以在TensorFlow的网站上阅读有关Conv2D类的更多信息，网址为<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D">https://www . tensor flow . org/API _ docs/python/TF/keras/layers/maxpool 2d</a>。</p>
			<h2 id="_idParaDest-197">CNN架构<a id="_idTextAnchor218"/>图</h2>
			<p>正如您前面看到的，您可以通过指定隐藏层的类型和数量、要使用的激活函数等来定义自己的定制CNN架构。但这对于新手来说可能有点望而生畏。我们如何知道每层需要添加多少个滤波器，或者正确的步距是多少？我们将不得不尝试多种组合，看看哪个有效。</p>
			<p>幸运的是，许多深度学习的研究人员已经做了这样的探索性工作，并公布了他们设计的架构。目前，最著名的是这些:</p>
			<ul>
				<li>AlexNet</li>
				<li>VGG</li>
				<li>雷斯内特</li>
				<li>Inception<p class="callout-heading">注意</p><p class="callout">我们不会详细介绍每种架构，因为这不在本书的范围内，但您可以在https://www . TensorFlow . org/API _ docs/python/TF/keras/applications上阅读更多关于tensor flow上实现的不同CNN架构的信息。</p></li>
			</ul>
			<h2 id="_idParaDest-198">活动6。使用CNN评估时尚图像识别模型</h2>
			<p>在本次活动中，我们将训练CNN识别来自时尚MNIST数据集的10个不同类别的服装图像。我们将会发现CNN模型的准确性。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在TensorFlow的网站https://www.tensorflow.org/datasets/catalog/fashion_mnist<a href="https://www.tensorflow.org/datasets/catalog/fashion_mnist">上了解更多关于这个数据集的信息。</a></p>
			<p class="callout">原始数据集由<em class="italic">晓寒</em>共享。</p>
			<p>以下步骤将帮助您完成活动:</p>
			<ol>
				<li value="1">导入时尚MNIST数据集。</li>
				<li>重塑培训和测试集。</li>
				<li>通过应用除以<code>255</code>来标准化数据。</li>
				<li>Create a neural network architecture with the following layers:<p>三个卷积层，<code>Conv2D(64, (3,3), activation='relu')</code>后跟<code>MaxPooling2D(2,2)</code></p><p>变平层</p><p>与<code>Dense(128, activation=relu)</code>完全连接的层</p><p>与<code>Dense(10, activation='softmax')</code>完全连接的层</p></li>
				<li>指定一个学习率为<code>0.001</code>的<code>Adam</code>优化器。</li>
				<li>训练模型。</li>
				<li>在测试集上评估模型。</li>
			</ol>
			<p>预期输出如下:</p>
			<pre>10000/10000 [==============================] - 1s 108us/sample - loss: 0.2746 - accuracy: 0.8976
[0.27461639745235444, 0.8976]</pre>
			<p class="callout-heading">注意</p>
			<p class="callout">这项活动的解决方案可在第382页找到。</p>
			<p>在接下来的部分，我们将了解一种不同类型的深度学习架构:RNN。</p>
			<h1 id="_idParaDest-199">递归神经网络</h1>
			<p>在上一节中，我们学习了如何使用CNN来完成计算机视觉任务，例如对图像进行分类。通过深度学习，计算机现在能够实现有时甚至超过人类的表现。另一个吸引研究者兴趣的领域是自然语言处理。这是RNNs擅长的领域。</p>
			<p>在过去几年中，我们已经看到了RNN技术的许多不同应用，如语音识别、聊天机器人和文本翻译应用。但是rnn在预测时间序列模式方面也很有表现，这是用来预测股票市场的。</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor221"/> RNN层</h2>
			<p><a id="_idTextAnchor222"/>前面提到的所有应用的共同点是输入是顺序的。输入中有一个时间成分。例如，一个句子是一系列单词，单词的顺序很重要；股票市场数据由一系列日期和相应的股票价格组成。</p>
			<p>为了容纳这样的输入，我们需要神经网络能够处理输入序列，并能够保持对它们之间关系的理解。做到这一点的一个方法是创建网络可以考虑先前输入的存储器。这正是基本RNN的工作原理:</p>
			<div><div><img src="img/B16060_06_24.jpg" alt="Figure 6.24: Overview of a single RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.24:单个RNN的概述</p>
			<p>在上图中，我们可以看到一个神经网络，它接受一个名为<code>X</code> t的输入，执行一些转换，并给出输出结果<img src="img/B16060_06_24a.png" alt="a"/>。目前没有新消息。</p>
			<p>但是你可能已经注意到有一个额外的输出叫做Ht-1，它既是神经网络的输出也是输入。这就是RNN模拟记忆的方式——考虑之前的结果，并把它们作为额外的输入。因此，结果<img src="img/B16060_06_24b.png" alt="b"/>将取决于输入xt，但也取决于Ht-1。现在，我们可以表示输入到同一个神经网络的四个输入序列:</p>
			<div><div><img src="img/B16060_06_25.jpg" alt="Figure 6.25: Overview of an RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.25:RNN概述</p>
			<p>我们可以看到，神经网络在每个时间步(<code>t</code>、<code>t+1</code>、…、<code>t+3</code>)接收一个输入(<code>x</code>)并生成一个输出(<code>y</code>)，还有另一个输出(<code>h</code>)，它为下一次迭代提供信息。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">前面的图可能有点误导——这里实际上只有一个RNN(中间的所有RNN盒形成一个神经网络)，但在这种格式下更容易看到测序是如何工作的。</p>
			<p>RNN细胞内部看起来像这样:</p>
			<div><div><img src="img/B16060_06_26.jpg" alt="Figure 6.26: Internal workings of an RNN using tanh&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.26:使用tanh的RNN的内部工作方式</p>
			<p>它非常类似于一个简单的神经元，但它需要更多的输入，并使用<code>tanh</code>作为激活函数。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">您可以在RNN单元中使用任何激活功能。TensorFlow中的默认值是<code>tanh</code>。</p>
			<p>这是RNNs的基本逻辑。在TensorFlow中，我们可以用<code>layers.SimpleRNN</code>实例化一个RNN层:</p>
			<pre>from tensorflow.keras import layers
layers.SimpleRNN(4, activation='tanh')</pre>
			<p>在代码片段中，我们创建了一个带有<code>4</code>输出和<code>tanh</code>激活函数(这是RNNs使用最广泛的激活函数)的RNN层。</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor223"/>GRU层</h2>
			<p>先前类型的层的一个缺点是最终输出考虑了所有先前的输出。如果您有一个1000个输入单元的序列，那么最终输出<code>y</code>会受到之前每个结果的影响。如果这个序列由1，000个单词组成，并且我们试图预测下一个单词，那么在进行预测之前必须记住所有的1，000个单词将是大材小用。大概，你只需要从最后的输出看前面的100个字。</p>
			<p>这正是<strong class="bold">门控循环单位</strong> ( <strong class="bold"> GRU </strong>)细胞的用途。让我们看看它们里面有什么:</p>
			<div><div><img src="img/B16060_06_27.jpg" alt="Figure 6.27: Internal workings of an RNN using tanh and sigmoid&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.27:使用tanh和sigmoid的RNN的内部工作原理</p>
			<p>与简单的RNN细胞相比，GRU细胞有更多的元素:</p>
			<ul>
				<li>第二激活功能，即<code>sigmoid</code></li>
				<li>在产生输出<img src="img/B16060_06_27a.png" alt="39"/>和Ht之前执行的乘法运算</li>
			</ul>
			<p>带有<code>tanh</code>的通常路径仍然负责做出预测，但是这一次我们将称它为“候选”。sigmoid路径充当“更新”门。这将告诉GRU小区它是否需要放弃使用这个候选。记住输出范围在<strong class="bold"> 0 </strong>和<strong class="bold"> 1 </strong>之间。如果接近0，更新门(即sigmoid路径)会说我们不应该考虑这个候选。</p>
			<p>另一方面，如果它更接近1，我们肯定应该使用这个候选人的结果。</p>
			<p>记住输出Ht和Ht-1有关，Ht-1和Ht-2有关，以此类推。因此，这个更新门也将定义我们应该保留多少“内存”。它倾向于优先考虑更接近当前产出的先前产出。</p>
			<p>这是GRU的基本逻辑(注意，GRU单元还有一个组件，即复位门，但为了简单起见，我们不看它)。在TensorFlow中，我们可以用<code>layers.GRU</code>实例化这样一个层:</p>
			<pre>from tensorflow.keras import layers
layers.GRU(4, activation='tanh', \
           recurrent_activation='sigmoid')</pre>
			<p>在代码片段中，我们创建了一个带有<code>4</code>输出单元的GRU层，以及用于候选预测的<code>tanh</code>激活函数和用于更新门的sigmoid。</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor224"/>LSTM层</h2>
			<p>re是RNN建筑中另一种非常流行的单元，叫做LSTM单元。<strong class="bold"> LSTM </strong>代表<strong class="bold">长短期记忆</strong>。LSTM先于GRU，但后者要简单得多，这就是我们首先提出它的原因。以下是LSTM的内幕:</p>
			<div><div><img src="img/B16060_06_28.jpg" alt="Figure 6.28: Overview of LSTM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">图6.28:LSTM概述</p>
			<p>起初，这看起来非常复杂。它由几个要素组成:</p>
			<ul>
				<li><code>Cell state</code>:这是所有先前输出的串联。这是LSTM细胞的“记忆”。</li>
				<li><code>Forget gate</code>:这个负责定义我们是应该保留还是忘记一段给定的记忆。</li>
				<li><code>Input gate</code>:负责定义新的候选内存是否需要更新。然后，这个新的候选存储器被添加到先前的存储器中。</li>
				<li><code>Output gate</code>:负责根据之前的输出(Ht-1)、当前的输入(xt)和存储器进行预测。</li>
			</ul>
			<p>LSTM细胞可以考虑以前的结果，也可以考虑过去的记忆，这就是它如此强大的原因。</p>
			<p>在TensorFlow中，我们可以用<code>layers.SimpleRNN</code>实例化这样一个层:</p>
			<pre>from tensorflow.keras import layers
layers.LSTM(4, activation='tanh', \
            recurrent_activation='sigmoid')</pre>
			<p>在代码片段中，我们创建了一个LSTM层，它具有用于候选预测的<code>4</code>输出单元和<code>tanh</code>激活函数，以及用于更新门的sigmoid。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在这里阅读更多关于TensorFlow中SimpleRNN的实现:<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN">https://www . tensor flow . org/API _ docs/python/TF/keras/layers/simple rnn</a>。</p>
			<h2 id="_idParaDest-203">活动6.03:评估<a id="_idTextAnchor226"/>用RNN评估雅虎股票模型</h2>
			<p>在这项活动中，我们将和LSTM一起训练一个RNN模型来预测雅虎的股票价格。基于过去<code>30</code>天的数据。我们将找到最佳的均方误差值，并检查模型是否过度拟合。我们将使用在第2章、<em class="italic">回归介绍</em>中看到的相同的雅虎股票数据集。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">数据集文件也可以在我们的GitHub资源库中找到:<a href="https://packt.live/3fRI5Hk">https://packt.live/3fRI5Hk</a>。</p>
			<p>以下步骤将帮助您完成此活动:</p>
			<ol>
				<li value="1">导入雅虎股票数据集。</li>
				<li>提取<code>close price</code>列。</li>
				<li>标准化数据集。</li>
				<li>创建前<code>30</code>天的股价特征。</li>
				<li>重塑训练集和测试集。</li>
				<li>Create the neural network architecture with the following layers:<p>五层LSTM带<code>LSTM(50, (3,3), activation='relu') followed by Dropout(0.2)</code></p><p>与<code>Dense(1)</code>完全连接的层</p></li>
				<li>指定一个学习率为<code>0.001</code>的<code>Adam</code>优化器。</li>
				<li>训练模型。</li>
				<li>在测试集上评估模型。</li>
			</ol>
			<p>预期输出如下:</p>
			<pre>1000/1000 [==============================] - 0s 279us/sample - loss: 0.0016 - mse: 0.0016
[0.00158528157370165, 0.0015852816]</pre>
			<p class="callout-heading">注意</p>
			<p class="callout">这项活动的解决方案可以在第387页找到。</p>
			<p>在下一部分，我们将了解深度学习所需的硬件。</p>
			<h2 id="_idParaDest-204">深L <a id="_idTextAnchor227"/>收入的硬件</h2>
			<p>你可能已经注意到了，训练深度学习模型比传统的机器学习算法需要更长的时间。这是因为正向传递和反向传播需要大量的计算。在本书中，我们训练了只有几层的非常简单的模型。但是有些架构有数百层，有些甚至更多。这种网络可能需要几天甚至几周的时间来训练。</p>
			<p>为了加快训练过程，建议使用一种称为GPU的特定硬件。GPU擅长执行数学运算，因此非常适合深度学习。与<strong class="bold">中央处理器</strong> ( <strong class="bold"> CPU </strong>)相比，GPU在训练深度学习模型方面可以快10倍。你可以个人买个GPU，组建自己的深度学习电脑。你只需要得到一个符合CUDA的(目前只有NVIDIA GPUs是)。</p>
			<p>另一种可能是使用云提供商，如AWS或Google云平台，并在云端训练您的模型。你只需为你所使用的东西付费，并且在用完之后可以马上关掉它们。好处是您可以根据项目的需要扩大或缩小配置，但要注意成本。即使你不是在训练一个模型，你也要为你的实例运行的时间付费。所以，不要忘记关掉你不用的东西。</p>
			<p>最后，谷歌最近发布了一些致力于深度学习的新硬件:<strong class="bold">张量处理单元</strong> ( <strong class="bold"> TPUs </strong>)。它们比GPU快得多，但成本相当高。目前，只有谷歌云平台在其云实例中提供这样的硬件。</p>
			<h2 id="_idParaDest-205">挑战和未来趋势</h2>
			<p>与任何新技术一样，深度学习也伴随着挑战。其中之一是进入的巨大障碍。要成为一名深度学习实践者，你以前必须非常了解深度学习背后的所有数学理论，并且是一名经过确认的程序员。最重要的是，你必须了解你选择使用的深度学习框架的细节(无论是TensorFlow、PyTorch、Caffe还是其他什么)。有一段时间，深度学习无法触及广泛的受众，主要局限于研究人员。这种情况已经改变，尽管并不完美。例如，TensorFlow现在提供了一个更高级的API，叫做Keras(这就是你在本章看到的那个),它比核心API更容易使用。希望这一趋势将继续下去，并使对该领域感兴趣的任何人都更容易获得深度学习框架。</p>
			<p>第二个挑战是深度学习模型需要大量的计算能力，如前一节所述。对于任何想尝试一下的人来说，这又是一个主要的障碍。即使GPU的成本已经下降，深度学习仍然需要一些前期投资。幸运的是，现在有了一个用GPU训练深度学习模型的免费选项:Google Colab。这是谷歌的一项举措，旨在通过免费提供临时云计算来促进研究。你唯一需要的是一个谷歌账户。注册后，你可以创建笔记本(类似于Jupyter笔记本)，并选择在CPU、GPU(每天限于10小时)甚至TPU(每天限于1小时)上运行的内核。所以，在投资购买或出租GPU之前，可以先用Google Colab练习一下。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在https://colab.research.google.com/找到更多关于谷歌实验室的信息。</p>
			<p>更高级的深度学习模型可能非常深入，需要数周的训练。所以，基础的从业者很难使用这样的架构。但值得庆幸的是，许多研究人员已经接受了开源运动，不仅分享了他们设计的架构，还分享了网络的权重。这意味着您现在可以访问最先进的预训练模型，并对其进行微调以适应您自己的项目。这被称为迁移学习(这超出了本书的范围)。它在计算机视觉领域非常流行，例如，你可以在ImageNet或MS-Coco上找到预先训练好的模型，这些模型是大型图片数据集。自然语言处理中也在发生迁移学习，但没有计算机视觉那么发达。</p>
			<p class="callout-heading">注意</p>
			<p class="callout">你可以在<a href="http://www.image-net.org/">http://www.image-net.org/</a>和<a href="http://cocodataset.org/">http://cocodataset.org/</a>找到关于这些数据集的更多信息。</p>
			<p>与深度学习相关的另一个非常重要的主题是越来越需要能够解释模型结果。很快，这些类型的算法可能会受到监管，深度学习从业者将必须能够解释为什么模型会做出给定的决定。目前，由于网络的复杂性，深度学习模型更像是黑盒。研究人员已经有一些举措来寻找解释和理解深度神经网络的方法，如<em class="italic">泽勒和弗格斯</em>、<em class="italic">可视化和理解卷积网络</em>、<em class="italic"> ECCV 2014 </em>。然而，随着这些技术在我们日常生活中的民主化，在这一领域还需要做更多的工作。例如，我们需要确保这些算法没有偏见，不会做出影响特定人群的不公平决定。</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor230"/>总结</h1>
			<p>我们刚刚完成了《应用人工智能工作坊》第二版的全书。在这个工作坊中，我们学习了人工智能及其应用的基础知识。我们写了一个Python程序来玩井字游戏。我们学习了广度优先搜索和深度优先搜索等搜索技术，以及它们如何帮助我们解决井字游戏。</p>
			<p>在接下来的几章中，我们学习了使用回归和分类的监督学习。这些章节包括数据预处理、训练测试分割和在几个真实场景中使用的模型。当预测股票数据时，线性回归、多项式回归和支持向量机都派上了用场。使用k-最近邻和支持向量分类器进行分类。几个活动帮助您在一个有趣的现实生活用例中应用分类的基础:信用评分。</p>
			<p>在<em class="italic">第4章</em>、<em class="italic">决策树介绍</em>中，向您介绍了决策树、随机森林和极度随机树。本章介绍了评估模型效用的不同方法。我们学习了如何计算模型的准确度、精确度、召回率和F1值。我们还学习了如何创建模型的混淆矩阵。通过对car数据的评估，将本章的模型应用于实践。</p>
			<p>无监督learni <a id="_idTextAnchor232"/> ng在<em class="italic">第5章</em>、<em class="italic">人工智能:聚类</em>中介绍，以及k-means和层次聚类算法。这些算法的一个有趣的方面是标签不是预先给定的，而是在聚类过程中检测到的。</p>
			<p>本次研讨会以第6章、<em class="italic">神经网络和深度学习</em>结束，其中介绍了使用TensorFlow的神经网络和深度学习。我们使用这些技术在实际应用中实现了最佳的准确性，例如检测书写数字、图像分类和时间序列预测。</p>
		</div>
		<div><div/>
		</div>
	

</body></html>