

# 五、监督学习——关键步骤

概观

在本章中，我们将深入研究神经网络的概念，并描述正向和反向传播的过程。我们将使用神经网络解决监督学习分类问题，并通过执行误差分析来分析神经网络的结果。

本章结束时，你将能够训练一个网络来解决一个分类问题，并微调网络的一些超参数来提高其性能。

# 简介

在前一章中，我们探索了三种机器学习算法来解决监督学习任务，无论是分类还是回归。在这一章中，我们将探索当今最流行的机器学习算法之一，人工神经网络，它属于机器学习的一个子群，称为深度学习。

**人工神经网络** ( **ANNs** )，也被称为**多层感知器** ( **MLPs** )，已经变得越来越受欢迎，主要是因为它们提出了一种复杂的算法，可以处理几乎任何具有挑战性的数据问题。尽管该理论是几十年前在 20 世纪 40 年代发展起来的，但这种网络现在越来越受欢迎，这要归功于允许收集大量数据的技术进步，以及允许用大量数据训练复杂算法的计算机基础设施的发展。

因此，下一章将重点介绍人工神经网络，它们的不同类型，以及它们的优缺点。此外，根据上一章所述，人工神经网络将用于根据个人的人口统计和财务信息预测个人的收入，以展示人工神经网络与其他监督学习算法相比在性能上的差异。

# 人工神经网络

虽然有几种机器学习算法可用于解决数据问题，但正如我们已经说过的，人工神经网络在数据科学家中变得越来越受欢迎，因为它们能够在大型复杂的数据集中找到人类无法解释的模式。

名称中的**神经**部分指的是模型的架构与人类大脑解剖结构的相似性。这一部分旨在复制人类从历史数据中学习的能力，方法是在神经元之间传递数据，直到得出结果。

在下图中，显示了一个人类神经元，其中 A 代表从其他神经元接收输入信息的**树突**，B 代表处理信息的神经元的**核**，C 代表监督将处理后的信息传递给下一个神经元的过程的**轴突**:

![Figure 5.1: Visual representation of a human neuron
](img/B15781_05_01.jpg)

图 5.1:人类神经元的视觉表现

此外，**人工**部分指的是模型的实际学习过程，其主要目标是最小化模型中的误差。这是一个人工学习过程，因为没有关于人类神经元如何处理接收到的信息的真实证据，因此该模型依赖于将输入映射到期望输出的数学函数。

## 人工神经网络是如何工作的？

在我们深入研究人工神经网络的流程之前，让我们先来看看它的主要组成部分:

*   `X`，因为它包含数据集的所有数据(每个实例都有其特征)。
*   **Hidden layers**: This layer is in charge of processing the input data in order to find patterns that are useful for making a prediction. The ANN can have as many hidden layers as desired, each with as many neurons (units) as required. The first layers are in charge of the simpler patterns, while the layers at the end search for the more complex ones.

    隐藏层使用一组表示权重和偏差的变量来帮助训练网络。权重和偏差的值用作在每次迭代中变化的变量，以使预测接近实际情况。这个后面会解释。

*   `Y_hat`，这一层是模型根据从隐藏层接收的数据做出的预测。该预测以概率的形式呈现，其中具有较高概率的类别标签是被选择作为预测的类别标签。

下图说明了前面三层的架构，其中 1 下面的圆圈表示输入层的神经元，2 下面的圆圈表示 2 个隐藏层的神经元(每层由一列圆圈表示)，最后，3 下面的圆圈是输出层的神经元:

![Figure 5.2: Basic architecture of an ANN
](img/B15781_05_02.jpg)

图 5.2:人工神经网络的基本结构

作为类比，考虑制造汽车零件的制造过程。这里，输入层由原材料组成，在这种情况下，原材料可以是铝。该过程的初始步骤包括抛光和清洁材料，这可以被视为第一对隐藏层。接下来，材料被弯曲以实现汽车部件的形状，这由更深的隐藏层来处理。最后将零件交付给客户端，可以认为是输出层。

考虑到这些步骤，制造过程的主要目标是实现与该过程旨在构建的零件高度相似的最终零件，这意味着输出`Y_hat`应最大化其与`Y`(基本事实)的相似性，以使模型被认为与数据非常吻合。

训练人工神经网络的实际方法是一个迭代过程，包括以下步骤:前向传播、成本函数的计算、反向传播以及权重和偏差更新。一旦权重和偏差被更新，该过程再次开始，直到满足迭代次数。

让我们详细探索迭代过程的每个步骤。

### 正向传播

输入层向人工神经网络提供初始信息。数据的处理是通过在网络的深度(隐藏层数)和宽度(每层中的单元数)传播数据位来完成的。信息由每层中的每个神经元使用线性函数进行处理，并结合旨在打破线性的激活函数，如下所示:

![Figure 5.3: The linear and activation functions used by an ANN
](img/B15781_05_03.jpg)

图 5.3:人工神经网络使用的线性和激活函数

这里， *W* 1 和 *b* 1 分别是包含权重和偏差的矩阵和向量，并且用作可以通过迭代更新以训练模型的变量。 *Z* 1 是给定神经元的线性函数，而 *A* 1 是将激活函数(由 sigma 符号表示)应用于线性函数后该单元的输出。

为每层中的每个神经元计算前两个公式，其中隐藏层(而不是输入层)的值 *X* 被前一层的输出( *A* n)替换，如下所示:

![Figure 5.4: The values calculated for the second layer of the ANN
](img/B15781_05_04.jpg)

图 5.4:人工神经网络第二层的计算值

最后，来自最后一个隐藏层的输出被馈送到输出层，在输出层中再次计算线性函数以及激活函数。在根据需要进行一些处理之后，这一层的结果将与实际情况进行比较，以便在进入下一次迭代之前评估算法的性能。

第一次迭代的权重值在 0 和 1 之间随机初始化，而偏差值最初可以设置为 0。一旦第一次迭代运行，值将被更新，以便流程可以重新开始。

激活功能可以是不同类型的。一些最常见的是**整流线性单元**(**ReLU**)**双曲正切** ( **tanh** )以及 **Sigmoid** 和 **Softmax** 函数，这些将在后续章节中解释。

### 成本函数

考虑到训练过程的最终目标是基于映射预期输出的给定数据集建立模型，通过比较预测值(`Y_hat`)和实际值(`Y`)之间的差异来测量模型估计`X`和`Y`之间关系的能力尤为重要。这是通过计算成本函数(也称为**损失函数**)来确定模型的预测有多差来实现的。为每次迭代计算成本函数，以测量模型在迭代过程中的进度，目标是找到最小化成本函数的权重和偏差的值。

对于分类任务，最常用的代价函数是**交叉熵代价函数**，其中代价函数的值越高，预测值与实际值之间的偏差越大。

对于二元分类任务，即仅具有两个类输出标签的任务，交叉熵成本函数计算如下:

```py
cost = -(y * log(yhat) + (1-y) *(1-yhat))
```

这里， *y* 将是 1 或 0(两个类别标签中的任一个)， *y* 将是模型计算的概率， *log* 将是自然对数。

对于多类分类任务，公式如下:

![Figure 5.5: The cost function for a multiclass classification task
](img/B15781_05_05.jpg)

图 5.5:多类分类任务的成本函数

这里， *c* 表示类别标签， *M* 表示类别标签的总数。

一旦计算了成本函数，训练过程就继续执行反向传播步骤，这将在下一节中解释。

此外，对于回归任务，成本函数将是 RMSE，这在*第 3 章*、*监督学习-关键步骤*中有所解释。

### 反向传播

反向传播过程被引入作为神经网络训练过程的一部分，以使学习更快。它基本上包括计算成本函数相对于权重和网络偏差的偏导数。这样做的目的是通过改变权重和偏差来最小化成本函数。

考虑到权重和偏差不直接包含在成本函数中，使用链规则从成本函数向后传播误差，直到它到达网络的第一层。接下来，计算导数的加权平均值，将其用作在运行新的迭代之前更新权重和偏差的值。

有几种算法可用于执行反向传播，但最常用的是**梯度下降**。梯度下降是一种优化算法，试图找到某个函数的局部或全局最小值，在这种情况下，该函数是成本函数。它通过确定模型应该移动的方向来减少误差。

例如，下图显示了通过不同迭代的人工神经网络的训练过程的示例，其中反向传播的工作是确定权重和偏差应该更新的方向，以便误差可以继续最小化，直到它达到最小点:

![Figure 5.6: Example of the iterative process of training an ANN
](img/B15781_05_06.jpg)

图 5.6:训练人工神经网络的迭代过程示例

重要的是要强调，反向传播并不总是找到全局最小值，因为它一旦到达斜率的最低点就停止更新，而不管任何其他区域。例如，考虑下图:

![Figure 5.7: Examples of minimum points
](img/B15781_05_07.jpg)

图 5.7:最小点示例

尽管与它们左边和右边的点相比，这三个点都可以被认为是最小点，但是它们中只有一个是全局最小值。

### 更新权重和偏差

取反向传播过程中计算的导数的平均值，迭代的最后一步是更新权重和偏差的值。此过程使用以下公式来更新权重和偏差:

```py
New weight = old weight – derivative rate * learning rate
New bias = old bias – derivative rate * learning rate
```

这里，旧值是用于执行正向传播步骤的值，导数率是从反向传播步骤获得的值，其对于权重和偏差是不同的，并且学习率是用于抵消导数率的影响的常数，使得权重和偏差的变化小而平滑。这已被证明有助于更快地达到最低点。

一旦权重和偏差被更新，整个过程再次开始。

## 了解超参数

正如您到目前为止所看到的，超参数是可以微调以提高模型准确性的参数。对于神经网络，超参数可以分为两大类:

*   那些改变网络结构的
*   那些修改过程来训练它的人

构建人工神经网络的一个重要部分是通过执行错误分析和摆弄有助于解决影响网络的条件的超参数来微调超参数的过程。作为一般提示，遭受高偏差的网络通常可以通过创建更大的网络或训练更长的持续时间(即，更多的迭代)来改善，而遭受高方差的网络可以受益于更多训练数据的添加或通过引入正则化技术，这将在后续部分中解释。

考虑到可以为训练人工神经网络而改变的超参数的数量很大，最常用的超参数将在下面的章节中解释。

### 隐藏层数和单元数

如前所述，研究人员可以设置隐藏层的数量和每层中的单元数量。同样，没有精确的科学来选择这个数字，相反，这个数字的选择是测试不同近似的微调过程的一部分。

尽管如此，在选择隐藏层的数量时，一些数据科学家倾向于一种方法，其中训练多个网络，每个网络都有一个额外的层。误差最小的模型是具有正确隐藏层数的模型。不幸的是，这种方法并不总是很好，因为更复杂的数据问题并不真正通过简单地改变隐藏层的数量来显示性能的差异，而不管其他超参数。

另一方面，有几种方法可以用来选择隐藏层中的单元数。数据科学家通常会根据在线提供的类似研究论文来选择这两个超参数的初始值。这意味着一个好的起点是复制已经成功用于类似领域项目的网络架构，然后通过误差分析，微调超参数以提高性能。

尽管如此，重要的是要考虑这样一个事实，即根据研究活动，较深的网络(有许多隐藏层的网络)比较宽的网络(每层有许多单元的网络)表现更好。

### 激活功能

如前所述，激活函数用于向模型引入非线性。最常用的激活功能如下:

*   **ReLU** :该函数的输出要么是 0，要么是线性函数导出的数，取较高者。这意味着每当这个数字大于 0 时，输出将是它接收到的原始数字，否则，输出将是 0。
*   **Tanh** :该函数由输入的双曲正弦除以双曲余弦组成。输出是一个介于-1 和 1 之间的数字。
*   **S 形**:功能呈 S 形。它接受输入并将其转换成概率。该函数的输出介于 0 和 1 之间。
*   **Softmax** :类似于 sigmoid 函数，它计算输入的概率，不同之处在于 Softmax 函数可用于多类分类任务，因为它能够计算一个类标签相对于其他类标签的概率。

激活函数的选择应考虑到，按照惯例，ReLU 和双曲正切(tanh)激活函数都用于所有隐藏层，ReLU 由于其与大多数数据问题相关的性能而在科学家中最受欢迎。

此外，Sigmoid 和 Softmax 激活函数应用于输出图层，因为它们的结果是概率的形式。Sigmoid 激活函数用于二元分类问题，因为它只输出两个类标签的概率，而 Softmax 激活函数可用于二元或多类分类问题。

### 正规化

正则化是一种在机器学习中使用的技术，用于改善正在遭受过拟合的模型，这意味着该超参数主要在严格要求时使用，其主要目标是增加模型的泛化能力。

正则化技术有多种，但最常用的是 L1、L2 和剔除技术。尽管 scikit-learn 仅支持 L2 的 MLP 分类器，但对这三种正则化形式的简要说明如下:

*   L1 和 L2 技术将正则化项添加到成本函数中，作为惩罚可能影响模型性能的高权重的一种方式。这些方法之间的主要区别在于，L1 的正则化项是权重大小的绝对值，而 L2 的正则化项是权重大小的平方。对于常规数据问题，L2 已被证明效果更好，而 L1 主要用于特征提取任务，因为它可以创建稀疏模型。
*   另一方面，Dropout 是指模型在迭代的一个步骤中放弃一些单元以忽略其输出的能力，这简化了神经网络。dropout 值设置在 0 和 1 之间，它表示将被忽略的单位的百分比。在每个迭代步骤中被忽略的单元是不同的。

### 批量大小

在构建人工神经网络的过程中，另一个需要调整的超参数是批量大小。这是指在一次迭代过程中输入到神经网络的实例数量，它将用于在网络中执行向前和向后传递。对于下一次迭代，将使用一组新的实例。

这种技术还有助于提高模型对训练数据进行归纳的能力，因为在每次迭代中，都会向模型提供新的实例组合，这在处理过度拟合的模型时非常有用。

注意

根据多年的研究结果，一个好的做法是将批量大小设置为 2 的倍数。一些最常见的值是 32、64、128 和 256。

### 学习率

如前所述，引入学习率是为了帮助确定模型在每次迭代中达到局部或全局最小值所需的步长。学习率越低，网络的学习过程就越慢，但这会产生更好的模型。另一方面，学习率越大，模型的学习过程越快；但是，这可能会导致模型不收敛。

注意

默认学习率值通常设置为 0.001。

### 迭代次数

如前所述，通过迭代过程训练神经网络。因此，有必要设置模型将执行的迭代次数。设置理想迭代次数的最佳方式是从 200 到 500 之间的较低值开始，并在每次迭代的成本函数图显示递减线的情况下增加迭代次数。不用说，迭代次数越大，训练一个模型的时间就越长。

此外，增加迭代次数是一种已知的解决网络不足的技术。这是因为它给了网络更多的时间来找到归纳到训练数据的正确权重和偏差。

## 神经网络的应用

除了前面的架构之外，由于神经网络的流行，随着时间的推移，出现了许多新的架构。其中最受欢迎的是**卷积神经网络**，它可以通过使用过滤器作为层来处理图像，以及**循环神经网络**，它用于处理文本翻译等数据序列。

由于这个原因，神经网络的应用扩展到几乎任何数据问题，从简单到复杂。虽然神经网络能够在非常大的数据集中发现模式(无论是用于分类还是回归任务)，但它们也以有效处理挑战性问题而闻名，例如自动驾驶汽车的自主能力，聊天机器人的构建和人脸识别。

## 神经网络的局限性

训练神经网络的一些限制如下:

*   训练过程需要时间。不管使用什么超参数，它们通常都需要时间来收敛。
*   为了更好地工作，他们需要非常大的数据集。神经网络意味着更大的数据集，因为它们的主要优势是能够在数百万个值中找到模式。
*   它们被认为是一个黑箱，因为没有网络如何得出结果的实际知识。虽然训练过程背后的数学是清楚的，但是不可能知道模型在被训练时做出什么假设。
*   硬件要求大。同样，问题越复杂，对硬件的要求就越大。

虽然人工神经网络可以应用于几乎任何数据问题，但由于其局限性，在处理更简单的数据问题时，测试其他算法总是一个好的做法。这很重要，因为将神经网络应用于可以通过更简单的模型解决的数据问题会使成本超过收益。

# 应用人工神经网络

现在您已经知道了人工神经网络的组件，以及它训练模型和进行预测所遵循的不同步骤，让我们使用 scikit-learn 库来训练一个简单的网络。

在本主题中，scikit-learn 的神经网络模块将用于使用前一章的练习和活动中使用的数据集(即生育率数据集和经处理的人口普查收入数据集)来训练网络。值得一提的是，scikit-learn 不是最适合神经网络的库，因为它目前不支持许多类型的神经网络，并且它在更深层次网络上的性能不如其他神经网络专用库，如 TensorFlow 和 PyTorch。

scikit-learn 中的神经网络模块目前支持用于分类的 MLP、用于回归的 MLP 和受限玻尔兹曼机器架构。考虑到案例研究由分类任务组成，将使用分类 MLP。

## Scikit-Learn 的多层感知器

MLP 是一种监督学习算法，顾名思义，它使用多个层(隐藏层)来学习一个非线性函数，该函数将输入值转换为输出值，用于分类或回归。正如我们之前所解释的，层的每个单元的工作是通过计算线性函数来转换从前一层接收的数据，然后应用激活函数来打破线性。

值得一提的是，MLP 具有非凸损失函数，如前所述，这意味着可能存在多个局部最小值。这意味着权重和偏差的不同初始化将导致不同的训练模型，从而指示不同的精度水平。

scikit-learn 中的 MLP 分类器具有大约 20 个不同的与架构或学习过程相关联的超参数，这些超参数可以被改变以修改网络的训练过程。幸运的是，所有这些超参数都设置了默认值，这使得我们可以毫不费力地运行一个初始模型。该模型的结果可用于根据需要调整超参数。

为了训练 MLP 分类器，需要输入两个数组:首先是包含训练数据的维度输入(`n_samples`、`n_features`)的`X`，然后是包含每个样本标签值的维度输入(`n_sample`)的`Y`。

与我们在上一章中看到的算法类似，使用`fit`方法训练模型，然后在训练好的模型上使用`predict`方法获得预测。

## 练习 5.01:应用 MLP 分类器

在本练习中，您将使用 scikit-learn 的 MLP 训练一个模型来解决一个分类任务，该任务包括确定受试者的生育能力是否受到其人口统计数据、环境条件和既往医疗状况的影响。

注意

对于本章中的练习和活动，您需要在系统上安装 Python 3.7、NumPy、Jupyter、pandas 和 scikit-learn。

1.  打开一个 Jupyter 笔记本来实现这个练习。导入所有必要的元素来读取数据集和计算模型的准确性，以及 scikit-learn 的`MLPClassifier`类:

    ```py
    import pandas as pd from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score
    ```

2.  使用上一章的生育数据集，阅读`.csv`文件。考虑到数据集不包含标题行:

    ```py
    data = pd.read_csv("fertility_Diagnosis.csv", header=None)
    ```

    ，确保将等于`None`的`header`参数添加到`read_csv`函数中
3.  将数据集分成`X`和`Y`集合，以便将特征数据从标签值中分离出来:

    ```py
    X = data.iloc[:,:9] Y = data.iloc[:,9]
    ```

4.  Instantiate the `MLPClassifier` class from the `neural_network` module of scikit-learn and use the `fit` method to train a model. When instantiating the model, leave all the hyperparameters at their default values, but add a `random_state` argument equal to `101` to ensure that you get the same results as the one shown in this exercise:

    ```py
    model = MLPClassifier(random_state=101)
    model = model.fit(X, Y)
    ```

    解决运行`fit`方法后出现的警告:

    ![Figure 5.8: Warning message displayed after running the fit method
    ](img/B15781_05_08.jpg)

    图 5.8:运行 fit 方法后显示的警告消息

    正如您所看到的，警告指出在运行默认的迭代次数之后，也就是`200`，模型还没有达到收敛。

5.  To address this issue, try higher values for the iterations until the warning stops appearing. To change the number of iterations, add the `max_iter` argument inside the parentheses during the instantiation of the model:

    ```py
    model = MLPClassifier(random_state=101, max_iter =1200)
    model = model.fit(X, Y)
    ```

    此外，警告下方的输出解释了用于 MLP 所有超参数的值。

6.  Finally, perform a prediction by using the model that you trained previously, for a new instance with the following values for each feature: `−0.33`, `0.69`, `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`.

    使用以下代码:

    ```py
    pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])
    print(pred)
    ```

    模型的预测等于`N`，即模型预测具有指定特征的人诊断正常。

7.  Calculate the accuracy of your model, based on the predictions it achieves over the `X` variable, as follows:

    ```py
    pred = model.predict(X)
    score = accuracy_score(Y, pred)
    print(score)
    ```

    你的模型精度等于`98%`。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2BaKHRe](https://packt.live/2BaKHRe)。

    你也可以在 https://packt.live/37tTxpv 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地训练和评估了 MLP 模型的性能。

## 活动 5.01:为人口普查收入数据集训练一个 MLP

为了将前一章中训练的算法的性能与神经网络的性能进行比较，在本次活动中，我们将继续使用预处理的人口普查收入数据集。考虑以下场景:您的公司不断为员工提供课程以提高他们的能力，您最近了解了神经网络及其能力。您决定建立一个网络来对之前提供给您的数据集进行建模，以测试神经网络是否更擅长根据人口统计数据预测个人收入。

注意

使用上一章的预处理数据集开始本活动:`census_income_dataset_preprocessed.csv`。你也可以在本书的 GitHub 资源库中找到预处理过的数据集，该资源库位于 https://packt.live/2UQIthA[的](https://packt.live/2UQIthA)。

执行以下步骤来完成本练习:

1.  导入加载和分割数据集、训练 MLP 以及测量精度所需的所有元素。
2.  使用预处理的人口普查收入数据集，从目标中分离特征，创建变量`X`和`Y`。
3.  Divide the dataset into training, validation, and testing sets, using a split ratio of 10%.

    注意

    在执行数据集分割时，请记住继续使用与`101`相等的`random_state`参数，以便设置一个种子来获得与本书中相同的结果。

4.  Instantiate the `MLPClassifier` class from scikit-learn and train the model with the training data.

    保留所有超参数的默认值。再次使用一个等于 101 的`random_state`。

    虽然会出现一个警告，指出在给定的迭代中没有达到收敛，但不要处理这个警告，因为超参数微调将在本章的以下部分中探讨。

5.  Calculate the accuracy of the model for all three sets (training, validation, and testing).

    注意

    这项活动的解决方案可在第 240 页找到。

    三个集合的准确度分数应该如下:

    列车组= 0.8465

    开发集= 0.8246

    测试集= 0.8415

# 性能分析

在下一节中，我们将首先使用准确性度量作为工具来执行错误分析，以确定影响(在更大程度上)算法性能的条件。一旦模型被诊断，超参数可以被调整以提高算法的整体性能。最终模型将与前一章中创建的模型进行比较，以确定神经网络是否优于其他模型。

## 错误分析

使用在*活动 5.01* 、*为我们的人口普查收入数据集*训练 MLP 中计算的准确度分数，我们可以计算每个数据集的错误率，并将它们相互比较，以诊断影响模型的条件。为此，假设贝叶斯误差等于 1%，考虑到上一章中的其他模型能够达到超过 97%的精度水平:

![Figure 5.9: Accuracy score and error rate of the network
](img/B15781_05_09.jpg)

图 5.9:网络的准确度分数和错误率

注意

考虑*图 5.9* ，请记住，为了检测影响网络的条件，有必要获取一个错误率，并从中减去高于该错误率的值。最大的正面差异是我们用来诊断模型的差异。

根据差异栏，很明显，在训练集中的错误率和贝叶斯错误之间找到了最大的差异。基于此，可以得出结论，模型正遭受*高偏差*，如前几章所述，可以通过训练更大的网络和/或训练更长的时间(更高的迭代次数)来处理。

## 超参数微调

通过误差分析，有可能确定网络正遭受高偏差。这一点非常重要，因为它指出了为了更大程度地提高模型的性能而需要采取的措施。

考虑到迭代的次数和网络的大小(层数和单元数)都应该使用试错法来改变，将进行以下实验:

![Figure 5.10: Suggested experiments to tune the hyperparameters
](img/B15781_05_10.jpg)

图 5.10:优化超参数的建议实验

注意

由于实验的复杂性，有些实验可能需要更长的时间。例如，实验 3 将比实验 2 花费更长的时间。

这些实验背后的想法是能够测试不同超参数的不同值，以便找出是否可以实现改进。如果通过这些实验实现的改进是显著的，那么应该考虑进一步的实验。

类似于将`random_state`参数添加到 MLP 的初始化中，迭代次数和网络大小的值的改变可以使用以下代码来实现，该代码显示了实验 3 的值:

```py
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(random_state=101, max_iter = 500, \
                      hidden_layer_sizes=(100,100,100))
model = model.fit(X_train, Y_train)
```

注意

要找到使用什么术语来改变每个超参数，请访问位于[的 scikit-learn 的`MLPClassifier`页面 http://sci kit-learn . org/stable/modules/generated/sk learn . neural _ network。MLPClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)

正如您在前面的代码片段中看到的，`max_iter`参数用于设置在网络训练期间运行的迭代次数。`hidden_layer_sizes`参数用于设置隐藏层的数量和每个层中的单元数量。例如，在前面的例子中，通过将参数设置为`(100,100,100)`，网络的架构有 3 个隐藏层，每个隐藏层有 100 个单元。当然，这种架构还包括所需的输入和输出层。

注意

使用示例来训练具有实验 3 的配置的网络，鼓励您尝试对实验 1 和实验 2 的配置执行训练过程。

在下表中可以看到运行前面实验的准确度分数:

![Figure 5.11: Accuracy scores for all experiments
](img/B15781_05_11.jpg)

图 5.11:所有实验的准确度分数

注意

请记住，调整超参数的主要目的是减少训练集的错误率和贝叶斯错误之间的差异，这就是为什么大多数分析都是通过仅考虑该值来完成的。

通过对实验的准确度分数的分析，可以得出结论，超参数的最佳配置是在实验 2 期间使用的配置。此外，考虑到增加迭代次数对算法的性能没有积极的影响，可以得出结论，尝试迭代次数的其他值很可能没有意义。

尽管如此，为了测试隐藏层的宽度，将考虑以下实验，使用实验 2 的迭代次数和隐藏层数量的选定值，但是改变每层中的单元数量:

![Figure 5.12: Suggested experiments to vary the width of the network
](img/B15781_05_12.jpg)

图 5.12:改变网络宽度的建议实验

显示了两个实验的准确度分数，并解释了其背后的逻辑:

![Figure 5.13: Accuracy scores for the second round of experiments
](img/B15781_05_13.jpg)

图 5.13:第二轮实验的准确度分数

可以看出，与初始模型相比，所有数据集的两个实验的精确度都降低了。通过观察这些值，可以得出结论，实验 2 的性能在测试集方面是最高的，这给我们留下了一个迭代 500 步的网络，其中有一个输入和输出层以及两个各有 100 个单元的隐藏层。

注意

没有理想的方法来测试超参数的不同配置。要考虑的唯一重要的事情是，焦点集中在那些解决以更大比例影响网络的条件的超参数上。如果你愿意，请随意尝试更多的实验。

考虑实验 2 的所有三组的准确度分数来计算错误率，最大的差异仍然在训练组错误和贝叶斯错误之间。这意味着模型可能不是数据集的最佳拟合，考虑到训练集误差不能更接近最小可能误差。

注意

要访问该特定部分的源代码，请参考[https://packt.live/3e2O8bS](https://packt.live/3e2O8bS)。

本节目前没有在线交互示例，需要在本地运行。

## 车型对比

当训练了一个以上的模型时，与创建模型的过程相关的最后一步是模型之间的比较，以便选择最能概括地表示训练数据的一个模型，以便它能很好地处理看不见的数据。

如前所述，这种比较必须通过仅使用被选择来测量模型对于数据问题的性能的度量来完成。这一点很重要，因为一个模型在每个指标上的表现可能大不相同，所以应该选择在理想指标下性能最大化的模型。

尽管为了能够执行错误分析，度量标准是对所有三组数据(训练、验证和测试)进行计算的，但是在大多数情况下，应该通过对测试组获得的结果进行优先排序来进行比较和选择。这主要是由于集的目的，考虑到训练集用于创建模型，验证集用于微调超参数，最后，测试集用于测量模型在看不见的数据上的整体性能。

考虑到这一点，在最大限度地改进所有模型之后，在测试集上具有优异性能的模型将是在看不见的数据上表现最好的模型。

## 活动 5.02:比较不同的模型，选择最适合人口普查收入数据问题的模型

考虑以下场景:在用可用数据训练了四个不同的模型之后，您被要求执行分析以选择最适合案例研究的模型。

注意

以下活动主要是分析性的。使用从上一章的活动以及本章的活动中获得的结果。

执行以下步骤来比较不同的型号:

1.  打开您用来训练模型的 Jupyter 笔记本。
2.  Compare the four models, based only on their accuracy scores. Fill in the details in the following table:![Figure 5.14: Accuracy scores of all four models for the Census Income Dataset
    ](img/B15781_05_14.jpg)

    图 5.14:人口普查收入数据集的所有四个模型的准确度得分

3.  On the basis of the accuracy scores, identify the model with the best performance.

    注意

    这项活动的解决方案可在第 242 页找到。

# 总结

本章主要关注人工神经网络(特别是 MLP)，人工神经网络在机器学习领域变得越来越重要，因为它们能够处理高度复杂的数据问题，这些问题通常使用非常大的数据集，其模式是人眼无法看到的。

主要目的是通过使用数学函数处理数据来模拟人脑的结构。用于训练人工神经网络的过程包括前向传播步骤、成本函数的计算、反向传播步骤以及帮助将输入值映射到输出的不同权重和偏差的更新。

除了权重和偏差的变量之外，人工神经网络还具有多个超参数，可以通过调整这些参数来改善网络的性能，这可以通过修改算法的架构或训练过程来实现。一些最流行的超参数是网络的大小(根据隐藏层和单元)、迭代次数、正则化项、批量大小和学习速率。

一旦涵盖了这些概念，我们就创建了一个简单的网络来解决上一章介绍的人口普查收入数据集问题。接下来，通过执行误差分析，我们微调了网络的一些超参数以提高其性能。

在下一章中，我们将学习如何开发一个端到端的机器学习解决方案，从对数据的理解和模型的训练开始，就像到目前为止所看到的那样，并以保存训练好的模型的过程结束，以便能够在将来使用它。