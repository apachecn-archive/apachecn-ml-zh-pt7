

# 附录

# 1。Scikit 简介-了解

## 活动 1.01:选择目标特征并创建目标矩阵

解决方案:

1.  Load the `titanic` dataset using the `seaborn` library:

    ```
    import seaborn as sns
    titanic = sns.load_dataset('titanic')
    titanic.head(10)
    ```

    前几行应该如下所示:

    ![Figure 1.22: An image showing the first 10 instances of the Titanic dataset
    ](img/B15781_01_22.jpg)

    图 1.22:显示泰坦尼克号数据集的前 10 个实例的图像

2.  Select your preferred target feature for the goal of this activity.

    首选目标特征可以是`survived`或`alive`。这主要是因为两者都标注了一个人是否在坠机事件中幸存。对于下面的步骤，已经选择的变量是`survived`。但是，选择`alive`不会影响变量的最终形状。

3.  创建特征矩阵和目标矩阵。确保将特征矩阵中的数据存储在变量 X 中，将目标矩阵中的数据存储在另一个变量 Y 中:

    ```
    X = titanic.drop('survived',axis = 1) Y = titanic['survived']
    ```

4.  Print out the shape of `X`, as follows:

    ```
    X.shape
    ```

    输出如下所示:

    ```
    (891, 14)
    ```

    对`Y`进行同样的操作:

    ```
    Y.shape
    ```

    输出如下所示:

    ```
    (891,)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/37BwgSv](https://packt.live/37BwgSv)。

    你也可以在[https://packt.live/2MXFtuP](https://packt.live/2MXFtuP)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地将数据集拆分为两个子集，稍后将使用这两个子集来训练模型。

## 活动 1.02:预处理整个数据集

解决方案:

1.  Import `seaborn` and the `LabelEncoder` class from scikit-learn. Next, load the `titanic` dataset and create the features matrix, including the following features: `sex`, `age`, `fare`, `class`, `embark_town`, and `alone`:

    ```
    import seaborn as sns
    from sklearn.preprocessing import LabelEncoder
    titanic = sns.load_dataset('titanic')
    X = titanic[['sex','age','fare','class',\
                 'embark_town','alone']].copy()
    X.shape
    ```

    创建特征矩阵作为数据集的副本，以避免每次通过预处理过程更新矩阵时收到警告消息。

    输出如下所示:

    ```
    (891, 6)
    ```

2.  Check for missing values in all the features. As we did previously, use `isnull()` to determine whether a value is missing and use `sum()` to sum up the occurrences of missing values along each feature:

    ```
    print("Sex: " + str(X['sex'].isnull().sum()))
    print("Age: " + str(X['age'].isnull().sum()))
    print("Fare: " + str(X['fare'].isnull().sum()))
    print("Class: " + str(X['class'].isnull().sum()))
    print("Embark town: " + str(X['embark_town'].isnull().sum()))
    print("Alone: " + str(X['alone'].isnull().sum()))
    ```

    输出将如下所示:

    ```
    Sex: 0
    Age: 177
    Fare: 0
    Class: 0
    Embark town: 2
    Alone: 0
    ```

    从前面的输出中可以看出，只有一个特性包含大量缺失值:`age`。因为它包含许多丢失的值，几乎占总数的 20%，所以应该替换这些值。将采用平均值插补方法，如以下代码所示:

    ```
    mean = X['age'].mean()
    mean =round(mean)
    X['age'].fillna(mean,inplace = True)
    ```

    接下来，发现数值特征中存在的异常值。让我们使用三个标准差作为度量来计算数字要素的最小和最大阈值:

    ```
    features = ["age", "fare"]
    for feature in features:
        min_ = X[feature].mean() - (3 * X[feature].std())
        max_ = X[feature].mean() + (3 * X[feature].std())
        X = X[X[feature] <= max_]
        X = X[X[feature] >= min_]
        print(feature,    ":", X.shape)
    ```

    输出如下所示:

    ```
    age: (884, 6)
    fare: (864, 6)
    ```

    年龄和费用特征的异常值总数分别为 7 和 20，将初始矩阵的形状减少了 27 个实例。

    接下来，使用`for`循环，发现文本特征中存在的异常值。`value_counts()`函数用于统计每个特征中类的出现次数:

    ```
    features = ["sex", "class", "embark_town", "alone"]
    for feature in features:
        count_ = X[feature].value_counts()
        print(feature)
        print(count_, "\n")
    ```

    输出如下所示:

    ![Figure 1.23: Count of occurrence of the classes in each feature
    ](img/B15781_01_23.jpg)

    图 1.23:每个特征中类的出现次数

    任何要素的类都不会被视为异常值，因为它们都占整个数据集的 5%以上。

3.  Convert all text features into their numeric representations. Use scikit-learn's `LabelEncoder` class, as shown in the following code:

    ```
    enc = LabelEncoder()
    X["sex"] = enc.fit_transform(X['sex'].astype('str'))
    X["class"] = enc.fit_transform(X['class'].astype('str'))
    X["embark_town"] = enc.fit_transform(X['embark_town'].\
                                         astype('str'))
    X["alone"] = enc.fit_transform(X['alone'].astype('str'))
    ```

    打印出功能矩阵的前五个实例，以查看转换结果:

    ```
    X.head()
    ```

    输出如下所示:

    ![Figure 1.24: A screenshot displaying the first five instances of the features matrix
    ](img/B15781_01_24.jpg)

    图 1.24:显示特征矩阵前五个实例的屏幕截图

4.  Rescale your data, either by normalizing or standardizing it.

    从以下代码中可以看出，所有要素都经过了规范化过程，但只有那些不符合规范化变量标准的要素才会被更改:

    ```
    X = (X - X.min()) / (X.max() - X.min())
    X.head(10)
    ```

    最终输出的前 10 行显示在下面的屏幕截图中:

    ![Figure 1.25: Displaying the first 10 instances of the normalized dataset
    ](img/B15781_01_25.jpg)

图 1.25:显示规范化数据集的前 10 个实例

注意

要访问该特定部分的源代码，请参考 https://packt.live/2MY1wld 的。

你也可以在 https://packt.live/3e2lyqt 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地对数据集执行了数据预处理，现在可以使用它来训练 ML 算法。

# 2。无监督学习——现实生活中的应用

## 活动 2.01:使用数据可视化来辅助预处理过程

解决方案:

1.  导入加载数据集所需的所有元素并对其进行预处理:

    ```
    import pandas as pd import matplotlib.pyplot as plt import numpy as np
    ```

2.  使用 pandas 的`read_csv()`功能加载之前下载的数据集。将数据集存储在名为`data` :

    ```
    data = pd.read_csv("wholesale_customers_data.csv")
    ```

    的 pandas 数据帧中
3.  Check for missing values in your DataFrame. Using the `isnull()` function plus the `sum()` function, count the missing values of the entire dataset at once:

    ```
    data.isnull().sum()
    ```

    输出如下所示:

    ```
    Channel             0
    Region              0
    Fresh               0
    Milk                0
    Grocery             0
    Frozen              0
    Detergents_Paper    0
    Delicassen          0
    dtype: int64
    ```

    正如您在前面的屏幕截图中看到的，数据集中没有丢失值。

4.  Check for outliers in your DataFrame. Mark as outliers all the values that are three standard deviations away from the mean.

    以下代码片段允许您一次在整个功能集中查找异常值。但是，另一种有效的方法是一次一个要素地检查异常值:

    ```
    outliers = {}
    for i in range(data.shape[1]):
        min_t = data[data.columns[i]].mean() \
                - (3 * data[data.columns[i]].std())
        max_t = data[data.columns[i]].mean() \
                + (3 * data[data.columns[i]].std())
        count = 0
        for j in data[data.columns[i]]:
            if j < min_t or j > max_t:
                count += 1
        outliers[data.columns[i]] = [count,data.shape[0]-count]
    print(outliers)
    ```

    每个要素的异常值计数如下:

    ```
    {'Channel': [0, 440], 'Region': [0, 440], 'Fresh': [7, 433], 'Milk': [9, 431], 'Grocery': [7, 433], 'Frozen': [6, 434], 'Detergents_Paper': [10, 430], 'Delicassen': [4, 436]}
    ```

    从前面的截图中可以看出，一些特性确实有异常值。考虑到每个特性只有几个异常值，有两种可能的方法来处理它们。

    首先，您可以决定删除离群值。可以通过显示带有异常值的要素的直方图来支持这一决策:

    ```
    plt.hist(data["Fresh"])
    plt.show()
    ```

    输出如下所示:

    ![Figure 2.14: An example histogram plot for the “Fresh” feature
    ](img/B15781_02_14.jpg)

    ```
    plt.figure(figsize=(8,8))
    plt.pie(outliers["Detergents_Paper"],autopct="%.2f")
    plt.show()
    ```

    输出如下所示:

    ![Figure 2.15: A pie chart showing the participation of outliers from the Detergents_papers feature in the dataset
    ](img/B15781_02_15.jpg)

    图 2.15:一个饼图显示了数据集中来自 Detergents_papers 特征的异常值的参与情况

    上图显示了来自`Detergents_papers`要素的异常值的参与情况，该要素是数据集中异常值最多的要素。只有 2.27%的值是异常值，这个值非常低，也不会影响模型的性能。

    对于本书中的解决方案，我们决定保留离群值，因为它们不太可能影响模型的性能。

5.  Rescale the data.

    对于此解决方案，使用了标准化公式。请注意，公式可以一次应用于整个数据集，而不是分别应用于每个要素:

    ```
    data_standardized = (data - data.mean())/data.std()
    data_standardized.head()
    ```

    输出如下所示:

    ![Figure 2.16: Rescaled data
    ](img/B15781_02_16.jpg)

图 2.16:重新调整的数据

注意

要访问该特定部分的源代码，请参考[https://packt.live/2Y3ooGh](https://packt.live/2Y3ooGh)。

你也可以在[https://packt.live/2B8vKPI](https://packt.live/2B8vKPI)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功地预处理了批发客户数据集，该数据集将在后续活动中用于构建一个模型，该模型将这些观察结果分类到聚类中。

## 活动 2.02:将 k-means 算法应用于数据集

解决方案:

1.  Open the Jupyter Notebook that you used for the previous activity. There, you should have imported all the required libraries and performed the necessary steps to pre-process the dataset.

    标准化数据应该如下所示:

    ![Figure 2.17: A screenshot displaying the first five instances of the standardized dataset
    ](img/B15781_02_17.jpg)

    图 2.17:显示标准化数据集的前五个实例的屏幕截图

2.  Calculate the average distance of data points from its centroid in relation to the number of clusters. Based on this distance, select the appropriate number of clusters to train the model on.

    首先，导入算法类:

    ```
    from sklearn.cluster import KMeans
    ```

    接下来，使用以下代码片段中的代码，根据创建的聚类数计算数据点到质心的平均距离:

    ```
    ideal_k = []
    for i in range(1,21):
        est_kmeans = KMeans(n_clusters=i, random_state=0)
        est_kmeans.fit(data_standardized)
        ideal_k.append([i,est_kmeans.inertia_])
    ideal_k = np.array(ideal_k)
    ```

    最后，绘制关系以找到线的断点，并选择聚类数:

    ```
    plt.plot(ideal_k[:,0],ideal_k[:,1])
    plt.show()
    ```

    输出如下所示:

    ![Figure 2.18: The output of the plot function used
    ](img/B15781_02_18.jpg)

    图 2.18:所用绘图函数的输出

    同样， *x 轴*表示聚类的数量，而 *y 轴*指的是聚类中的数据点到其质心的计算平均距离。

3.  Train the model and assign a cluster to each data point in your dataset. Plot the results.

    若要定型模型，请使用以下代码:

    ```
    est_kmeans = KMeans(n_clusters=6, random_state = 0)
    est_kmeans.fit(data_standardized)
    pred_kmeans = est_kmeans.predict(data_standardized)
    ```

    选择的簇数为`6`；但是，由于没有确切的断点，5 到 10 之间的值也是可以接受的。

    最后，绘制聚类过程的结果。由于数据集包含八个不同的要素，因此选择两个要素同时进行绘制，如以下代码所示:

    ```
    plt.subplots(1, 2, sharex='col', \
                 sharey='row', figsize=(16,8))
    plt.scatter(data.iloc[:,5], data.iloc[:,3], \
                c=pred_kmeans, s=20)
    plt.xlim([0, 20000])
    plt.ylim([0, 20000])
    plt.xlabel('Frozen')
    plt.subplot(1, 2, 1)
    plt.scatter(data.iloc[:,4], data.iloc[:,3], \
                c=pred_kmeans, s=20)
    plt.xlim([0, 20000])
    plt.ylim([0,20000])
    plt.xlabel('Grocery')
    plt.ylabel('Milk')
    plt.show()
    ```

    输出如下所示:

    ![Figure 2.19: Two example plots obtained after the clustering process
    ](img/B15781_02_19.jpg)

图 2.19:聚类过程后获得的两个示例图

注意

要访问此活动的源代码，请参考[https://packt.live/3fhgO0y](https://packt.live/3fhgO0y)。

你也可以在[https://packt.live/3eeEOB6](https://packt.live/3eeEOB6)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

来自`matplotlib`的`subplots()`函数被用于一次绘制两个散点图。对于每个图形，轴代表相对于另一个特性的值的所选特性的值。从图中可以看出，由于我们只能使用数据集中八个特征中的两个，因此没有明显的视觉关系。然而，模型的最终输出创建了六个不同的集群，代表六个不同的客户端配置文件。

## 活动 2.03:将均值漂移算法应用于数据集

解决方案:

1.  打开您在之前的活动中使用的 Jupyter 笔记本。
2.  Train the model and assign a cluster to each data point in your dataset. Plot the results.

    首先，导入算法类:

    ```
    from sklearn.cluster import MeanShift
    ```

    若要定型模型，请使用以下代码:

    ```
    est_meanshift = MeanShift(0.4)
    est_meanshift.fit(data_standardized)
    pred_meanshift = est_meanshift.predict(data_standardized)
    ```

    使用`0.4`的带宽训练该模型。但是，可以随意测试其他值，看看结果如何变化。

    最后，绘制聚类过程的结果。由于数据集包含八个不同的要素，因此选择两个要素同时进行绘制，如以下代码片段所示。与上一个练习类似，由于只能从八个特征中画出两个，所以看不到聚类之间的分隔:

    ```
    plt.subplots(1, 2, sharex='col', \
                 sharey='row', figsize=(16,8))
    plt.scatter(data.iloc[:,5], data.iloc[:,3], \
                c=pred_meanshift, s=20)
    plt.xlim([0, 20000])
    plt.ylim([0,20000])
    plt.xlabel('Frozen')
    plt.subplot(1, 2, 1)
    plt.scatter(data.iloc[:,4], data.iloc[:,3], \
                c=pred_meanshift, s=20)
    plt.xlim([0, 20000])
    plt.ylim([0,20000])
    plt.xlabel('Grocery')
    plt.ylabel('Milk')
    plt.show()
    ```

    输出如下所示:

    ![Figure 2.20: Example plots obtained at the end of the process
    ](img/B15781_02_20.jpg)

图 2.20:流程结束时获得的示例图

对于每个图，轴代表所选特性的值，而不是另一个特性的值。

注意

要访问此活动的源代码，请参考[https://packt.live/3fviVy1](https://packt.live/3fviVy1)。

你也可以在 https://packt.live/2Y1aqEF 的[在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。](https://packt.live/2Y1aqEF)

您已经成功地对批发客户数据集应用了均值漂移算法。稍后，您将能够比较同一数据集上不同算法的结果，以选择性能最佳的算法。

## 活动 2.04:将 DBSCAN 算法应用于数据集

解决方案:

1.  打开您在之前的活动中使用的 Jupyter 笔记本。
2.  Train the model and assign a cluster to each data point in your dataset. Plot the results.

    首先，导入算法类:

    ```
    from sklearn.cluster import DBSCAN
    ```

    若要定型模型，请使用以下代码:

    ```
    est_dbscan = DBSCAN(eps=0.8)
    pred_dbscan = est_dbscan.fit_predict(data_standardized)
    ```

    使用ε值`0.8`训练该模型。但是，可以随意测试其他值，看看结果如何变化。

    最后，绘制聚类过程的结果。由于数据集包含八个不同的要素，因此选择两个要素同时进行绘制，如以下代码所示:

    ```
    plt.subplots(1, 2, sharex='col', \
                 sharey='row', figsize=(16,8))
    plt.scatter(data.iloc[:,5], data.iloc[:,3], \
                c=pred_dbscan, s=20)
    plt.xlim([0, 20000])
    plt.ylim([0,20000])
    plt.xlabel('Frozen')
    plt.subplot(1, 2, 1)
    plt.scatter(data.iloc[:,4], data.iloc[:,3], \
                c=pred_dbscan, s=20)
    plt.xlim([0, 20000])
    plt.ylim([0,20000])
    plt.xlabel('Grocery')
    plt.ylabel('Milk')
    plt.show()
    ```

    输出如下所示:

    ![Figure 2.21: Example plots obtained at the end of the clustering process
    ](img/B15781_02_21.jpg)

图 2.21:聚类过程结束时获得的示例图

注意

要访问此活动的源代码，请参考[https://packt.live/2YCFvh8](https://packt.live/2YCFvh8)。

你也可以在 https://packt.live/2MZgnvC 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

与上一个练习类似，由于一次只能从八个要素中画出两个，所以看不到聚类之间的分隔。

## 活动 2.05:测量和比较算法的性能

解决方案:

1.  打开您在之前的活动中使用的 Jupyter 笔记本。
2.  Calculate both the Silhouette Coefficient score and the Calinski–Harabasz index for all the models that you trained previously.

    首先，导入指标:

    ```
    from sklearn.metrics import silhouette_score
    from sklearn.metrics import calinski_harabasz_score
    ```

    计算所有算法的轮廓系数得分，如以下代码所示:

    ```
    kmeans_score = silhouette_score(data_standardized, \
                                    pred_kmeans, \
                                    metric='euclidean')
    meanshift_score = silhouette_score(data_standardized, \
                                       pred_meanshift, \
                                       metric='euclidean')
    dbscan_score = silhouette_score(data_standardized, \
                                    pred_dbscan, \
                                    metric='euclidean')
    print(kmeans_score, meanshift_score, dbscan_score)
    ```

    k-means、mean-shift 和 DBSCAN 算法的得分分别约为`0.3515`、`0.0933`和`0.1685`。

    最后，计算所有算法的卡林斯基-哈拉巴斯指数。以下是这方面的代码片段:

    ```
    kmeans_score = calinski_harabasz_score(data_standardized, \
                                           pred_kmeans)
    meanshift_score = calinski_harabasz_score(data_standardized, \
                                              pred_meanshift)
    dbscan_score = calinski_harabasz_score(data_standardized, \
                                           pred_dbscan)
    print(kmeans_score, meanshift_score, dbscan_score)
    ```

    按照前面代码片段中给出的顺序，三种算法的得分大约为`145.73`、`112.90`和`42.45`。

    注意

    要访问此活动的源代码，请参考[https://packt.live/2Y2xHWR](https://packt.live/2Y2xHWR)。

    你也可以在 https://packt.live/3hszegy 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

通过快速查看我们从这两个指标获得的结果，可以得出这样的结论:k-means 算法优于其他模型，因此应该选择它来解决数据问题。

# 3。监督学习–关键步骤

## 活动 3.01:手写数字数据集的数据划分

解决方案:

1.  导入分割数据集所需的所有元素，以及 scikit-learn 中的`load_digits`函数来加载`digits`数据集。使用下面的代码来做到这一点:

    ```
    from sklearn.datasets import load_digits import pandas as pd from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold
    ```

2.  Load the `digits` dataset and create Pandas DataFrames containing the features and target matrices:

    ```
    digits = load_digits()
    X = pd.DataFrame(digits.data)
    Y = pd.DataFrame(digits.target)
    print(X.shape, Y.shape)
    ```

    特征和目标矩阵的形状应分别如下:

    ```
    (1797, 64) (1797, 1)
    ```

3.  Perform the conventional split approach, using a split ratio of 60/20/20%.

    使用`train_test_split`功能，将数据分为初始训练集和测试集:

    ```
    X_new, X_test, \
    Y_new, Y_test = train_test_split(X, Y, test_size=0.2)
    print(X_new.shape, Y_new.shape, X_test.shape, Y_test.shape)
    ```

    您创建的集合的形状应该如下所示:

    ```
    (1437, 64) (1437, 1) (360, 64) (360, 1)
    ```

    接下来，计算`test_size`的值，它将开发集的大小设置为等于之前创建的测试集的大小:

    ```
    dev_size = X_test.shape[0]/X_new.shape[0]
    print(dev_size)
    ```

    前面操作的结果是`0.2505`。

    最后，将`X_new`和`Y_new`分成最终的列车和开发组。请使用以下代码来完成此操作:

    ```
    X_train, X_dev, \
    Y_train, Y_dev = train_test_split(X_new, Y_new, \
                                      test_size = dev_size)
    print(X_train.shape, Y_train.shape, X_dev.shape, \
          Y_dev.shape, X_test.shape, Y_test.shape)
    ```

    上述代码片段的输出如下:

    ```
    (1077, 64) (1077, 1) (360, 64) (360, 1) (360, 64) (360, 1)
    ```

4.  Using the same DataFrames, perform a 10-fold cross-validation split.

    首先，将数据集分为初始训练集和测试集:

    ```
    X_new_2, X_test_2, \
    Y_new_2, Y_test_2 = train_test_split(X, Y, test_size=0.1)
    ```

    使用`KFold`类，执行 10 倍分割:

    ```
    kf = KFold(n_splits = 10)
    splits = kf.split(X_new_2)
    ```

    请记住，交叉验证执行不同的分割配置，每次都混洗数据。考虑到这一点，执行一个`for`循环，该循环将遍历所有分割配置:

    ```
    for train_index, dev_index in splits:
        X_train_2, X_dev_2 = X_new_2.iloc[train_index,:], \
                             X_new_2.iloc[dev_index,:]
        Y_train_2, Y_dev_2 = Y_new_2.iloc[train_index,:], \
                             Y_new_2.iloc[dev_index,:]
    ```

    负责训练和评估模型的代码应在`for`循环体内，以便训练和评估具有各种分割配置的模型:

    ```
    print(X_train_2.shape, Y_train_2.shape, X_dev_2.shape, \
          Y_dev_2.shape, X_test_2.shape, Y_test_2.shape)
    ```

    按照前面的代码片段，通过打印所有子集的形状，输出如下:

    ```
    (1456, 64) (1456, 1) (161, 64) (161, 1) (180, 64) (180, 1)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/37xatv3](https://packt.live/37xatv3)。

    你也可以在[https://packt.live/2Y2nolS](https://packt.live/2Y2nolS)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经使用传统的拆分方法和交叉验证方法成功地拆分了数据集。这些集合现在可以用来训练在看不见的数据上表现良好的优秀模型。

## 活动 3.02:评估在手写数据集上训练的模型的性能

解决方案:

1.  导入加载和分割数据集所需的所有元素，以便训练模型并评估分类任务的性能:

    ```
    from sklearn.datasets import load_digits import pandas as pd from sklearn.model_selection import train_test_split from sklearn import tree from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score from sklearn.metrics import precision_score from sklearn.metrics import recall_score
    ```

2.  从 scikit-learn 加载`digits`玩具数据集，并创建包含特征和目标矩阵的熊猫数据帧:

    ```
    digits = load_digits() X = pd.DataFrame(digits.data) Y = pd.DataFrame(digits.target)
    ```

3.  将数据分成训练集和测试集。使用 20%作为测试集的大小:

    ```
    X_train, X_test, \ Y_train, Y_test = train_test_split(X,Y, test_size = 0.2,\                                    random_state = 0)
    ```

4.  在训练集上训练决策树。然后，使用模型预测测试集上的类标签(提示:训练决策树，重温*练习 3.04* ，*计算一个分类任务上不同的评价度量* ):

    ```
    model = tree.DecisionTreeClassifier(random_state = 0) model = model.fit(X_train, Y_train) Y_pred = model.predict(X_test)
    ```

5.  Use scikit-learn to construct a confusion matrix:

    ```
    confusion_matrix(Y_test, Y_pred)
    ```

    混淆矩阵的输出如下:

    ![Figure 3.14: Output of the confusion matrix
    ](img/B15781_03_14.jpg)

    图 3.14:混淆矩阵的输出

6.  Calculate the accuracy of the model:

    ```
    accuracy = accuracy_score(Y_test, Y_pred)
    print("accuracy:", accuracy)
    ```

    精度等于`84.72` %。

7.  Calculate the precision and recall. Considering that both the precision and recall can only be calculated on binary data, we'll assume that we are only interested in classifying instances as number 6 or any other number:

    ```
    Y_test_2 = Y_test[:]
    Y_test_2[Y_test_2 != 6] = 1
    Y_test_2[Y_test_2 == 6] = 0
    Y_pred_2 = Y_pred
    Y_pred_2[Y_pred_2 != 6] = 1
    Y_pred_2[Y_pred_2 == 6] = 0
    precision = precision_score(Y_test_2, Y_pred_2)
    print("precision:", precision)
    recall = recall_score(Y_test_2, Y_pred_2)
    print("recall:", recall)
    ```

    上述代码片段的输出如下:

    ```
    precision: 0.9841269841269841
    recall: 0.9810126582278481
    ```

    据此，准确率和召回率得分应该分别等于`98.41` %和`98.10` %。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2UJMFPC](https://packt.live/2UJMFPC)。

    你也可以在 https://packt.live/2zwqkgX 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

您已经成功测量了分类任务的性能。

## 活动 3.03:对训练识别手写数字的模型进行错误分析

解决方案:

1.  导入所需的元素以加载和拆分数据集。我们将这样做来训练模型并测量其准确性:

    ```
    from sklearn.datasets import load_digits import pandas as pd from sklearn.model_selection import train_test_split import numpy as np from sklearn import tree from sklearn.metrics import accuracy_score
    ```

2.  从 scikit-learn 加载`digits`玩具数据集，并创建包含特征和目标矩阵的熊猫数据帧:

    ```
    digits = load_digits() X = pd.DataFrame(digits.data) Y = pd.DataFrame(digits.target)
    ```

3.  Split the data into training, validation, and testing sets. Use `0.1` as the size of the test set, and an equivalent number to build a validation set of the same shape:

    ```
    X_new, X_test, \
    Y_new, Y_test = train_test_split(X, Y, test_size = 0.1,\
                                     random_state = 101)
    test_size = X_test.shape[0] / X_new.shape[0]
    X_train, X_dev, \
    Y_train, Y_dev = train_test_split(X_new, Y_new, \
                                      test_size= test_size, \
                                      random_state = 101)
    print(X_train.shape, Y_train.shape, X_dev.shape, \
          Y_dev.shape, X_test.shape, Y_test.shape)
    ```

    生成的形状如下:

    ```
    (1437, 64) (1437, 1) (180, 64) (180, 1) (180, 64) (180, 1)
    ```

4.  Create a train/dev set for both the features and the target values that contains `90` instances/labels of the train set and `90` instances/labels of the dev set:

    ```
    np.random.seed(101)
    indices_train = np.random.randint(0, len(X_train), 90)
    indices_dev = np.random.randint(0, len(X_dev), 90)
    X_train_dev = pd.concat([X_train.iloc[indices_train,:], \
                             X_dev.iloc[indices_dev,:]])
    Y_train_dev = pd.concat([Y_train.iloc[indices_train,:], \
                             Y_dev.iloc[indices_dev,:]])
    print(X_train_dev.shape, Y_train_dev.shape)
    ```

    生成的形状如下:

    ```
    (180, 64) (180, 1)
    ```

5.  在训练集数据上训练决策树:

    ```
    model = tree.DecisionTreeClassifier(random_state = 101) model = model.fit(X_train, Y_train)
    ```

6.  Calculate the error rate for all sets of data and determine which condition is affecting the performance of the model:

    ```
    sets = ["Training", "Train/dev", "Validation", "Testing"]
    X_sets = [X_train, X_train_dev, X_dev, X_test]
    Y_sets = [Y_train, Y_train_dev, Y_dev, Y_test]
    scores = {}
    for i in range(0, len(X_sets)):
        pred = model.predict(X_sets[i])
        score = accuracy_score(Y_sets[i], pred)
        scores[sets[i]] = score
    print(scores)
    ```

    输出如下所示:

    ```
    {'Training': 1.0, 'Train/dev': 0.9444444444444444, 'Validation': 0.8833333333333333, 'Testing': 0.8833333333333333}
    ```

    错误率见下表:

    ![Figure 3.15: Error rates of the Handwritten Digits model
    ](img/B15781_03_15.jpg)

图 3.15:手写数字模型的错误率

从前面的结果可以得出结论，该模型同样遭受方差和数据不匹配。

注意

要访问该特定部分的源代码，请参考[https://packt.live/3d0c4uM](https://packt.live/3d0c4uM)。

你也可以在[https://packt.live/3eeFlTC](https://packt.live/3eeFlTC)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

现在，您已经成功地执行了错误分析，确定了改进模型性能的措施。

# 4。监督学习算法:预测年收入

## 活动 4.01:为我们的人口普查收入数据集训练一个朴素贝叶斯模型

解决方案:

1.  在 Jupyter 笔记本中，导入所有必需的元素来加载和拆分数据集，以及训练一个朴素贝叶斯算法:

    ```
    import pandas as pd from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB
    ```

2.  Load the pre-processed Census Income dataset. Next, separate the features from the target by creating two variables, `X` and `Y`:

    ```
    data = pd.read_csv("census_income_dataset_preprocessed.csv")
    X = data.drop("target", axis=1)
    Y = data["target"]
    ```

    注意有几种方法可以实现`X`和`Y`的分离。用你觉得最舒服的那个。但是，要考虑到`X`应该包含所有实例的特性，而`Y`应该包含所有实例的类标签。

3.  Divide the dataset into training, validation, and testing sets, using a split ratio of 10%:

    ```
    X_new, X_test, \
    Y_new, Y_test = train_test_split(X, Y, test_size=0.1, \
                                     random_state=101)
    test_size = X_test.shape[0] / X_new.shape[0]
    X_train, X_dev, \
    Y_train, Y_dev = train_test_split(X_new, Y_new, \
                                      test_size=test_size, \
                                      random_state=101)
    print(X_train.shape, Y_train.shape, X_dev.shape, \
          Y_dev.shape, X_test.shape, Y_test.shape)
    ```

    最终形状将如下所示:

    ```
    (26047, 9) (26047,) (3257, 9) (3257,) (3257, 9) (3257,)
    ```

4.  使用`fit`方法在训练集上训练一个朴素贝叶斯模型(`X_train`和`Y_train` ):

    ```
    model_NB = GaussianNB() model_NB.fit(X_train,Y_train)
    ```

5.  Finally, perform a prediction using the model that you trained previously for a new instance with the following values for each feature – `39`, `6`, `13`, `4`, `0`, `2174`, `0`, `40`, `38`:

    ```
    pred_1 = model_NB.predict([[39,6,13,4,0,2174,0,40,38]])
    print(pred_1)
    ```

    预测的输出如下:

    ```
    [0]
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3ht1TCs](https://packt.live/3ht1TCs)。

    你也可以在 https://packt.live/2zwqxkf 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

这意味着个人的收入少于或等于 50K，考虑到 0 是工资少于或等于 50K 的个人的标签。

## 活动 4.02:为我们的人口普查收入数据集训练决策树模型

解决方案:

1.  打开您在之前活动中使用的 Jupyter 笔记本，从 scikit-learn:

    ```
    from sklearn.tree import DecisionTreeClassifier
    ```

    导入决策树算法
2.  在 scikit-learn 的`DecisionTreeClassifier`类上使用`fit`方法训练模型。为了训练模型，使用来自先前活动的训练集数据(`X_train`和`Y_train` ):

    ```
    model_tree = DecisionTreeClassifier(random_state=101) model_tree.fit(X_train,Y_train)
    ```

3.  Finally, perform a prediction using the model that you trained before for a new instance with the following values for each feature – `39`, `6`, `13`, `4`, `0`, `2174`, `0`, `40`, `38`:

    ```
    pred_2 = model_tree.predict([[39,6,13,4,0,2174,0,40,38]])
    print(pred_2)
    ```

    上述代码片段的输出如下:

    ```
    [0]
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2zxQIqV](https://packt.live/2zxQIqV)。

    你也可以在 https://packt.live/2AC7iWX 在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

这意味着主体的收入低于或等于 50K。

## 活动 4.03:为人口普查收入数据集训练一个 SVM 模型

解决方案:

1.  打开您在之前的练习中使用的 Jupyter 笔记本，从 scikit-learn:

    ```
    from sklearn.svm import SVC
    ```

    导入 SVM 算法
2.  使用 scikit-learn 的`SVC`类上的`fit`方法训练模型。为了训练模型，使用来自先前活动的训练集数据(`X_train`和`Y_train` ):

    ```
    model_svm = SVC() model_svm.fit(X_train, Y_train)
    ```

3.  Finally, perform a prediction using the model that you trained before for a new instance with the following values for each feature – `39`, `6`, `13`, `4`, `0`, `2174`, `0`, `40`, `38`:

    ```
    pred_3 = model_svm.predict([[39,6,13,4,0,2174,0,40,38]])
    print(pred_3)
    ```

    输出如下所示:

    ```
    [0]
    ```

    对个人的预测等于零，这意味着个人的收入低于或等于`50K`。

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2Nb6J9z](https://packt.live/2Nb6J9z)。

    你也可以在[https://packt.live/3hbpCGm](https://packt.live/3hbpCGm)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

# 5。人工神经网络:预测年收入

## 活动 5.01:为人口普查收入数据集训练 MLP

解决方案:

1.  导入加载和分割数据集、训练 MLP 以及测量准确度所需的所有元素:

    ```
    import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score
    ```

2.  Using the preprocessed Census Income Dataset, separate the features from the target, creating the variables `X` and `Y`:

    ```
    data = pd.read_csv("census_income_dataset_preprocessed.csv")
    X = data.drop("target", axis=1)
    Y = data["target"]
    ```

    如前所述，有几种方法可以实现`X`和`Y`的分离，主要考虑的是`X`应该包含所有实例的特性，而`Y`应该包含所有实例的类标签。

3.  Divide the dataset into training, validation, and testing sets, using a split ratio of 10%:

    ```
    X_new, X_test, \
    Y_new, Y_test = train_test_split(X, Y, test_size=0.1, \
                                     random_state=101)
    test_size = X_test.shape[0] / X_new.shape[0]
    X_train, X_dev, \
    Y_train, Y_dev = train_test_split(X_new, Y_new, \
                                      test_size=test_size, \
                                      random_state=101)
    print(X_train.shape, X_dev.shape, X_test.shape, \
          Y_train.shape, Y_dev.shape, Y_test.shape)
    ```

    创建的集合的形状应该如下所示:

    ```
    (26047, 9) (3257, 9) (3257, 9) (26047,) (3257,) (3257,)
    ```

4.  从 scikit-learn 实例化`MLPClassifier`类，并用训练数据训练模型。将超参数保留为默认值。同样，使用一个等于`101` :

    ```
    model = MLPClassifier(random_state=101) model = model.fit(X_train, Y_train)
    ```

    的`random_state`
5.  Calculate the accuracy of the model for all three sets (training, validation, and testing):

    ```
    sets = ["Training", "Validation", "Testing"]
    X_sets = [X_train, X_dev, X_test]
    Y_sets = [Y_train, Y_dev, Y_test]
    accuracy = {}
    for i in range(0,len(X_sets)):
        pred = model.predict(X_sets[i])
        score = accuracy_score(Y_sets[i], pred)
        accuracy[sets[i]] = score
    print(accuracy)
    ```

    三个集合的准确度分数应该如下:

    ```
    {'Training': 0.8465909090909091, 'Validation': 0.8246314496314496, 'Testing': 0.8415719987718759}
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3hneWFr](https://packt.live/3hneWFr)。

    本节目前没有在线交互示例，需要在本地运行。

您已经成功训练了一个 MLP 模型来解决现实生活中的数据问题。

## 活动 5.02:比较不同的模型，选择最适合人口普查收入数据问题的模型

解决方案:

1.  打开您用来训练模型的 Jupyter 笔记本。
2.  Compare the four models, based only on their accuracy scores.

    通过获取前一章中模型的准确性分数，以及本章中训练的模型的准确性，可以执行最终比较，以选择最能解决数据问题的模型。为此，下表显示了所有四种模型的准确度分数:

    ![Figure 5.15: Accuracy scores of all four models for the Census Income Dataset
    ](img/B15781_05_15.jpg)

    图 5.15:人口普查收入数据集的所有四个模型的准确度得分

3.  On the basis of the accuracy scores, identify the model that best solves the data problem.

    要确定最能解决数据问题的模型，首先要比较训练集的准确率。由此可以得出结论，决策树模型更适合数据问题。尽管如此，在验证和测试集上的性能低于使用 MLP 获得的性能，这表明决策树模型中存在高方差。

    因此，一个好的方法是通过简化模型来解决决策树模型的高方差。这可以通过添加修剪参数来实现，该参数“修剪”树的叶子以简化树，并忽略树的一些细节，以便将模型推广到数据。理想情况下，该模型应该能够对所有三组数据达到相似的精度水平，这将使其成为解决数据问题的最佳模型。

    但是，如果模型无法克服高方差，并且假设所有模型都已经过微调以实现可能的最大性能，则考虑到 MLP 在测试集中的性能最佳，应该选择它。这主要是因为模型在测试集上的性能定义了它在看不见的数据上的整体性能，这意味着从长远来看，具有更高测试集性能的模型将更有用。

# 6。构建您自己的程序

## 活动 6.01:执行银行营销数据集的准备和创建阶段

解决方案:

注意

为了确保在[https://packt.live/2RpIhn9](https://packt.live/2RpIhn9)获得的结果的可再现性，请确保在分割数据集时使用`0`的`random_state`，在训练模型时使用`2`的`random_state`。

1.  打开一个 Jupyter 笔记本，输入所有需要的元素:

    ```
    import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.neural_network import MLPClassifier from sklearn.metrics import precision_score
    ```

2.  Load the dataset into the notebook. Make sure that you load the one that was edited previously, named `bank-full-dataset.csv`, which is also available at [https://packt.live/2wnJyny](https://packt.live/2wnJyny):

    ```
    data = pd.read_csv("bank-full-dataset.csv")
    data.head(10)
    ```

    输出如下所示:

    ![Figure 6.8: A screenshot showing the first 10 instances of the dataset
    ](img/B15781_06_08.jpg)

    图 6.8:显示数据集前 10 个实例的屏幕截图

    如前所述，缺失值显示为`NaN`。

3.  Select the metric that's the most appropriate for measuring the performance of the model, considering that the purpose of the study is to detect clients who would subscribe to the term deposit.

    评估模型性能的指标是**精度**指标，因为它将正确分类的阳性标签与预测为阳性的实例总数进行比较。

4.  Pre-process the dataset.

    处理缺失值的过程是按照我们在第一章、*Scikit-Learn 简介*中了解到的概念来处理的，这些概念在本书中已经得到了应用。使用以下代码检查缺少的值:

    ```
    data.isnull().sum()
    ```

    根据结果，您将观察到只有四个特征包含缺失值:`job` (288)、`education` (1，857)、`contact` (13，020)和`poutcome` (36，959)。

    考虑到丢失的值不到整个数据的 5%,前两个特征可以不处理。另一方面，28.8%的值在`contact`功能中缺失，考虑到该功能指的是联系方式，而联系方式被认为与确定一个人是否会订阅新产品无关，因此从研究中删除该功能是安全的。最后，`poutcome`特征缺少其值的 81.7%，这就是为什么该特征也被从研究中移除。

    使用下面的代码，前面的两个功能将被删除:

    ```
    data = data.drop(["contact", "poutcome"], axis=1)
    ```

    正如我们在*第 1 章*、*sci kit-Learn 简介*中所解释的，并且贯穿本书，将分类特征转换成它们的数字形式的过程如下。

    对于所有标称特征，使用以下代码:

    ```
    enc = LabelEncoder()
    features_to_convert=["job","marital","default",\
                         "housing","loan","month","y"]
    for i in features_to_convert:
        data[i] = enc.fit_transform(data[i].astype('str'))
    ```

    如前几章所述，前面的代码将所有定性特征转换成数字形式。

    接下来，为了处理序数特征，我们必须使用下面的代码，正如在*步骤 4* 中提到的:

    ```
    data['education'] = data['education'].fillna('unknown')
    encoder = ['unknown','primary','secondary','tertiary']
    for i, word in enumerate(encoder):
        data['education'] = data['education'].astype('str').\
                            str.replace(word, str(i))
    data['education'] = data['education'].astype('int64')
    data.head()
    ```

    这里，第一行将`NaN`值转换成单词`unknown`，而第二行设置特性中值的顺序。接下来，使用一个`for`循环将每个单词替换为一个遵循顺序的数字。对于前面的例子，将使用`0`替换单词`unknown`，然后使用`1`替换`primary`，以此类推。最后，整个列被转换成整数类型，因为`replace`函数将数字写成字符串。

    如果我们显示结果数据帧的头部，输出如下:

    ![Figure 6.9: A screenshot showing the first five instances of the dataset after converting the categorical features into numerical ones
    ](img/B15781_06_09.jpg)

    ```
    outliers = {}
    for i in range(data.shape[1]):
        min_t = data[data.columns[i]].mean() \
                - (3 * data[data.columns[i]].std())
        max_t = data[data.columns[i]].mean() \
                + (3 * data[data.columns[i]].std())
        count = 0
        for j in data[data.columns[i]]:
            if j < min_t or j > max_t:
                count += 1
        outliers[data.columns[i]] = [count, data.shape[0]]
    print(outliers)
    ```

    如果我们打印结果字典，我们会得到以下输出:

    ```
    {'age': [381, 45211], 'job': [0, 45211], 'marital': [0, 45211], 'education': [0, 45211], 'default': [815, 45211], 'balance': [745, 45211], 'housing': [0, 45211], 'loan': [0, 45211], 'day': [0, 45211], 'month': [0, 45211], 'duration': [963, 45211], 'campaign': [840, 45211], 'pdays': [1723, 45211], 'previous': [582, 45211], 'y': [0, 45211]}
    ```

    正如我们所见，异常值在每个要素中所占的比例不超过总值的 5%，这就是为什么它们可以不处理的原因。

    这可以通过采用具有最多异常值的特征(`pdays`)并将异常值的数量除以实例的总数(1，723 除以 45，211)来验证。该操作的结果是 0.038，相当于 3.8%。这意味着该特征只有 3.8%的异常值。

5.  Separate the features from the class label and split the dataset into three sets (training, validation, and testing).

    要将特性与目标值分开，请使用以下代码:

    ```
    X = data.drop("y", axis = 1)
    Y = data["y"]
    ```

    接下来，要执行 60/20/20 分割，请使用以下代码:

    ```
    X_new, X_test, \
    Y_new, Y_test = train_test_split(X, Y, test_size=0.2,\
                                     random_state = 0)
    test_size = X_test.shape[0] / X_new.shape[0]
    X_train, X_dev, \
    Y_train, Y_dev = train_test_split(X_new, Y_new, \
                                      test_size=test_size,\
                                      random_state = 0)
    print(X_train.shape, Y_train.shape, X_dev.shape, \
        Y_dev.shape, X_test.shape, Y_test.shape)
    ```

    如果我们打印所有子集的形状，输出如下:

    ```
    (27125, 14) (27125,) (9043, 14) (9043,) (9043, 14) (9043,)
    ```

6.  Use the decision tree algorithm on the dataset and train the model:

    ```
    model_tree = DecisionTreeClassifier(random_state = 2)
    model_tree.fit(X_train, Y_train)
    ```

    注意

    提醒一下，调用`fit`方法的输出包含了当前被训练的模型及其接受的所有参数。

7.  对数据集使用多层感知器算法并训练模型。为了重温这一点，进入第 5 章、*人工神经网络:预测年收入* :

    ```
    model_NN = MLPClassifier(random_state = 2) model_NN.fit(X_train, Y_train)
    ```

8.  Evaluate both models by using the metric that was selected previously.

    使用以下代码，可以测量决策树模型的精度分数:

    ```
    X_sets = [X_train, X_dev, X_test]
    Y_sets = [Y_train, Y_dev, Y_test]
    precision = []
    for i in range(0, len(X_sets)):
        pred = model_tree.predict(X_sets[i])
        score = precision_score(Y_sets[i], pred)
        precision.append(score)
    print(precision)
    ```

    如果我们打印包含决策树模型的每个集合的精度分数的列表，输出如下:

    ```
    [1.0, 0.43909348441926344, 0.4208059981255858]
    ```

    可以修改相同的代码来计算多层感知器的分数:

    ```
    X_sets = [X_train, X_dev, X_test]
    Y_sets = [Y_train, Y_dev, Y_test]
    precision = []
    for i in range(0, len(X_sets)):
        pred = model_NN.predict(X_sets[i])
        score = precision_score(Y_sets[i], pred)
        precision.append(score)
    print(precision)
    ```

    如果我们打印包含多层感知器模型的每个集合的精度分数的列表，输出如下:

    ```
    [0.35577647236029525, 0.35199283475145543, 0.3470483005366726]
    ```

    下表显示了两个模型的所有数据子集的精度得分:

    ![Figure 6.10: Precision scores for both models
    ](img/B15781_06_10.jpg)

    图 6.10:两种模型的精度分数

9.  Fine-tune some of the hyperparameters to fix the issues that were detected during the evaluation of the model by performing error analysis.

    虽然决策树在训练集上的精度是完美的，但是在将其与其他两个集的结果进行比较时，可以得出结论，该模型存在较高的方差。

    另一方面，多层感知器在所有三个集合上具有相似的性能，但是整体性能较低，这意味着该模型更可能遭受高偏差。

    考虑到这一点，对于决策树模型，为了简化模型，需要在叶节点的最小样本数和树的最大深度都被改变。另一方面，对于多层感知器，改变迭代次数、隐藏层数、每层中的单元数和优化容限。

    以下代码显示了用于决策树算法的超参数的最终值，考虑到要达到这些值需要尝试不同的值:

    ```
    model_tree = DecisionTreeClassifier(random_state = 2, \
                                        min_samples_leaf=100, \
                                        max_depth=100)
    model_tree.fit(X_train, Y_train)
    ```

    以下代码片段显示了用于多层感知器算法的超参数的最终值:

    ```
    model_NN = \
        MLPClassifier(random_state = 2, max_iter=1000,\
                      hidden_layer_sizes = [100,100,50,25,25], \
                      tol=1e-4)
    model_NN.fit(X_train, Y_train)
    ```

    注意

    提醒一下，调用`fit`方法的输出包含了当前被训练的模型及其接受的所有参数。

10.  Compare the final versions of your models and select the one that you consider best fits the data.

    使用与前面步骤相同的代码，可以计算不同数据集上决策树模型的精度:

    ```
    X_sets = [X_train, X_dev, X_test]
    Y_sets = [Y_train, Y_dev, Y_test]
    precision = []
    for i in range(0, len(X_sets)):
        pred = model_tree.predict(X_sets[i])
        score = precision_score(Y_sets[i], pred)
        precision.append(score)
    print(precision)
    ```

    输出列表应该如下所示:

    ```
    [0.6073670992046881, 0.5691158156911582, 0.5448113207547169]
    ```

    为了计算多层感知器的精度，可以使用下面的代码片段:

    ```
    X_sets = [X_train, X_dev, X_test]
    Y_sets = [Y_train, Y_dev, Y_test]
    precision = []
    for i in range(0, len(X_sets)):
        pred = model_NN.predict(X_sets[i])
        score = precision_score(Y_sets[i], pred)
        precision.append(score)
    print(precision)
    ```

    结果列表应该如下所示:

    ```
    [0.759941089837997, 0.5920398009950248, 0.5509259259259259]
    ```

    通过计算新训练的模型的所有三个集合的精度分数，我们获得以下值:

![Figure 6.11: Precision scores for the newly trained models
](img/B15781_06_11.jpg)

图 6.11:新训练模型的精度分数

注意

要访问该特定部分的源代码，请参考[https://packt.live/2RpIhn9](https://packt.live/2RpIhn9)。

本节目前没有在线交互示例，需要在本地运行。

两种模型的性能都有所提高，通过比较这些值，可以得出多层感知器优于决策树模型的结论。在此基础上，选择多层感知器作为解决数据问题的较好模型。

注意

鼓励您继续微调参数，以达到更高的精度分数。

## 活动 6.02:保存并加载银行营销数据集的最终模型

解决方案:

1.  打开*活动 6.01* 、*中的 Jupyter 笔记本，执行银行营销数据集*的准备和创建阶段。
2.  出于学习的目的，将您选择的模型作为最佳模型，去掉`random_state`参数，并运行几次。
3.  将你选择的最佳模型保存到一个名为`final_model.pkl`的文件中。

    ```
    path = os.getcwd() + "/final_model.pkl" file = open(path, "wb") pickle.dump(model_NN, file)
    ```

4.  打开一个新的 Jupyter 笔记本，导入需要的模块和类:

    ```
    from sklearn.neural_network import MLPClassifier import pickle import os
    ```

5.  加载保存的模型:

    ```
    path = os.getcwd() + "/final_model.pkl" file = open(path, "rb") model = pickle.load(file)
    ```

6.  Perform a prediction for an individual by using the following values: `42`, `2`, `0`, `0`, `1`, `2`, `1`, `0`, `5`, `8`, `380`, `1`, `-1`, `0`:

    ```
    pred = model.predict([[42,2,0,0,1,2,1,0,5,8,380,1,-1,0]])
    print(pred)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/2UIWFss](https://packt.live/2UIWFss)。

    本节目前没有在线交互示例，需要在本地运行。

如果我们打印`pred`变量，输出是`0`，它是`No`的数字形式。这意味着个人更有可能不订阅新产品。

## 活动 6.03:允许与银行营销数据集模型交互

解决方案:

1.  In a text editor, create a class object that contains two main functions. One should be an initializer that loads the saved model, while the other should be a `predict` method where the data is fed to the model to retrieve an output:

    ```
    import pickle
    import os
    ```

    按照前面的代码片段，第一步是导入所有必需的元素来定位保存的模型并对其进行反序列化:

    ```
    Class NN_Model(object):
        def __init__(self):
            path = os.getcwd() + "/model_exercise.pkl"
            file = open(path, "rb")
            self.model = pickle.load(file)
        def predict(self, age, job, marital, education, \
                    default, balance, housing, loan, day, \
                    month, duration, campaign, pdays, previous):
            X = [[age, job, marital, education, default, \
                  balance, housing, loan, day, month, \
                  duration, campaign, pdays, previous]]
            return self.model.predict(X)
    ```

    接下来，按照前面的代码片段，编写将保存的模型与交互通道连接起来的类。它应该有一个初始化器方法来反序列化和加载保存的模型，还有一个`predict`方法来将输入数据提供给模型以执行预测。

2.  In a Jupyter Notebook, import and initialize the class that you created in the previous step. Next, create the variables that will hold the values for the features of a new observation and use the following values: `42`, `2`, `0`, `0`, `1`, `2`, `1`, `0`, `5`, `8`, `380`, `1`, `-1`, `0`:

    ```
    from trainedModel import NN_Model
    model = NN_Model()
    age = 42
    job = 2
    marital = 0
    education = 0
    default = 1
    balance = 2
    housing = 1
    loan = 0
    day = 5
    month = 8
    duration = 380
    campaign = 1
    pdays = -1
    previous = 0
    ```

    应用`predict`方法进行预测:

    ```
    pred = model.predict(age=age, job=job, marital=marital, \
                         education=education, default=default, \
                         balance=balance, housing=housing, \
                         loan=loan, day=day, month=month, \
                         duration=duration, campaign=campaign, \
                         pdays=pdays, previous=previous)
    print(pred)
    ```

    通过打印变量，预测等于`0`；也就是说，具有给定特征的个人不太可能订阅该产品，如下所示:

    ```
    [0]
    ```

    注意

    要访问该特定部分的源代码，请参考 https://packt.live/2Y2yBCJ 的。

    你也可以在[https://packt.live/3d6ku3E](https://packt.live/3d6ku3E)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

通过本章中的活动，您已经成功地学习了如何开发一个完整的机器学习解决方案，从数据预处理和训练模型到使用错误分析选择性能最佳的模型并保存模型以便能够有效地利用它。