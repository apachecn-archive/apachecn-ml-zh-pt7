<title>B18335_02_ePub</title>

# *第二章*:流媒体和实时机器学习的架构

流式架构是实时机器学习和流式分析解决方案的重要组成部分。即使您有一个模型或其他分析工具可以实时处理数据、更新数据并立即做出响应，如果没有架构来支持您的解决方案，这也是没有用的。

第一个重要的考虑是确保你的模型和分析能在每个数据点上起作用；需要有一个更新函数和/或预测函数，它可以根据系统接收到的每个新观察值来更新解。

实时和流架构的另一个重要考虑因素是数据入口:例如，如何确保可以在每次观察的基础上接收数据，而不是更传统的每天更新数据库的批处理方法。

除此之外，了解如何让不同的软件系统进行通信也很重要。例如，数据必须从您的数据生成流程中快速流出，可能会经过数据存储解决方案、数据质量工具或安全层，然后被您的分析程序接收。分析程序将完成其工作，并将结果发送回数据源，或者可能将处理后的数据点转发到可视化解决方案、警报系统或类似系统。

在这一章中，你将会了解到流和实时机器学习的架构。这本书的中心焦点仍然是管道中的分析和机器学习部分。本章的目标是给你足够的元素来想象和实现粗糙的工作架构，而一些关于性能、可用性和安全性的高度专门化的部分将被省略。

本章涵盖以下主题:

1.  将您的分析定义为一项功能
2.  了解微服务架构
3.  通过 API 在服务之间进行通信
4.  揭秘 HTTP 协议
5.  在 AWS 上构建简单的 API
6.  面向实时流的大数据工具
7.  实时调用大数据环境

# 技术要求

你可以在 GitHub 上找到这本书的所有代码，链接如下:[https://GitHub . com/packt publishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python)。如果您还不熟悉 Git 和 GitHub，下载笔记本和代码示例的最简单方法如下:

1.  转到存储库的链接。
2.  转到绿色的**代码**按钮。
3.  选择**下载压缩文件**。

当您下载 ZIP 文件时，您将它解压缩到您的本地环境中，您将能够通过您喜欢的 Python 编辑器访问代码。

## Python 环境

为了阅读本书，您可以下载存储库中的代码，并使用您喜欢的 Python 编辑器来执行它。

如果你还不熟悉 Python 环境，我建议你去看看 Anaconda([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual))，它附带了 Jupyter Notebook 和 JupyterLab，这两个工具都非常适合执行笔记本。它还带有 Spyder 和 VSCode，用于编辑脚本和程序。

如果你在你的机器上安装 Python 或相关程序有困难，你可以看看 Google Colab([https://colab.research.google.com/](https://colab.research.google.com/))或 Kaggle 笔记本([https://www.kaggle.com/code](https://www.kaggle.com/code))，它们都允许你在网上免费运行 Python 代码，不需要做任何设置。

注意

书中的代码一般会使用 Python 3 . 7 . 13 版本的 Colab 和 Kaggle 笔记本，你可以设置自己的环境来模仿这个。

# 将您的分析定义为一项功能

为了让从架构开始，让我们使用不同的构建模块从头开始构建一个想法，这是最小化工作产品所必需的。

为此，您首先需要了解您想要执行的实时分析的类型。

现在，让我们看一下上一章中的同一个例子:一个实时业务规则，当生产线的温度或酸度超出可接受的限度时，它会打印一个警报。

在前一章中，该警报编码如下:

代码块 2-1

```
def super_simple_alert(datapoint):
```

```
  if datapoint['temperature'] < 10:
```

```
    print('this is a real time alert. temp too low')
```

```
  if datapoint['pH'] > 5.5:
```

```
    print('this is a real time alert. pH too high')
```

在前一章中，您使用了数据帧上的迭代来测试这段代码。实际上，你总是需要一个架构的概念，这样你就可以让你的代码从数据生成过程中实时接收数据。本章将介绍这一构造块。

在下面的示意图中，您可以看到我们流式解决方案的高级架构模式:

![Figure 2.1 – A high-level architectural schema for a streaming solution
](img/B18335_02_01.jpg)

图 2.1–流式解决方案的高级架构模式

在这个示意图中，您可以清楚地看到，编写代码将为您提供解决方案的一些关键组件。然而，您需要围绕这一点构建一个架构，以使解决方案变得真实。示例实现中仍然缺少较暗的部分。

虽然这本书的目标不是给出一个关于架构的全面深入的课程，但是你会在这里发现一些工具和构建块，它们将允许你交付一个 MVP 实时用例。为了让你的构建模块有条理，你需要为你的解决方案选择一个架构。微服务是一种架构模式，它将允许您构建干净、小的构建块，并让它们相互通信。

# 了解微服务架构

在处理架构时，理解**微服务**的概念很重要。虽然有其他方法来构建软件项目，但是微服务很受欢迎是有原因的。它们帮助团队变得灵活和有效，并帮助保持软件的灵活性和清晰的结构。

微服务背后的思想体现在名称中:软件被表示为许多独立运行的小服务。当查看整体架构时，每个微服务都在一个小的*黑盒*中，有明确定义的输入和输出。在正确的时间调用正确的黑盒的过程已经就位。

微服务架构是松散耦合的。这意味着不同微服务之间没有固定的通信。相反，任何其他服务或代码都可以调用或不调用每个微服务。

如果需要对其中一个微服务进行更改，更改的范围是相当局部的，因此不会影响其他微服务。由于输入和输出是预定义的，这也有助于保持程序的基本结构有序，而不用以任何方式固定它。

为了允许不同的微服务进行通信，一个经常选择的解决方案是使用**应用编程接口**(**API**)。现在让我们深入探讨一下。

# 通过 API 在服务之间进行通信

微服务架构中的一个核心组件是 API 的使用。API 是允许您将两个微服务(或其他代码片段)连接在一起的部分。

API 很像网站。就像网站一样，API 是建立在类似网站的链接或 IP 地址之后的。当您访问一个网站时，该网站的服务器会向您发送代表该网站的代码。然后，您的互联网浏览器解释这些代码，并向您显示一个网页。

当你调用一个 API 时，API 将接收你的请求。该请求触发您的代码在服务器上运行，并生成一个返回给您的响应。如果出现问题(可能您的请求不符合预期或者发生了错误)，您可能收不到任何响应，或者收到一个错误代码，如`request not authorized`或`internal server error`。

下图显示了涵盖这一点的流程图。计算机或用户发送一个 HTTP 请求，API 服务器根据在 API 服务器上运行的代码发回响应:

![Figure 2.2 – A high-level architectural schema for a streaming solution
](img/B18335_02_02.jpg)

图 2.2–流式解决方案的高级架构模式

你可以用很多不同的工具调用 API。有时，你甚至可以使用你的网络浏览器，否则，像 cURL 这样的工具可以在命令行上完成这项工作。你可以使用像Postman 或者失眠症这样的工具来调用带有用户界面的 API。所有的通信都包含在固定的规则和实践中，这些规则和实践统称为 HTTP 协议，我们将在下一节中探讨。

# 解密 HTTP 协议

服务(或网站)之间的交互使用 HTTP 协议。当使用 API 和构建通信微服务时，理解 HTTP 协议的基础很重要。

最重要的是知道如何发送和格式化请求和响应。

## 获取请求

最简单的 HTTP 请求是`GET`请求。当你需要从服务器或服务中获取某些东西时，你可以使用这个。例如，当你访问一个网站时，你的浏览器向该网站的 IP 地址发送一个`GET`请求，以获取该网站的布局代码。

使用下面的代码可以简单地从 Python 发送一个`GET`请求:

代码块 2-2

```
import requests
```

```
import json
```

```
response = requests.get('http://www.google.com')
```

```
print(response.status_code)
```

```
print(response.text)
```

这段代码使用 Python 中的`requests`库向 Google 主页发送一个`GET`请求。从技术上来说，这与你通过互联网浏览器进入谷歌主页的过程是一样的。您将获得 web 浏览器显示 Google 主页所需的所有代码。虽然你们中的许多人非常熟悉浏览器中 Google 主页的外观，但在这个代码响应中却很难识别。重要的是要明白，它实际上是完全相同的东西，只是格式不同。

## 发布请求

`POST`请求是你会经常遇到的另一个请求。它允许你在请求时发送一些数据。这通常是必要的，尤其是在分析 API 中，因为分析很可能发生在这些数据上。通过在`POST`请求的主体中添加数据，您可以确保您的分析代码收到了您的数据。

Python 中的语法类似于下面的代码块。目前，这段代码还不能工作，因为您还没有构建一个能够处理这些数据的服务器。但是，请记住,`POST`请求允许您将数据点发送给 API，目的是获得响应:

代码块 2-3

```
import requests
```

```
import json
```

```
data = {'temperature': 10, 'pH': 5.5}
```

```
response = requests.post('http://www.example.com',data=data)
```

```
print(response.status_code)
```

```
print(response.text)
```

## 系统间通信的 JSON 格式

服务间交互最常见的格式是 **JavaScript 对象表示法** ( **JSON** )格式。这是一种非常类似于 Python 中的字典格式的数据类型。实际上，它是一个被荣誉包围的键值对象。

JSON 有效负载的一个示例如下:

代码块 2-4

```
{
```

```
     'name': 'your name',
```

```
     'address': 'your address',
```

```
     'age': 'your age'
```

```
}
```

这种数据格式非常容易理解，也非常常用。因此，理解它是如何工作的是很重要的。在本章的后面你也会看到它的用法。

## RESTful API

虽然 API 开发超出了本书的范围，但是提供一些提示和最佳实践还是很有用的。使用最多的 API 结构是**具象状态转移** ( **REST** ) API。

REST API 的工作方式和其他 API 一样，但是它遵循一组特定的样式规则，这使得它可以被识别为 REST API，也称为 RESTful API。

REST APIs 中有六个指导性约束:

*   客户机-服务器体系结构
*   无国籍
*   可缓存性
*   分层系统
*   按需编码(可选)
*   统一界面

如果你想在这方面更进一步，本章末尾提供了一些进一步的阅读资料。现在我们已经了解了 HTTP 协议，让我们在**Amazon Web Services**(**AWS**)上构建一个 API。

# 在 AWS 上构建简单的 API

为了到做一些实际的事情，让我们在 AWS 上构建一个超级简单的 API。这将允许您理解不同的服务如何一起通信。它还可以作为一个很好的测试环境，用来测试本书其余部分中的例子。

您将使用 AWS 框架的以下组件。

## AWS 中的 API 网关

这是一个为您处理 API 请求的 AWS 服务。您可以指定希望收到的请求的类型，并指定在收到请求时应该采取的操作。当使用 API Gateway 构建 API 时，这将自动生成一个 IP 地址或链接，您可以向其发送 API 请求。

## 自动气象站中的λ

Lambda 是代码的一个无服务器执行环境。这意味着您可以编写 Python 代码，将其插入 API 网关，而不必考虑如何设置服务器、防火墙等。这对于解耦系统来说非常好，对于许多实时系统来说也足够快了。

## 本地机器上的数据生成过程

作为最后一个组件，您将用 Python 构建一个独立的数据生成过程。你可以在笔记本上执行这段代码。每当生成一个新的数据点时，代码将使用分析服务调用 API，并在需要时回复一个警报。

下图是该架构的示意图:

![Figure 2.3 – Detailed architecture schema for AWS
](img/B18335_02_03.jpg)

图 2.3–AWS 的详细架构模式

## 实现示例

为了实现该示例，我们将使用以下分步说明。如果您有 AWS 帐户，您可以跳过*步骤 0* 。

### 步骤 0–在 AWS 上创建帐户

如果您还没有 AWS 帐户，创建一个很容易。您必须使用信用卡进行设置，但是我们在这里使用的服务都有一个免费层。只要您在测试结束时关闭资源，就不太可能产生任何费用。但是，一定要小心，因为错误是会发生的，如果你使用了 AWS 上的大量资源，最终你会付出代价。

要设置账户，您只需按照[aws.amazon.com](http://aws.amazon.com)上的步骤操作即可。

### 步骤 1–设置 Lambda 函数

在收到的`POST`请求后，必须调用一个 Lambda 函数来执行我们的警报并发回响应。

进入**服务**菜单中的**λ**，点击**创建功能**。您将看到以下屏幕:

![Figure 2.4 – Creating a Lambda function
](img/B18335_02_04.jpg)

图 2.4–创建一个 Lambda 函数

确保选择 **Python** 并给你的函数起一个合适的名字。

创建完函数后，就该编码了。为此，您可以使用以下代码:

代码块 2-5

```
import json
```

```
def super_simple_alert(datapoint):    
```

```
    answer = ''
```

```
    if datapoint['temperature'] < 10:
```

```
        answer += 'temp too low ' 
```

```
    if datapoint['pH'] > 5.5:
```

```
        answer += 'pH too high '
```

```
    if answer == '':
```

```
        answer = 'all good'
```

```
    return answer
```

```
def lambda_handler(event, context):
```

```
    answer = super_simple_alert(event)
```

```
    return {
```

```
        'statusCode': 200,
```

```
        'body': json.dumps({'status': answer}),
```

```
    }
```

该代码有两个功能。`super_simple_alert`函数接受一个`datapoint`并返回一个答案(字符串格式的警报)。`lambda_handler`函数是处理传入 API 调用的代码。该事件包含`datapoint`，因此该事件被传递给`super_simple_alert`函数，以便分析是否应该发出警报。这存储在`answer`变量中。最后，`lambda_handler`函数返回一个 Python 字典，其中包含状态代码`200`和包含答案的主体。

该窗口现在应该如下所示:

![Figure 2.5 – The Lambda function window
](img/B18335_02_05.jpg)

图 2.5–λ函数窗口

### 步骤 2–设置 API 网关

作为第一个步骤，让我们设置 API 网关来接收`POST`请求。`POST`请求将包含一个主体，其中包含一个具有温度和 pH 值的 JSON，就像警报示例中一样。

要设置 API 网关，您必须进入 **API 网关**菜单，该菜单可通过**服务**菜单访问。**管理**控制台外观如下:

![Figure 2.6 – The AWS Management console
](img/B18335_02_06.jpg)

图 2.6–AWS 管理控制台

你应该在 **API Gateway** 菜单上结束，如下所示:

![Figure 2.7 – The API Gateway menu
](img/B18335_02_07.jpg)

图 2.7–API 网关菜单

当你进入 **API 网关**菜单，你可以进入**创建 API** 来设置你的第一个 API。

在**中创建 API** ，执行以下步骤:

1.  选择 **REST API** 。
2.  选择**休息**协议。
3.  将 API 构建为新的 API。
4.  创建一个 API 名称，例如`streamingAPI`。

您将获得一个空的 API 配置菜单，如下所示:

![Figure 2.8 – Adding a method in API Gateway
](img/B18335_02_08.jpg)

图 2.8–在 API 网关中添加方法

我们希望给增加一个`POST`方法，所以转到`POST`方法。将出现以下菜单用于设置`POST`方法:

![Figure 2.9 – The POST setup
](img/B18335_02_09.jpg)

图 2.9–开机自检设置

### 步骤 3–部署 API

仍然在 API 网关菜单中，点击`test`进行部署。您可以在这个阶段使用默认设置，但是重要的是要使用顶部的 URL，以便能够从数据生成过程中调用您的 API。您需要设置如下图所示的设置:

![Figure 2.10 – More details for the API
](img/B18335_02_010.jpg)

图 2.10–API 的更多细节

### 步骤 4——从另一个 Python 环境调用 API

现在，你可以从另一个 Python 环境调用你的 API，比如你自己电脑上的笔记本，或者从 Google Colab 笔记本。

您可以使用以下代码来实现这一点:

代码块 2-6

```
import requests
```

```
import json
```

```
data = {'temperature': 8, 'pH': 4}
```

```
response = requests.post('YOUR_URL', data = json.dumps(data))
```

```
print(json.loads(response.text))
```

您将获得以下答案:

代码块 2-7

```
{'statusCode': 200, 'body': '{"status": "temp too low "}'}
```

现在，你可以想象一下一个实时数据生成过程将如何在每个新数据点简单地调用 API，警报将立即生成！

## 更多架构考虑

尽管这是构建 API 的第一次伟大尝试，但你应该意识到，当你想以一种可靠和安全的方式构建它时，还有很多事情要考虑。数据科学和软件工程是不同的工作，从头到尾学习管理一个 API 所需的所有技能是需要时间的，这是有原因的。一般来说，数据科学家不会被要求这样做。

本例中未涉及的一些内容如下:

*   性能:伸缩性、负载平衡和延迟
*   DDoS 攻击
*   安全和黑客
*   API 调用的财务方面
*   依赖云提供商与不依赖云提供商

在本章的最后，有一些进一步阅读的资源，你可以去看看。

## 其他 AWS 服务和具有相同功能的其他服务

当前的示例使用 API Gateway 和 Lambda 函数来构建 API。这种方法的优点是易于访问和设置，这使得它成为本书中的一种很好的方法。但是，您应该知道还有许多其他工具和技术可以用来构建 API。

AWS 是使用最多的云提供商之一，大多数可以在 AWS 上完成的事情也可以在其他云提供商的平台上完成。其他大玩家的例子有谷歌的 GCP 和微软的 Azure。即使在 AWS 上，也有很多替代方案。

您也可以在本地环境中构建 API。当这样做时，您将再次拥有大量的工具和提供商选择。现在您已经看到了如何使用 Python 中的标准编程和微服务方法来构建 API，接下来您将看到一些使用大数据环境的替代方案。大数据环境通常有一个更陡峭的学习曲线，并且可能经常针对特定的使用情形，但是当处理高容量和高速度时，它们可能非常强大并且绝对必要。

# 用于实时流媒体的大数据工具

有许多大数据工具可以进行实时流分析。对于常规的实时系统来说，它们是很好的选择，尤其是当数据量很大并且需要高速的时候。

提醒一下，术语**大数据**通常用于重组工具，以解决过于复杂而不适合内存的问题。所解决的问题有三个核心特征:数量、多样性和速度。

大数据工具通常以在并行计算中完成大量工作而闻名。在编写非优化的常规 Python 代码时，代码通常会逐个传递数据点。大数据解决方案通过在多台服务器上并行处理数据点来解决这一问题。这种方法使得大数据工具在数据多的时候速度更快，但在数据少的时候速度更慢(由于管理不同工作人员的开销)。

大数据工具往往相对具体；它们应该只用于拥有大量数据的用例。开始为手头的每个问题开发大数据工具没有意义。

无数这样的解决方案是为处理流数据而制造的。让我们来看看一些常用的工具:

*   **Spark Streaming**:Spark Streaming是 Spark 的补充，是当今大数据的主要工具之一。Spark Streaming 可以插入 Kafka、Flume 和 Amazon Kinesis 等源，从而使流数据可以在 Spark 环境中访问。
*   **Apache Kafka** : Kafka 是由 Apache 管理的开源工具。这是一个为提供实时数据馈送而设计的框架。许多公司使用它来提供数据管道和流分析。甚至一些云提供商已经将 Kafka 集成到他们的解决方案中。
*   **Apache Flume**:ApacheFlume 是 Apache 管理的另一个开源工具，同样专注于流数据。Flume 专门用于处理大数据环境中的大量日志数据。
*   **Apache Beam**:Apache streaming 家族的另一个工具是 Apache Beam。这个工具可以处理批处理和流数据。它以构建 ETL 和数据处理管道而闻名。
*   **Apache Storm**:Apache Storm是一个流处理计算框架，允许进行分布式计算。它用于使用 Hadoop 实时处理数据流。
*   Apache NiFi : Apache NiFi 是一个专注于 ETL 的工具。它为用户提供了自动化和管理系统间数据流的可能性。它可以和卡夫卡一起工作。
*   **Google Cloud data flow**:GoogleCloud data flow 是 Google 云平台提出的工具。它是专门为处理流用例而开发的。它允许用户在一个完全托管的服务中执行 Apache Beam 管道。
*   **亚马逊 Kinesis** :亚马逊 Kinesis 强烈基于之前讨论过的开源 Apache Kafka。与 Kafka 相比，使用 Kinesis 的优势在于它附带了许多为您管理的东西，而如果您直接使用 Kafka，您将花费更多的精力来管理服务。当然，作为回报，你必须使用 AWS 平台来访问它。
*   **Azure Stream Analytics**:AzureStream Analytics 是微软云平台 Azure 上提出的主要流媒体分析服务。这是一个基于 Trill 的实时分析服务。
*   **IBM Streams**:IBM Streams是一个流分析工具，是在 IBM cloud 上提出的。就像 Kinesis 一样，基于开源的 Kafka 项目。

## 实时调用大数据环境

如果您的实时分析服务是由大数据或特定流工具管理的，您不能总是遵循 API 方法将您的实时流程连接到您的分析流程。

在大多数情况下，您需要查看您选择的工具的文档，并确保您了解如何使连接工作。在这一点上，您通常需要一个专门的概要文件来与您一起工作，因为这个级别的架构和数据工程通常被认为超出了大多数数据科学家的范围。

微服务系统和大数据系统之间的一个普遍区别是，在微服务方法中，我们通常认为调用服务必须考虑来自 API 的响应。

在大数据环境中，网站等服务向大数据环境发送数据但不需要响应的情况更为常见。您可以想象这样一个网站，它将用户的每一次交互以 JSON 文件的形式写到一个固定的位置。然后将大数据流工具插入该数据存储位置，以流的方式读入数据，并将其转换为分析、可视化或其他内容。

让我们构建一个最小的示例来展示如何做到这一点:

1.  首先，创建一个名为`example.json`的 JSON 文件，在该文件中只写入以下数据:

代码块 2-8

```
{'value':'hello'}
```

1.  你现在可以写一小段 Spark 流代码，以流的方式读取这些数据:

    ```
    from pyspark.sql import SparkSession from pyspark.sql.types import * spark = SparkSession \     .builder \     .appName("quickexample") \     .getOrCreate() schema = StructType([ StructField("value", StringType(), True) ]) streamingDF = (   spark     .readStream     .schema(schema)     .json('example.json') ) display(streamingDF)
    ```

简而言之，这段代码从创建一个`spark`会话开始。一旦创建了会话，就会为`example.json`文件定义一个模式。因为它只有一个键(称为`value`，所以模式非常短。该值的数据类型为`string`。

然后您会看到数据是使用`.readStream`方法导入的，这实际上为您做了很多繁重的流式处理工作。如果你想进一步了解这个例子，你可以使用`streamingDF`库和编写各种分析 Spark 函数，你将使用众所周知的大数据工具 **PySpark** 进行流分析。

# 总结

在这一章中，你已经开始发现建筑领域。您已经在 AWS 上构建了自己的 API，并且已经看到了系统间通信的基本基础。您现在应该明白，数据是系统间通信的关键，系统间的良好通信对于通过分析交付价值至关重要。

在实时和流分析的情况下尤其如此。如果在项目中没有尽早发现架构瓶颈，高速且通常较大的数据很容易造成问题。

您还必须记住考虑其他主题，包括安全性、可用性和法规遵从性。这些话题最好留给那些全职负责处理这类数据架构问题的人。

在下一章，我们将回到本书的核心，因为你会发现如何在流数据上构建分析用例。

# 延伸阅读

*   *微服务架构*:[https://cloud . Google . com/learn/what-is-microservice-Architecture](https://cloud.google.com/learn/what-is-microservices-architecture)
*   API:[https://www . red hat . com/en/topics/API/what-are-application-programming-interfaces](https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces)
*   HTTP:[https://developer.mozilla.org/en-US/docs/Web/HTTP](https://developer.mozilla.org/en-US/docs/Web/HTTP)
*   *十大实时数据流工具*:[https://ipspecialist . net/Top-10-real-time-data-streaming-tools/](https://ipspecialist.net/top-10-real-time-data-streaming-tools/)
*   spark Streaming:[https://spark . Apache . org/docs/latest/Streaming-programming-guide . html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
*   卡夫卡:[https://kafka.apache.org/](https://kafka.apache.org/)
*   水槽:[https://flume.apache.org/](https://flume.apache.org/)
*   光束:[https://beam.apache.org/](https://beam.apache.org/)
*   风暴:[https://storm.apache.org/](https://storm.apache.org/)
*   尼菲:[https://nifi.apache.org/](https://nifi.apache.org/)
*   谷歌云数据流:[https://cloud.google.com/dataflow](https://cloud.google.com/dataflow)
*   亚马逊 Kinesis:[https://aws.amazon.com/kinesis/](https://aws.amazon.com/kinesis/)
*   azure Stream Analytics:[https://azure . Microsoft . com/en-us/services/Stream-Analytics/](https://azure.microsoft.com/en-us/services/stream-analytics/)
*   IBM Streams:[https://www.ibm.com/docs/en/streams](https://www.ibm.com/docs/en/streams)
*   *用亚马逊 Kinesis* 捕捉网页滚动进度，由 AWS:[https://docs . AWS . Amazon . com/SDK-for-JavaScript/v2/developer-guide/kine sis-examples-capture-Page-scrolling . html](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html)