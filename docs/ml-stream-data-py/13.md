<title>B18335_10_ePub</title>

# *第十章*:特征变换和缩放

在前一章中，您已经看到了如何在流式和在线机器学习模型中管理漂移和漂移检测。漂移检测虽然不是机器学习中的主要概念，但却是生产中机器学习的一个非常重要的辅助方面。

虽然许多次要主题在机器学习中很重要，但一些附属主题对于在线模型尤其重要。漂移检测尤其重要，因为模型在重新学习中的自主性使其对开发人员或数据科学家来说更像是一个黑盒。只有通过漂移检测和类似方法正确管理再训练过程，这才有很大的优势。

在这一章中，你将看到另一个次要的机器学习主题，它对在线机器学习和流式传输有着重要的影响。特征转换和缩放是在传统的批处理机器学习中相对较好定义的实践。它们一般不会造成任何理论上的困难。

在在线机器学习中，缩放和特征转换并不简单。有必要使这种做法适应新数据与原始数据不完全可比的可能性。这就产生了这样的问题:是否要在每一个新到达的数据上重新调整特征变换和缩放器，以及这种做法是否会将偏差引入到已经训练好的和不断重新训练的模型中。

本章涵盖的主题如下:

*   流数据的数据准备挑战
*   缩放数据以进行流式传输
*   在流环境中转换特征

# 技术要求

你可以在 GitHub 上找到这本书的所有代码，链接如下:[https://GitHub . com/packt publishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python)。如果您还不熟悉 Git 和 GitHub，下载笔记本和代码示例的最简单方法如下:

1.  转到存储库的链接。
2.  转到绿色的**代码**按钮。
3.  选择**下载压缩文件**。

当您下载 ZIP 文件时，在您的本地环境中解压缩它，您将能够通过您喜欢的 Python 编辑器访问代码。

## Python 环境

为了阅读本书，您可以下载存储库中的代码，并使用您喜欢的 Python 编辑器执行它。

如果你还不熟悉 Python 环境，我建议你去看看 Anaconda([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual))，它带有 Jupyter Notebook 和 JupyterLabs，这两个工具都非常适合执行笔记本。它还带有 Spyder 和 VSCode，用于编辑脚本和程序。

如果你在你的机器上安装 Python 或相关程序有困难，你可以看看Google Colab(【https://colab.research.google.com/】)或ka ggle Notebooks([https://www.kaggle.com/code](https://www.kaggle.com/code))，它们都允许你在网上免费运行 Python 代码，不需要任何设置。

注意

书中的代码一般会使用 Python 版本 *3.7.13* 的 Colab 和 Kaggle 笔记本，你可以设置自己的环境来模仿这个。

# 流数据的数据准备挑战

在深入研究具体的算法和解决方案之前，让我们先大致讨论一下，在处理以流式方式到达的数据时，为什么数据准备会有所不同。可以确定多种原因，例如:

*   第一个明显的问题是数据漂移。正如上一章详细讨论的那样，由于数据漂移，数据的趋势和描述性统计数据会随着时间慢慢改变。如果您的特征工程或数据准备过程过于依赖遵循特定分布的数据，当数据漂移发生时，您可能会遇到问题。由于在上一章中已经提出了许多解决方案，所以这一主题在本章中将不予考虑。
*   第二个问题是人口参数未知。当以流式方式观察数据时，您对总体参数的估计可能会随着时间的推移而慢慢改善。正如第三章 中的 [*所示，随着数据量的增加，你对描述性统计的估计会更加精确。当描述性统计估计值不断提高时，它们会随着时间的推移而变化，这一事实并不容易修正您的数据准备、特征工程、缩放等公式:*](B18335_03_ePub.xhtml#_idTextAnchor051)
    *   作为第一个例子，考虑范围。该范围代表您观察到的数据的最小值和最大值。这在数据缩放和其他算法中广泛使用。现在，假设批处理中的范围(最小值和最大值)可以不同于数据的全局范围(全局最小值和全局最大值)。毕竟，当新数据到达时，您可能会观察到比您的历史数据中观察到的任何值都高或低的值，只是通过随机采样的过程。如果处理不当，观察到高于最大值的观察值可能会导致缩放出现问题。
    *   另一个例子是使用正态分布进行缩放。批次中的标准差和平均值可能不同于总体标准差和总体平均值。这可能会导致您的缩放器在一段时间后表现不同，这是一种由您自己的缩放算法引起的数据漂移。显然，这必须避免。
*   还存在许多其他此类问题，包括观察分类值中的新类别，这将导致您的一次性编码器或使用分类变量的模型出现问题。您还可以想象，在数据中出现新类型的值(如 NAs 和 INF)需要很好地管理，而不是让它们导致错误。这在一般情况下是正确的，但是在处理流数据时，这往往会比处理常规数据带来更多的麻烦。

在下一节中，我们将学习什么是缩放以及如何使用缩放。

# 缩放数据以进行流式传输

在本节的第一个部分，让我们先来看看一些流缩放数据的解决方案。在讨论解决方案之前，让我们快速回顾一下什么是扩展以及它是如何工作的。

## 引入缩放

数字变量可以是任何规模的，例如，意味着它们可以有很高的平均值或很低的平均值。一些机器学习算法根本不受变量规模的影响，而其他机器学习算法可能会受到强烈影响。

缩放是取一个数值变量并缩小其范围，以及潜在的标准偏差，到一个预先指定的范围。这将允许所有机器学习算法毫无问题地从数据中学习。

### 使用 MinMaxScaler 进行缩放

为了实现这个目标，一个常用的方法是最小-最大缩放器。最小-最大缩放器将接受任何范围内的输入变量，并减少范围(`0`至`1`)内的所有值，这意味着缩放变量的最小值将是`0`，缩放变量的最大值将是`1`。有时，使用一种替代方案，其中最小值不是`0`，而是`-1`。

最小-最大缩放的数学公式如下:

![](img/Formula_10_001.jpg)

### 用标准定标器定标

另一种非常常见的扩展方法是标准化。标准化是一种强烈基于统计的方法，它允许你取任何变量并把它带回到标准的正态分布。标准正态分布的平均值为`0`，标准差为`1`。

标准定标器的数学公式如下:

![](img/Formula_10_002.jpg)

缩放变量的值将不在任何特定范围内；缩放变量的新值表示原始值偏离原始平均值的标准偏差数。一个非常极端的值(想象四或五个偏离平均值的标准偏差)的值是四或五，顺便说一下，它可以是正的也可以是负的。

### 选择您的缩放方法

缩放算法的选择取决于用例，并且通常对不同算法使用不同缩放方法的机器学习管道进行调整是一个好主意。毕竟，缩放方法的选择对该方法的训练的性能有影响。

众所周知，最小-最大缩放器很难处理异常值。毕竟，一个非常极端的异常值将被设置为最大值，即`1`。然后，这可能会导致其他值减少到一个小得多的范围。

StandardScaler 以更好的方式处理这个问题，因为异常值仍然是异常值，只是在缩放变量中呈现高值。这同时也是一个缺点，主要是当你使用的机器学习算法需要的值在`0`和`1`之间的时候。

## 使缩放适应流环境

现在，让我们来看看如何将这些方法分别应用于流数据的情况。我们将从最小-最大缩放器开始。

### 使 MinMaxScaler 适应流式传输

最小最大缩放器在固定数据集上运行良好。它保证了缩放后的数据的值将在`0`和`1`之间，正如一些机器学习算法所要求的那样。然而，在流数据的情况下，这就不那么容易管理了。

当新数据一个接一个(以流的形式)到达时，不可能决定最小值或最大值。毕竟，你不能期望一个值既是最小值又是最大值。批处理时也会出现同样的问题:不能保证批处理最大值高于全局最大值，最小值也是如此。

您可以使用训练数据来决定最小值和最大值，但问题是您的新数据可能高于训练最大值或低于训练最小值。这将导致缩放值超出范围(`0`到`1`)。

对此的解决方案是使用运行最小值和运行最大值。这意味着您要继续更新 MinMaxScaler，以便每次观察到较低的最小值时，您都要更新 MinMaxScaler 公式中的最小值，而每次观察到较高的最大值时，您都要更新最大值。

这种方法的优点是，它保证您的缩放数据将始终在`0`和`1`之间。一个缺点是用于训练最小最大缩放器的第一个值将被缩放得非常糟糕。这很容易通过使用一些训练数据来初始化 MinMaxScaler 来解决。离群值也可能是一个问题，因为有一个非常极端的值会强烈影响最小最大值公式，之后的分数会非常不同。这可以通过使用异常值检测方法来解决，详见第 5 章中的[和*。*](B18335_05_ePub.xhtml#_idTextAnchor097)

现在让我们继续讨论自适应最小最大缩放器的 Python 实现:

1.  为此，我们将使用 Python 库`River`中的 MinMaxScaler 的实现。我们将在本例中使用以下数据:

代码块 10-1

```
import numpy as np
data = np.random.randint(0, 100, size=1000)
```

1.  可以使用以下代码创建该数据的直方图:

代码块 10-2

```
import matplotlib.pyplot as plt
plt.hist(data)
```

生成的直方图如下所示:

![Figure 10.1 – Resulting histogram of Code Block 10-2
](img/B18335_10_1.jpg)

图 10.1–代码块 10-2 的结果直方图

1.  现在，为了缩放这些数据，让我们使用 River 的`MinMaxScaler`函数。在数据中循环将模拟数据以流的方式到达，使用`learn_one`方法显示数据逐步更新:

代码块 10-3

```
!pip install river
from river import preprocessing
# convert the data to required format
data_stream = [{'x':float(x)} for x in list(data)]
# initialize list for scaled values
data_scaled = []
# initialize scaler
my_scaler = preprocessing.MinMaxScaler()
# streaming
for observation in data_stream:
  # learn (update)
  my_scaler.learn_one(observation)
  # scale the observation
  scaled_obs = my_scaler.transform_one(observation)

  # store the scaled result
  data_scaled.append(scaled_obs['x'])
```

1.  现在，将会有趣地看到缩放数据的直方图。它可以按如下方式创建:

代码块 10-4

```
import matplotlib.pyplot as plt
plt.hist(data_scaled)
```

直方图如下所示:

![Figure 10.2 – Resulting histogram of Code Block 10-4
](img/B18335_10_2.jpg)

图 10.2–代码块 10-4 的结果直方图

这张直方图清楚地表明，我们已经成功地将数据缩放到`0`到`1`的范围内。

既然您已经看到了 MinMaxScaler 的理论和实现，那么现在让我们来看看 StandardScaler，它是这种方法的一种常见替代方法。

### 使标准定标器适应流

在未来观察更多极端数据时，标准缩放中可能出现的问题与最小-最大缩放中出现的问题并不完全相同。最小-最大缩放器使用最小值和最大值来计算缩放方法，而标准缩放器使用平均值和标准差。

之所以有很大的不同，是因为最小值和最大值在某个时间点被超越的可能性相对较大。这将导致缩放后的值高于`1`或低于`0`，这可能会给你的机器学习算法带来真正的问题。

在标准定标器中，未来出现的任何极值都会影响您对全局平均值和标准差的估计，但它们不太可能对它们产生非常严重的影响。毕竟，均值和标准差对少数极值的观察要不敏感得多。

鉴于这种理论上的考虑，您可能会得出结论，没有必要更新标准的定标器。然而，无论如何最好是更新它，因为这是保持你的机器学习方法最新的好方法。与使用最小-最大缩放器相比，这样做的附加价值影响较小，但无论如何，这是一种最佳做法。

您可以使用的一个解决方案是使用`Riverml`包中的 AdaptiveStandardScaler。它使用指数加权运行均值和方差来确保数据的正态分布的轻微漂移被考虑在内，而不会有太大的权重。让我们看一个如何使用 AdaptiveStandardScaler 的 Python 示例:

1.  我们将在本例中使用以下数据:

代码块 10-5

```
import numpy as np
data = np.random.normal(12, 15, size=1000)
```

1.  从直方图中可以看出，该数据遵循正态分布。您可以创建如下直方图:

代码块 10-6

```
import matplotlib.pyplot as plt
plt.hist(data)
```

生成的直方图如下所示:

![Figure 10.3 – Resulting histogram of Code Block 10-6
](img/B18335_10_3.jpg)

图 10.3–代码块 10-6 的结果直方图

数据显然遵循正态分布，但它不是以`0`为中心，也没有标准化为`1`的标准差。

1.  现在，为了缩放这些数据，让我们使用来自 River 的`StandardScaler`。同样，我们将通过数据循环来模拟流。同样，我们再次使用`learn_one`方法逐步更新数据:

代码块 10-7

```
from river import preprocessing
# convert the data to required format
data_stream = [{'x':float(x)} for x in list(data)]
# initialize list for scaled values
data_scaled = []
# initialize scaler
my_scaler = preprocessing.StandardScaler()
# streaming
for observation in data_stream:
  # learn (update)
  my_scaler.learn_one(observation)
  # scale the observation
  scaled_obs = my_scaler.transform_one(observation)

  # store the scaled result
  data_scaled.append(scaled_obs['x'])
```

1.  为了验证是否正常工作，让我们使用以下代码重做直方图:

代码块 10-8

```
plt.hist(data_scaled)
```

直方图如下所示:

![Figure 10.4 – Resulting histogram of Code Block 10-8
](img/B18335_10_4.jpg)

图 10.4–代码块 10-8 的结果直方图

正如您可以从看到的，数据明显以`0`为中心，新的缩放值表示每个数据点偏离平均值的标准偏差数。

在下一节中，您将看到如何在流环境中调整特性转换。

# 在流环境中转换功能

缩放数据是机器学习预处理数据的一种方式，但许多其他统计方法也可以用于数据准备。在本章的第二部分，让我们深入探讨一下**主成分分析** ( **PCA** )方法，这是一种在任何机器学习开始时准备数据的常用方法。

## 引入 PCA

PCA 是一种机器学习方法，可以用于多种应用。当处理高度多元的数据时，主成分分析可用于解释，您可以使用它来理解和分析多元数据集。这是主成分分析在数据分析中的应用。

另一种使用 PCA 的方法是为机器学习准备数据。从高层次的角度来看，PCA 可以被视为缩放的替代方法，可以减少您的数据的变量数量，使模型更容易拟合。这是与本章最相关的主成分分析的用法，也是它在示例中的用法。

## PCA 的数学定义

PCA 对多元数据(或多列数据)起作用。这些列通常有一个业务定义。PCA 的目标是保留数据中的所有信息，但将当前变量定义更改为具有不同解释的变量。

新变量被称为主成分**主成分**，并且它们以这样的方式被发现，即第一成分包含最可能的变化，第二成分是与第一成分正交的成分，并且解释了正交时最可能的变化。

此处显示了示意图:

![Figure 10.5 – Schematic overview of PCA
](img/B18335_10_5.jpg)

图 10.5–PCA 示意图

这个例子清楚地显示了左边的原始数据是如何转换成右边的主成分的。就信息而言，第一个主成分比任何原始变量都更有价值。当处理数百个变量时，您可以想象您只需要保留有限数量的组件(基于不同的标准和您的用例)，这可能会使您的机器学习算法更容易从数据中学习。

## Python 中的常规 PCA

为了更好地比较常规 PCA 和增量 PCA，最好让每个人都跟上进度，先做一个常规 PCA 的快速示例:

1.  为此，让我们创建一些模拟样本数据来处理这个示例。我们可以制作一个小型数据集示例如下:

代码块 10-9

```
import numpy as np
import pandas as pd
X1 = np.random.normal(5, 1, size=100)
X2 = np.random.normal(5, 0.5, size=100)
data = pd.DataFrame({'X1': X1, 'X2': X1 + X2})
data.head()
```

数据看起来像这样:

![Figure 10.6 – The resulting data
](img/B18335_10_6.jpg)

图 10.6–结果数据

1.  您可以按如下方式绘制该数据的图表:

代码块 10-10

```
import matplotlib.pyplot as plt
plt.scatter(data['X1'], data['X2'])
```

散点图显示了一个与之前示意图中的草图非常相似的图:

![Figure 10.7 – The resulting image of Code Block 10-10
](img/B18335_10_7.jpg)

图 10.7–代码块 10-10 的结果图像

1.  现在，让我们使用一个常规的 PCA 来识别组件并转换数据。以下代码块显示了如何使用`scikit-learn`来拟合 PCA:

代码块 10-11

```
from sklearn.decomposition import PCA
my_pca = PCA()
transformed_data = my_pca.fit_transform(data)
transformed_data = pd.DataFrame(transformed_data, columns = ['PC1', 'PC2'])
transformed_data.head()
```

转换后的数据如下所示:

![Figure 10.8 – The transformed data
](img/B18335_10_8.jpg)

图 10.8–转换后的数据

1.  我们可以像对待之前的数据一样绘制它。这可以使用以下代码来完成:

代码块 10-12

```
plt.scatter(transformed_data['PC1'], transformed_data['PC2'])
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
```

情节看起来如下:

![Figure 10.9 – Plot of the transformed data
](img/B18335_10_9.jpg)

图 10.9-转换数据图

你可以清楚地看到，这看起来很像早期理论介绍中的结果图。这种 PCA 已经成功地将第一个主成分识别为解释大部分数据的成分。第二部分解释了剩余数据的最大部分(在第一部分之后)。

## 用于流式传输的增量 PCA

在流环境中，PCA 不容易在单个数据点上计算。毕竟，你可以想象，不可能确定单个数据点的标准偏差，因此，没有可能确定最佳成分的方法。

建议的解决方案是通过批处理来完成，并分批计算您的 PCA，而不是一次全部计算。`scikit-learn`包有一个叫做`IncrementalPCA`的功能，允许你批量安装 PCA。让我们使用下面的代码在与之前相同的数据上拟合`IncrementalPCA`,并比较结果。使用`IncrementalPCA`进行拟合和变换的代码如下所示:

代码块 10-13

```
from sklearn.decomposition import IncrementalPCA
```

```
my_incremental_pca = IncrementalPCA(batch_size = 10)
```

```
transformed_data_2 = my_incremental_pca.fit_transform(data)
```

```
transformed_data_2 = pd.DataFrame(transformed_data_2, columns = ['PC1', 'PC2'])
```

```
transformed_data_2.head()
```

使用第二种方法的转换数据如下:

![Figure 10.10 – The transformed data using incremental PCA
](img/B18335_10_10.jpg)

图 10.10–使用增量 PCA 转换的数据

现在，让我们也对这些数据进行绘图，看看这种批量 PCA 是否成功地拟合了真实的成分，或者它是否远离原始 PCA:

代码块 10-14

```
plt.scatter(transformed_data_2['PC1'], transformed_data_2['PC2'])
```

```
plt.xlim(-4, 4)
```

```
plt.ylim(-4, 4)
```

```
plt.show()
```

生成的散点图如下所示:

![Figure 10.11 – The scatter plot of the transformed data using incremental PCA
](img/B18335_10_11.jpg)

图 10.11–使用增量 PCA 的转换数据的散点图

该散点图显示 PCA 已被正确安装。不要被增量 PCA 反转第一个分量的事实所迷惑(与前一个相比，图像从左向右镜像)。这并没有错，只是一种镜像。这种增量 PCA 很好地捕捉了两个分量。

# 总结

在本章中，您已经看到了一些适用于流数据和在线数据的常用数据准备方法。对于流数据，很重要的一点是要有容易改装或重新估计的模型。

在本章的第一部分，你已经看到了两种缩放的方法。最小最大缩放器将数据缩放至`0`至`1`范围，因此需要确保没有新数据点超出此范围。标准定标器使用均值和标准差进行统计归一化处理。

本章的第二部分演示了一个常规 PCA 和一个新的增量版本`IncrementalPCA`。这种增量方法允许您批量拟合 PCA，这在对流数据拟合 PCA 时会有所帮助。

通过本章中的缩放和特征转换，以及上一章中的漂移检测，您已经看到了机器学习在流上的辅助任务的很大一部分。在下一章中，您将看到机器学习和流的第三个也是最后一个次要主题，即灾难性遗忘:在线机器学习中可能发生的有影响的问题，导致模型忘记重要的学习趋势。本章将解释如何检测和避免它。

# 进一步阅读

*   *河流中的最大水垢*:[https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/](https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/)
*   *河流中的 standard scaler*:[https://River ml . XYZ/latest/API/preprocessing/standard scaler/](https://riverml.xyz/latest/api/preprocessing/StandardScaler/)
*   *sci kit-learn 中的 PCA*:[https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
*   *sci kit-learn 中的增量 PCA*:[https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . Incremental PCA . html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)