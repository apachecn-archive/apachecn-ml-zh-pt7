<html><head/><body>



<title>Chapter 5. Decision Tree based learning</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title">第五章。决策树学习</h1></div></div></div><p>从本章开始，我们将深入研究每一种机器学习算法。我们从用于分类和回归的非参数监督学习方法、决策树和高级技术开始。我们将概述一个可以通过构建基于决策树的模型来解决的业务问题，并学习如何在 Apache Mahout、R、Julia、Apache Spark 和 Python 中实现它。</p><p>本章深入讨论了以下主题:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">决策树:定义、术语、需求、优势和局限性。</li><li class="listitem" style="list-style-type: disc">构建和理解决策树的基础知识以及一些关键方面，如信息增益和熵。您还将学习构建回归、树的分类和测量误差。</li><li class="listitem" style="list-style-type: disc">了解决策树的一些常见问题，修剪决策树的必要性，以及修剪的技巧。</li><li class="listitem" style="list-style-type: disc">你会学习 CART、C4.5、C5.0 等决策树算法；以及专门的树，如随机森林、斜树、进化树和海灵格树。</li><li class="listitem" style="list-style-type: disc">理解分类和回归树的业务用例，以及使用 Apache Mahout、R、Apache Spark、Julia 和 Python (scikit-learn)库和模块实现的业务用例。</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec31"/>决策树</h1></div></div></div><p>决策树<a id="id653" class="indexterm"/>被认为是机器学习领域中最强大和最广泛使用的建模技术之一。</p><p>决策树自然地归纳出可用于数据分类和预测的规则。以下是从构建决策树中得出的规则定义的示例:</p><p>如果(笔记本电脑型号为<em> x </em>)和(由<em> y </em>制造)和(为<em> z </em>岁)和(有部分机主为<em> k </em>)那么(续航时间为<em> n </em>小时)。</p><p>当仔细观察时，这些规则以简单、人类可读和可理解的格式表达。此外，这些规则可以存储在数据存储中供以后参考。下面的概念图描述了决策树的各种特征和属性，这些将在下面的部分中介绍。</p><div><img src="img/B03980_05_01.jpg" alt="Decision trees"/></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec70"/>术语</h2></div></div></div><p>决策树<a id="id655" class="indexterm"/>通过从根到叶的树形结构对实例进行分类。最重要的是，在高层次上，决策树有两种表示形式——节点和连接节点的弧线。为了做出决策，流程从根节点开始，导航到弧，直到到达叶节点，然后做出决策。树的每个节点表示一个属性的测试，分支表示属性可能取的值。</p><p>以下是决策树表示的一些特征:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">每个非叶节点(例如，决策节点)表示属性值的一种表示</li><li class="listitem" style="list-style-type: disc">每个分支表示值表示的其余部分</li><li class="listitem" style="list-style-type: disc">每个叶(或终端)节点代表目标属性的值</li><li class="listitem" style="list-style-type: disc">开始的<a id="id656" class="indexterm"/>节点称为根节点</li></ul></div><p>下图是相同的表示:</p><div><img src="img/B03980_05_02.jpg" alt="Terminology"/></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec71"/>目的和用途</h2></div></div></div><p>决策树是用于分类和回归的<a id="id657" class="indexterm"/>。在此上下文中使用了两种类型的树:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">分类树</li><li class="listitem" style="list-style-type: disc">回归树</li></ul></div><p>分类<a id="id658" class="indexterm"/>树用于将给定的数据集分类。要使用分类树，目标变量的响应需要是一个分类值，如是/否、真/假。另一方面，回归树用于解决预测需求，并且总是在目标或响应变量是数值或离散值(如股票价值、商品价格等)时使用。</p><p>下图描述了决策树和相关树类别作为分类或回归树的用途:</p><div><img src="img/B03980_05_03.jpg" alt="Purpose and uses"/></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec72"/>构造决策树</h2></div></div></div><p>决策树可以通过一个简单的例子和手工构建一个决策树来最好地学习。在这一节中，我们来看一个简单的例子；下表显示了现有的数据集。我们的目标是根据客户的人口统计数据，预测他们是否会接受贷款。很明显，如果我们能拿出一个规则作为这个数据集的模型，这对业务用户来说是最有用的。</p><div><img src="img/B03980_05_04.jpg" alt="Constructing a Decision tree"/></div><p>根据上表，由于年龄和经验高度相关，我们可以选择忽略其中一个属性。这有助于隐含的特征选择。</p><p>案例 1:让我们开始构建决策树。首先，我们将选择按照 CCAvg(信用卡平均余额)进行拆分。</p><div><img src="img/B03980_05_05.jpg" alt="Constructing a Decision tree"/></div><p>有了这个<a id="id660" class="indexterm"/>决策树，我们现在有了两条非常明确的规则:</p><p><em>如果 CCAvg 为中，则 loan = accept </em>或<em>如果 CCAvg 为高，则 loan = accept </em></p><p>为了使规则更加清晰，让我们添加收入属性。我们还有两条规则:</p><p><em>如果 CCAvg 低且收入低，则不接受贷款</em></p><p><em>如果 CCAvg 低，收入高，则接受贷款</em></p><p>通过结合这里的第二条规则和前两条规则，我们可以导出以下规则:</p><p><em>如果(CCAvg 中等)或(CCAvg 高)或(CCAvg 低，收入高)那么贷款=接受</em></p><p>案例 2:让我们开始使用 Family 构建决策树:</p><div><img src="img/B03980_05_06.jpg" alt="Constructing a Decision tree"/></div><p>在这种情况下，只有一个规则<a id="id661" class="indexterm"/>没有给出准确的结果，因为它只有两个数据点。</p><p>因此，选择一个有效的属性来开始树会对模型的准确性产生影响。从前面的例子中，让我们列出一些构建决策树的核心规则:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">我们通常从一个属性开始构建决策树，根据属性分割数据，并对其他属性继续同样的过程。</li><li class="listitem" style="list-style-type: disc">对于给定的问题，可以有许多决策树。</li><li class="listitem" style="list-style-type: disc">树的深度与选择的属性数量成正比。</li><li class="listitem" style="list-style-type: disc">需要有一个终止标准来决定何时停止进一步构建树。在没有终止标准的情况下，模型会导致数据的过拟合。</li><li class="listitem" style="list-style-type: disc">最后，输出始终是简单规则的形式，可以存储并应用于不同的数据集进行分类和/或预测。</li></ul></div><p>决策树在机器学习领域受到青睐的原因之一是它们对错误的鲁棒性；当训练数据集中有一些未知值时，也可以使用它们(例如，收入数据不是所有记录都可用)。</p><div><div><div><div><h3 class="title"><a id="ch05lvl3sec67"/>处理缺失值</h3></div></div></div><p>给一些未知数赋值的一个有趣的方法是，根据出现次数来看，最常见的值被赋值，在某些情况下，它们可以属于同一类，如果可能的话，我们应该使它更接近准确。</p><p>还有另一种概率方法，其中预测按比例分布:</p><p>为<em> x </em>的每个值<em> vi </em>分配一个<a id="id663" class="indexterm"/>概率<em> pi </em>。</p><p>现在，将<em> x </em>的分数<em> pi </em>分配给每个后代。在节点<em> n </em>的例子中，这些概率可以基于 A 的各种值的观察频率来再次估计。</p><p>例如，让我们考虑一个布尔属性<em>一个</em>。假设<em> A </em>有 10 个值，其中 3 个值为真，其余 7 个值为假。所以，<em> A(x) =真</em>的概率是 0.3，<em> A(x) =假</em>的概率是 0.7。</p><p>对于<em> A =真</em>，其中的分数 0.3 沿分支分布，分数 0.7 沿另一分支分布。这些概率值用于计算信息增益，并且可以在需要测试第二个缺失属性值时使用。当我们需要为新的分支填充任何未知数时，同样的方法可以应用于学习的情况。C4.5 算法使用这种机制来填充缺失值。</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec68"/>构建决策树的注意事项</h3></div></div></div><p>构建决策树的关键是知道在哪里分割它们。为此，我们需要明确以下几点:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">从哪个属性开始，随后应用哪个属性？</li><li class="listitem" style="list-style-type: disc">我们什么时候停止构建决策树(即避免过度拟合)？</li></ul></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec18"/>选择合适的属性</h4></div></div></div><p>有三种不同的方法来识别最适合的属性:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">信息增益和熵</li><li class="listitem" style="list-style-type: disc">基尼指数</li><li class="listitem" style="list-style-type: disc">增益比</li></ul></div><div><div><div><div><h5 class="title"><a id="ch05lvl5sec07"/>信息增益和熵</h5></div></div></div><p>这个实体用于被称为<a id="id669" class="indexterm"/> C4.5 的<a id="id667" class="indexterm"/>算法<a id="id668" class="indexterm"/>中。熵是数据中不确定性的度量。让我们用直观的方法来理解信息增益和熵的概念。</p><p>例如，假设正在投掷一枚硬币，有五枚硬币正面的概率分别为 0、0.25、0.5、0.75 和 1。所以，如果我们认为哪一个具有最高的不确定性，哪一个具有最低的不确定性，那么 0 或 1 的情况将是最低的确定性，当它是 0.5 时最高。下图描述了相同的表示:</p><div><img src="img/B03980_05_07.jpg" alt="Information gain and Entropy"/></div><p>数学上的<a id="id670" class="indexterm"/>表示<a id="id671" class="indexterm"/>是这里显示的<a id="id672" class="indexterm"/>:</p><p>H = -∑p <sub> i </sub> log2p <sub> i </sub></p><p>这里，p <sub> i </sub>是特定状态的概率。</p><p>如果一个系统有四个概率为 1/2、1/4、1/5 和 1/8 的事件，则表示该系统的总熵，如下所示:</p><p>h =-1/2 log2(1/2)-1/4 log2(1/4)-1/5 log2(1/5)-1/8 log2(1/8)</p><p>在 C5.0 和 C4.5 算法(ID3)的原始版本中，根节点的选择基于如果选择该节点，总熵减少多少。这就是所谓的信息增益。</p><p>信息增益=拆分前系统的熵-拆分后系统的熵</p><p>拆分前系统中的熵如下所示:</p><div><img src="img/B03980_05_08.jpg" alt="Information gain and Entropy"/></div><p>使用<em> A </em>将<em> D </em>分割成<em> v </em>分区对<em> D </em>进行分类后的熵:</p><div><img src="img/B03980_05_09.jpg" alt="Information gain and Entropy"/></div><p>通过对属性进行分支<a id="id675" class="indexterm"/>得到的<a id="id674" class="indexterm"/>信息<a id="id673" class="indexterm"/>为:</p><p>现在让我们计算从数据中获得的信息:</p><div><img src="img/B03980_05_10.jpg" alt="Information gain and Entropy"/></div><p>P 类<a id="id676" class="indexterm"/>接受<a id="id677" class="indexterm"/>贷款=是/ 1。N 类接受贷款=否/ 0</p><p>分割前的熵如下:</p><div><img src="img/B03980_05_11.jpg" alt="Information gain and Entropy"/></div><p>这是显而易见的，也是意料之中的，因为我们几乎有一半的数据。现在让我们看看哪个属性给出了最好的信息增益。</p><p>如果分割基于 CCAvg 和系列，熵计算可以如下所示。总熵被加权为所创建的每个节点的熵的总和。</p><div><img src="img/B03980_05_12.jpg" alt="Information gain and Entropy"/></div><p>分割后的<a id="id679" class="indexterm"/>熵<a id="id680" class="indexterm"/>如下图<a id="id681" class="indexterm"/>:</p><div><img src="img/B03980_05_13.jpg" alt="Information gain and Entropy"/></div><p>信息增益如下:</p><div><img src="img/B03980_05_14.jpg" alt="Information gain and Entropy"/></div><p>该方法<a id="id682" class="indexterm"/>应用于<a id="id683" class="indexterm"/>计算所有其他属性的信息增益。它选择具有最高信息增益的一个。这在每个节点进行测试，以选择最佳节点。</p></div><div><div><div><div><h5 class="title"><a id="ch05lvl5sec08"/>基尼指数</h5></div></div></div><p>基尼指数<a id="id684" class="indexterm"/>是一个<a id="id685" class="indexterm"/>通用的划分标准。它以意大利统计学家和经济学家科拉多·基尼的名字命名。基尼指数用于衡量两个随机项目属于同一类别的概率。在真实数据集的情况下，这个概率值是 1。节点的基尼系数是类别比例的平方和。一个节点有两个类，每个类的得分为<em> 0.52 + 0.52 = 0.5 </em>。这是因为随机挑选同一个类的概率是 1/2。现在，如果我们对数据集应用基尼指数，我们得到以下结果:</p><p>原始基尼系数= <img src="img/B03980_05_15.jpg" alt="Gini index"/> = 0.502959</p><p>当与 CCAvg 和家庭分开时，基尼系数变化如下:</p><div><img src="img/B03980_05_16.jpg" alt="Gini index"/></div></div><div><div><div><div><h5 class="title"><a id="ch05lvl5sec09"/>增益比</h5></div></div></div><p>C4.5 相比 ID3 的另一个<a id="id686" class="indexterm"/>改进是决定属性的因素是增益比。增益比是信息增益与信息量的比值。给出最大增益比的属性是用于分割它的属性。</p><p>让我们用一个非常简单的例子来做一些计算，以强调为什么增益比是一个比信息增益更好的属性:</p><div><img src="img/B03980_05_17.jpg" alt="Gain ratio"/></div><p>因变量是他们是否在特定情况下结婚。我们假设在这种情况下，没有男人结婚。然而，除了最后一名妇女(60 名妇女)，所有妇女都已结婚。</p><p>因此，直觉上，规则必须如下:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">如果是男的，那他就是未婚</li><li class="listitem" style="list-style-type: disc">如果是一个女人，那么她已经结婚了(她没有结婚的唯一孤立的例子一定是噪音)。</li></ul></div><p>让我们系统地解决这个问题，以深入了解各种参数。首先，让我们将数据分成两半，作为训练和测试数据。因此，我们的训练集由最后 20 名男性(所有昏迷且年龄在 21-40 岁之间)和最后 30 名女性(所有已婚且年龄在 71-99 岁之间，除了最后一名)组成。测试包含所有女人都结婚的另一半。</p><p>增益比要求测量<strong>信息内容</strong>。</p><p>信息内容定义为<em>-f<sub>I</sub>log<sub>2</sub>f<sub>I</sub>T17】。注意，这里我们没有考虑因变量的值。我们只想知道一个州的成员数除以总成员数的分数。</em></p><p>性别的信息内容是它只有两种状态；男 20，女 30。因此，性别的信息量为<em> 2/5*LOG(2/5，2)-3/5*LOG(3/5，2)=0.9709 </em>。</p><p><a id="id688" class="indexterm"/>信息<a id="id689" class="indexterm"/>年龄的内容是总共有 49 个州为年龄。对于只有一个数据点的状态，信息内容为<em> -(1/50)*log(1/50，2) = 0.1129 </em>。</p><p>一个数据点有 48 个这样的状态。所以，它们的信息量是(0.1129*48)，5.4192。在最后一个状态中，有两个数据点。所以，它的信息量是<em> -(2/50 * LOG(2/50，2)) = 0.1857 </em>。该年龄的总信息量是 5.6039。</p><p>性别的增益比=性别的信息增益/性别的信息内容= 0.8549/0.9709 = 0.8805。</p><p>年龄的增益比= 0.1680</p><p>所以，如果我们考虑增益比，我们得到性别是一个更合适的衡量标准。这与直觉一致。现在假设我们使用了增益比并构建了树。我们的规则是，如果性别是男性，这个人就是未婚，如果是女性，这个人就是已婚。</p></div></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec19"/>终止准则/修剪决策树</h4></div></div></div><p>每个分支都<a id="id690" class="indexterm"/>长得足够深<a id="id691" class="indexterm"/>，可以通过决策树算法完美地对训练样本进行分类。这可能是一种可以接受的方法，当数据中有一些噪声时，大多数情况下会产生问题。如果训练数据集太小，不能代表实际数据集的真实情况，决策树可能会过度拟合训练示例。</p><p>有许多方法可以避免决策树学习中的过拟合。以下是两种不同的情况:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">一种情况是，在完成训练数据的完美分类之前，决策树因增长方式而终止</li><li class="listitem" style="list-style-type: disc">另一种情况是完成数据的过度拟合，然后修剪树以进行恢复</li></ul></div><p>虽然第一种情况可能看起来更直接，但第二种情况对过度拟合的树进行后期修剪在现实中更成功。原因是很难知道什么时候停止种树。无论采用何种方法，更重要的是确定标准，以确定最终合适的树大小。</p><p>以下是找到正确树大小的几种方法:</p><div><ol class="orderedlist arabic"><li class="listitem">识别与要使用的目标训练数据集不同的单独数据集，并评估树中修剪后节点的正确性。这是一种常见的方法，称为训练和验证集方法。</li><li class="listitem">代替<a id="id693" class="indexterm"/>的<a id="id692" class="indexterm"/>在训练集中具有数据的<a id="id694" class="indexterm"/>子集，用尽训练集中的所有数据，并且应用概率方法来检查修剪特定节点是否有任何可能性产生超过训练数据集的任何改进。使用所有可用的数据进行训练。例如，卡方检验可以用来检查这个概率。</li></ol></div><p>减少错误修剪(D):我们通过删除以节点为根的子树来在节点处进行修剪。我们使该节点成为一个叶子(带有相关例子的多数标签)；算法如下所示:</p><div><img src="img/B03980_05_18.jpg" alt="Termination Criteria / Pruning Decision trees"/></div><p>规则<a id="id695" class="indexterm"/>后剪枝是一种<a id="id696" class="indexterm"/>多<a id="id697" class="indexterm"/>常用的方法，是一种高度精确的假设技术。C4.5 中使用了这种修剪方法的一种变体。</p><p>以下是规则后清理过程的步骤:</p><div><ol class="orderedlist arabic"><li class="listitem">通过增长训练集来构建决策树，直到出现明显的过度拟合。</li><li class="listitem">从构建的决策树中生成规则，每条路径从根节点开始，到映射到规则的特定叶节点。</li><li class="listitem">对每个规则应用剪枝，以移除已识别的前提条件，并帮助提高概率准确性。</li><li class="listitem">接下来，在随后的实例中，按照准确性增加的顺序使用修剪后的规则。</li></ol></div><p>以下是基于规则的修剪的优点及其转换为规则的需要:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">提高规则的可读性</li><li class="listitem" style="list-style-type: disc">可以在根级和叶级节点上进行一致性测试</li><li class="listitem" style="list-style-type: disc">有一个明确的决定，要么删除决策节点，要么保留它</li></ul></div></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec69"/>以图形表示的决策树</h3></div></div></div><p>到目前为止，我们已经看到了<a id="id698" class="indexterm"/>决策树是如何通过划分节点上的数据并将值与常数进行比较来描述的。表示决策树的另一种方式是可视化和图形化表示。例如，我们可以在二维空间中选择两个输入属性，然后将一个属性的值与常量进行比较，并将数据的分割显示在平行轴上。我们也可以用属性的线性组合来比较两个属性，而不是平行于轴的超平面。</p><div><img src="img/B03980_05_19.jpg" alt="Decision trees in a graphical representation"/></div><p>为给定数据构建多个决策树是可能的。识别最小且完美的树的过程称为最小一致假设。让我们用两个论点来看看为什么这是最好的决策树:</p><p>奥卡姆剃刀<a id="id699" class="indexterm"/>简单；当有两种方法来解决一个问题并且都给出相同的结果时，以最简单的方法为准。</p><p>在数据挖掘分析中，人们很容易陷入复杂方法和大量计算的陷阱。因此，将奥卡姆所采用的推理路线内在化是至关重要的。总是选择一个决策树，它具有大小和错误的最佳组合。</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec70"/>归纳决策树——决策树算法</h3></div></div></div><p>有很多<a id="id700" class="indexterm"/>决策树归纳方法。在所有的方法中，C4.5 和 CART 是<a id="id701" class="indexterm"/>采用最多或最流行的方法。在本节中，我们将深入讨论这些方法，并简要介绍其他方法。</p><div><div><div><div><h4 class="title"><a id="ch05lvl4sec20"/>推车</h4></div></div></div><p>CART <a id="id702" class="indexterm"/>代表分类<a id="id703" class="indexterm"/>和回归树(Breiman 等人，1984)。CART 创建二叉树。这意味着总有两个分支可以从一个给定的节点出现。CART 算法的哲学是遵循一个<em>良好</em>标准，这是关于选择最好的可能分区。此外，随着树的增长，采用了成本复杂性修剪机制。CART 使用基尼指数来选择适当的属性或分割标准。</p><p>使用 CART，可以提供先验概率分布。我们可以使用 CART 生成回归树，反过来帮助预测一个类的实数。预测是通过应用节点的加权平均值来完成的。CART 识别最小化预测平方误差(即最小平方偏差)的分割。</p><p>下图中的 CART 描述与上一节中引用的示例相同，其中演示了决策树的构建:</p><div><img src="img/B03980_05_20.jpg" alt="CART"/></div></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec21"/> C4.5</h4></div></div></div><p>与 CART 类似，C4.5 <a id="id704" class="indexterm"/>是一种决策树算法，主要的<a id="id705" class="indexterm"/>区别是它可以生成比二叉树更多的树，这意味着支持多路分裂。对于属性选择，C4.5 使用信息增益度量。如前一节所述，具有最大信息增益(或最低熵减)值的属性有助于用最少的数据量实现更接近准确的分类。C4.5 的一个主要缺点是需要大量内存和 CPU 来生成规则。C5.0 算法是 1997 年推出的 C4.5 的商业版本。</p><p>C4.5 是 ID3 算法的发展。增益比测量用于识别分裂标准。当分割数量达到作为阈值的边界条件定义时，分割过程停止。在树的这个生长阶段之后，修剪就完成了，并且遵循基于错误的修剪方法。</p><p>下面是构建决策树的 C4.5 方法的一个示例，该示例与上一节中使用的示例相同:</p><div><img src="img/B03980_05_21.jpg" alt="C4.5"/></div><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>树形归纳法</p>
</th><th style="text-align: left" valign="bottom">
<p>它是如何工作的？</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>ID3</p>
</td><td style="text-align: left" valign="top">
<p><a id="id706" class="indexterm"/> ID3 ( <strong>迭代二分法 3 </strong>)算法被认为是决策树算法中最简单的<a id="id707" class="indexterm"/>。采用信息增益法作为分裂准则；分裂一直进行到最佳信息增益不大于零。ID3 没有特定的修剪。它不能处理数字属性和缺失值。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>CHAID</p>
</td><td style="text-align: left" valign="top">
<p><strong> CHAID </strong> ( <strong>卡方自动交互检测</strong>)被构建为仅支持<a id="id708" class="indexterm"/>名义属性。对于每个属性，以最接近目标属性的方式选择值<a id="id709" class="indexterm"/>。根据区分该算法的目标属性的类型，还有一个额外的统计度量。</p>
<p>连续目标属性使用 f 检验，名义目标属性使用皮尔逊卡方检验，顺序目标属性使用似然比检验。CHAID 检查可具有阈值的合并条件，并移动到下一个合并检查。重复这个过程，直到没有找到匹配对。</p>
<p>CHAID 以一种简单的方式处理缺失值，它基于所有值都属于一个有效类别的假设。在此过程中不进行修剪。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>探索</p>
</td><td style="text-align: left" valign="top">
<p>首字母缩写<a id="id710" class="indexterm"/> QUEST 代表快速、公正、高效和统计树。</p>
<p>这个算法<a id="id711" class="indexterm"/>支持单变量和线性组合分裂。根据属性的类型，使用 ANOVA F-test 或 Pearson 的卡方或双均值聚类方法来计算每个输入属性和目标属性之间的关系。拆分应用于与目标属性有更强关联的属性。为了确保达到最佳分裂点，应用<strong>二次判别分析</strong> ( <strong> QDA </strong>)。同样，QUEST 实现了二叉树，并使用 10 重交叉验证进行修剪。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>CAL5</p>
</td><td style="text-align: left" valign="top">
<p>这个<a id="id713" class="indexterm"/>与数字属性<a id="id714" class="indexterm"/>一起工作。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>事实</p>
</td><td style="text-align: left" valign="top">
<p>这个<a id="id715" class="indexterm"/>算法是 QUEST 的一个早期版本，它使用<a id="id716" class="indexterm"/>统计方法，然后使用判别分析进行属性选择。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LMDT</p>
</td><td style="text-align: left" valign="top">
<p>这使用了<a id="id717" class="indexterm"/>多元测试机制来构建<a id="id718" class="indexterm"/>决策树。</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>火星</p>
</td><td style="text-align: left" valign="top">
<p>使用<a id="id720" class="indexterm"/>线性样条及其张量积近似多重<a id="id719" class="indexterm"/>回归函数。</p>
</td></tr></tbody></table></div></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec71"/>贪婪的决策树</h3></div></div></div><p><a id="id721" class="indexterm"/>决策树的一个重要特征是<em>贪婪！贪婪算法的目标是通过在每个阶段获得局部最优来获得全局最优解。虽然不总是保证全局最优，但是局部最优有助于最大程度地实现全局最优。</em></p><p>每个节点都被贪婪地搜索以达到局部最优，并且陷入局部最优的可能性很高。大多数时候，以局部最优为目标可能有助于提供足够好的解决方案。</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec72"/>决策树的好处</h3></div></div></div><p>下面列出了使用<a id="id722" class="indexterm"/>决策树的一些优势:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">决策树构建起来既快又容易，而且几乎不需要实验</li><li class="listitem" style="list-style-type: disc">他们很健壮</li><li class="listitem" style="list-style-type: disc">它们很容易理解和解释</li><li class="listitem" style="list-style-type: disc">决策树不需要复杂的数据准备</li><li class="listitem" style="list-style-type: disc">他们可以处理分类数据和数字数据</li><li class="listitem" style="list-style-type: disc">它们支持使用统计模型进行验证</li><li class="listitem" style="list-style-type: disc">它们可以处理高维数据，也可以操作大型数据集</li></ul></div></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec73"/>特化树木</h2></div></div></div><p>在本节中，我们将探讨一些<a id="id723" class="indexterm"/>我们面临的重要特殊情况和特殊类型的决策树。这些在解决特殊类型的问题时变得很方便。</p><div><div><div><div><h3 class="title"><a id="ch05lvl3sec73"/>倾斜的树木</h3></div></div></div><p>倾斜树用于数据极其复杂的<a id="id724" class="indexterm"/>情况。如果属性<a id="id725" class="indexterm"/>是<em> x1、x2 和 x3…xn </em>，那么 C4.5 和 CART 测试标准为<em> x1 &gt;某个值</em>或<em> x2 &lt;某个其他值</em>，等等。这种情况下的目标是在每个节点找到一个要测试的属性。这些是图形平行轴分割，如下图所示:</p><div><img src="img/B03980_05_22.jpg" alt="Oblique trees"/></div><p>显然，我们需要建造巨大的树。至此，我们来学习一个数据挖掘行话，叫做超平面。</p><p>在一个<strong> 1 D </strong>问题中，一个点对空间进行分类。在<strong> 2 D </strong>中，一条线(直线或曲线)对空间进行分类。在 3 D 问题中，一个平面(直线或曲线)对空间进行分类。在更高维度的空间中，我们想象一个像东西一样的平面对空间进行分割分类，称之为<strong>超平面</strong>。这就是下图所示的<a id="id726" class="indexterm"/>:</p><div><img src="img/B03980_05_23.jpg" alt="Oblique trees"/></div><p>因此，传统的决策树<a id="id727" class="indexterm"/>算法会产生轴平行超平面来分割数据。如果数据很复杂，这可能会很麻烦。如果我们可以构造斜面，可解释性可能会下降，但我们可能会大大减少树的大小。因此，我们的想法是从以下方面改变测试条件:</p><p>xi &gt; K 或&lt; K to a1x1+ a2x2+ … + c &gt; K 或&lt; K</p><p>These oblique hyperplanes can at times drastically reduce the length of the tree. The same data shown in figure 2 is classified using oblique planes in the figure here:</p><div><img src="img/B03980_05_24.jpg" alt="Oblique trees"/></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec74"/>随机森林</h3></div></div></div><p>这些特殊化的树<a id="id728" class="indexterm"/>在维度过多的时候使用。我们已经在机器学习介绍章节中学习了<a id="id729" class="indexterm"/>关于维数灾难的知识。维数灾难的基本前提是高维数据带来了复杂性。尺寸和特征越多，出错的可能性也越高。在我们深入研究随机森林之前，让我们了解一下 Boosting 的概念。关于 boosting 方法的更多细节将在<a class="link" href="ch13.html" title="Chapter 13. Ensemble learning">第 13 章</a>、<em>集合学习</em>中介绍。在随机森林的情况下，boosting 的应用是关于如何将单个树方法结合在一起，以查看关于准确性的结果的提升。</p><div><img src="img/B03980_05_25.jpg" alt="Random forests"/></div><p>随机森林通过包含更多数量的决策树来扩展决策树。这些决策树是由随机选择的数据(样本)和随机选择的属性子集组合而成的。下图描述了随机选择数据集来构建每个决策树:</p><div><img src="img/B03980_05_26.jpg" alt="Random forests"/></div><p>制作多个决策树所需的另一个变量输入<a id="id730" class="indexterm"/>是属性的随机<a id="id731" class="indexterm"/>子集，如下图所示:</p><div><img src="img/B03980_05_27.jpg" alt="Random forests"/></div><p>因为每棵树都是使用随机数据集和随机变量集构建的，所以这些树被称为随机树。此外，许多这样的随机树定义了一个随机森林。</p><p>随机<a id="id732" class="indexterm"/>树的结果基于两个激进的信念。一个是每棵树<a id="id733" class="indexterm"/>对数据的最大部分进行精确预测。第二，错误发生在不同的地方。因此，平均而言，在决策树中进行一次结果投票以得出一个结果。</p><p>没有足够的观测值来获得好的估计，这导致了稀疏性问题。空间密度呈指数增长有两个重要原因，一是维数的增加，二是数据中等距点的增加。大多数数据都在尾部。</p><p>为了估计给定精度的密度，下表显示了样本大小如何随着维度的增加而增加。随后的计算表显示了多元正态分布估计的均方误差是如何随着维数的增加而增加的(正如 Silverman 所证明的，并通过此处给出的公式进行计算):</p><div><img src="img/B03980_05_31.jpg" alt="Random forests"/></div><div><img src="img/B03980_05_28.jpg" alt="Random forests"/></div><p>随机森林是决策树的一个重要扩展，非常容易理解并且非常有效，尤其是在处理高维空间的时候。当原始数据有多个维度时，我们随机选取维度(列)的一个小子集<a id="id735" class="indexterm"/>并构建一棵树。我们让它一路生长，没有修剪。现在，我们重复这个过程，每次用不同的属性集构建数百棵树。</p><p>对于预测，一个新的样本被推下树。训练样本的新标签被分配给它结束的终端节点。对组中的所有树重复该过程，并且所有树的平均投票被报告为随机森林预测。</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec75"/>进化树</h3></div></div></div><p>当达到全局<a id="id736" class="indexterm"/>最优看起来几乎不可能时，使用进化树<a id="id737" class="indexterm"/>。如你所知，决策树是贪婪的。所以有时候，我们可能会构建更大的树，只是因为我们陷入了局部最优。所以，如果你的树太长，试试斜树或者进化树。</p><p>进化树的概念源于一个非常令人兴奋的概念，叫做遗传算法。你将在另一门课程中详细了解它。让我们只看本质。</p><p>进化树不是用数学方法计算每个节点的最佳属性，而是在每个点随机选择一个节点并创建一棵树。然后它迭代并创建一个树的集合(森林)。现在，它为数据识别森林中最好的树。然后，它通过随机组合这些树来创建下一代森林。</p><p>另一方面，进化树选择了一个完全不同的顶端节点，并产生了一个更短的树，这具有相同的效率。进化算法需要更多的时间来计算。</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec76"/>海灵格树</h3></div></div></div><p>有人试图确定<a id="id738" class="indexterm"/>杂质指标，这些指标对因变量值的<a id="id739" class="indexterm"/>分布的敏感度低于熵或基尼指数。最近的一篇论文建议将海灵格距离作为不依赖于目标变量分布的杂质度量。</p><div><img src="img/B03980_05_29.jpg" alt="Hellinger trees"/></div><p>本质上，<em> P(Y+|X) </em>是找到每个属性的<em> Y+ </em>的概率，类似地，计算每个属性的<em> P(Y-|X) </em>。</p><div><img src="img/B03980_05_30.jpg" alt="Hellinger trees"/></div><p>从之前的图像来看，对于第一个属性的<strong>高</strong>值，只有第二个属性的<strong>高</strong>值导致概率值为 1。这将总距离值带到<em> sqrt(2) </em>。</p></div></div></div></div>





<title>Implementing Decision trees</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch05lvl1sec32"/>实施决策树</h1></div></div></div><p>参见本章提供的<a id="id740" class="indexterm"/>实现决策树和随机森林的源代码(该技术的每个文件夹下的源代码路径<code class="literal">.../chapter5/...</code>)。</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec74"/>使用 Mahout</h2></div></div></div><p>参见<a id="id741" class="indexterm"/>文件夹<code class="literal">.../mahout/chapter5/decisiontreeexample/</code>。</p><p>参见<a id="id742" class="indexterm"/>文件夹<code class="literal">.../mahout/chapter5/randomforestexample/</code>。</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec75"/>使用 R</h2></div></div></div><p>参见<a id="id743" class="indexterm"/>文件夹<code class="literal">.../r/chapter5/decisiontreeexample/</code>。</p><p>参见<a id="id744" class="indexterm"/>文件夹<code class="literal">.../r/chapter5/randomforestexample/</code>。</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec76"/>使用火花</h2></div></div></div><p>参见<a id="id745" class="indexterm"/>文件夹<code class="literal">.../spark/chapter5/decisiontreeexample/</code>。</p><p>参见<a id="id746" class="indexterm"/>文件夹<code class="literal">.../spark/chapter5/randomforestexample/</code>。</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec77"/>使用 Python (scikit-learn)</h2></div></div></div><p>参见<a id="id747" class="indexterm"/>文件夹<code class="literal">.../python scikit-learn/chapter5/decisiontreeexample/</code>。</p><p>参见<a id="id748" class="indexterm"/>文件夹<code class="literal">.../python scikit-learn/chapter5/randomforestexample/</code>。</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec78"/>利用朱丽亚</h2></div></div></div><p>参见<a id="id749" class="indexterm"/>文件夹<code class="literal">.../julia/chapter5/decisiontreeexample/</code>。</p><p>参见<a id="id750" class="indexterm"/>文件夹<code class="literal">.../julia/chapter5/randomforestexample/</code>。</p></div></div>





<title>Summary</title><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/><div><div><div><div><h1 class="title"><a id="ch05lvl1sec33"/>总结</h1></div></div></div><p>在本章中，您学习了使用决策树来解决分类和回归问题的监督学习技术。我们还讨论了选择属性、分割树和修剪树的方法。在所有其他决策树算法中，我们研究了 CART 和 C4.5 算法。对于一个特殊的需求或问题，您还学习了如何使用 Spark、R 和 Julia 的 MLib 实现基于决策树的模型。在下一章中，我们将讨论<strong>最近邻</strong>和<strong> SVM </strong> ( <strong>支持向量机</strong>)来解决监督和非监督学习问题。</p></div>
</body></html>