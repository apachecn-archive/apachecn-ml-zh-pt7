<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Neural Networks</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">神经网络</h1>
                
            
            
                
<p>就在十年前，人工神经网络被大多数研究人员认为是计算机科学的一个没有前途的分支。但随着计算能力的增长，以及在GPU上训练神经网络的有效算法的发现，情况发生了巨大变化。该领域的最新发现取得了前所未有的成果，比如在视频中追踪物体；合成现实的演讲、绘画和音乐，从一种语言到另一种语言的自动翻译；以及从文本、图像和视频中提取意义。神经网络被重新命名为<em>深度学习</em>，它们在计算机视觉和自然语言处理方面创造了各种记录，在过去几年(2014-2018年)中击败了几乎所有其他的人工智能方法。深度神经网络引起了新的机器学习热潮，引发了一波关于人工通用智能即将到来的讨论和预测。</p>
<p>现在，已经有如此多的神经网络类型，以至于很难跟踪它们:卷积、递归、递归、自动编码器、生成对抗、二元、有记忆、有注意力等等。新的架构和应用程序几乎每周都在出现，这要归功于世界各地不断增长的爱好者社区，他们尝试使用神经网络，将它们应用于各种可能的任务。</p>
<p>以下是NNs目前做得或多或少成功的一个简短列表:</p>
<ul>
<li>给黑白照片上色</li>
<li>绘制新的口袋妖怪</li>
<li>写广告脚本</li>
<li>诊断癌细胞</li>
</ul>
<p>由于深度学习的突破，我们几乎可以说(尽管还没有大声说)我们的计算机现在可以幻想、做梦和产生幻觉。今天，研究人员正在研究神经网络，它们可以自行设计和训练其他神经网络，编写计算机程序，帮助理解细胞内过程，破译被遗忘的文字和海豚的语言。从这一章开始，我们将开始投入深度学习。</p>
<p>在本章中，我们将讨论以下主题:</p>
<ul>
<li>什么是NNs，神经元，层，激活函数？</li>
<li>有哪些类型的激活功能？</li>
<li>如何训练神经网络:反向传播，随机梯度下降</li>
<li>什么是深度学习？</li>
<li>哪些深度学习框架最适合iOS应用？</li>
<li>实现多层感知器，以及如何训练它。</li>
</ul>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>What are artificial NNs anyway?</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">什么是人工神经网络？</h1>
                
            
            
                
<p>我们称之为人工神经网络的模型组是通用近似机；换句话说，这些函数可以模仿任何其他感兴趣的函数的行为。这里，我指的是数学意义上的函数，与计算机科学相反:接受实值输入向量并返回实值输出向量的函数。这个定义适用于前馈神经网络，我们将在本章中讨论。在接下来的章节中，我们将看到将输入张量(多维数组)映射到输出张量的网络，以及将自己的输出作为输入的网络。</p>
<p>我们可以把神经网络看作一个图，把神经元看作一个有向无环图中的一个节点。每个这样的节点接受一些输入并产生一些输出。现代神经网络只是受到生物大脑的松散启发。如果你想知道更多关于生物原型及其与神经网络的关系，查看<em>查看生物类比</em>部分。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Building the neuron</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">构建神经元</h1>
                
            
            
                
<p>考虑到一个生物神经元具有惊人复杂的结构(见<em>图8.1 </em>)，我们如何在程序中建模呢？实际上，可以说，这种复杂性大部分存在于硬件层面。我们可以把它抽象出来，把神经元想象成图中的一个节点，它接受一个或多个输入，产生一些输出(有时称为<em>激活</em>)。</p>
<p>等等，但这听起来不是很熟悉吗？是的，你是对的:人工神经元只是一个数学函数。</p>
<p>对神经元建模的最常见方式是通过非线性函数<em> f </em>使用输入的加权和:</p>
<div><img class="fm-editor-equation" height="19" src="img/a67d2b20-0ce2-4735-9482-91cb4e2ee8e7.png" width="95"/></div>
<p>其中<em> w </em>是权重向量，<em> x </em>是输入向量，<em> b </em>是偏置项。y是神经元的标量输出。</p>
<div><img height="472" src="img/c5241fed-f70d-4448-84ac-f81dafceb018.png" width="472"/></div>
<p>图8.1:脊椎动物的典型运动神经元。来自Wikimedia Commons的公共领域图</p>
<div><img height="203" src="img/90198b78-8b49-450f-9c31-fa3f0c5a1709.png" width="311"/></div>
<p>图8.2:人工神经元图</p>
<p>典型的人工神经元通过以下三个步骤处理输入，如前图所示(<em>图8.2 </em>):</p>
<ol>
<li>取输入的加权和。每个神经元都有一个权重向量，其长度与输入的数量相同。有时，引入一个额外的权重作为偏差项(总是等于1)。输入的加权和是输入向量和权重向量的点积。</li>
<li>通过非线性函数传递结果(同义词:激活函数、传递函数)。</li>
<li>将计算结果向下游传递给下一个神经元。</li>
</ol>
<p>第一步是非常熟悉的线性回归。如果激活是一个阶跃函数，这使得单个神经元在数学上等同于二元线性分类器。如果你用逻辑函数代替阶跃函数，你将得到逻辑回归。但是现在我们称它们为神经元，并且可以将它们组合成一个网络。</p>
<p>当神经元的输入权重调整到使整个神经元产生更好的输出时，就发生了神经元的学习。同样，这与线性回归相同。为了训练一个神经网络，我们通常使用反向传播算法，这是建立在熟悉的梯度下降。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Non-linearity function</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">非线性函数</h1>
                
            
            
                
<p>激活函数将神经元的加权输入映射为实数值，以产生神经元的输出。NN的许多特性取决于激活函数的选择，包括其泛化能力和训练过程收敛的速度。通常，我们希望它是可微的，所以我们可以使用梯度下降来优化整个网络。最常用的激活函数是非线性的:分段线性，或s形(见<em>表8.1 </em>)。非线性激活函数允许神经网络在仅使用少量神经元的许多重要任务中胜过其他算法。简单来说，激活功能可以分为两类:阶跃式和整流器式(参见<em>图8.3 </em>)。让我们仔细看看一些例子:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>名称</strong></p>
</td>
<td>
<p><strong>公式</strong></p>
</td>
<td>
<p><strong>衍生</strong></p>
</td>
</tr>
<tr>
<td>
<p>阶跃函数</p>
</td>
<td>
<p><img class="fm-editor-equation" height="29" src="img/de302304-bd5b-4fe2-a194-bd84b6131821.png" width="96"/></p>
</td>
<td>
<p><img class="fm-editor-equation" height="29" src="img/7b855105-fe52-4544-a8f6-51e5ec2c7a02.png" width="100"/></p>
</td>
</tr>
<tr>
<td>
<p>物流的</p>
</td>
<td>
<p><img class="fm-editor-equation" height="31" src="img/1c67be5b-7cce-48f5-aab1-fe71f5f91c38.png" width="94"/></p>
</td>
<td>
<p><img class="fm-editor-equation" height="15" src="img/76ee5fe6-0689-4807-9de7-5db5925ae71c.png" width="119"/></p>
</td>
</tr>
<tr>
<td>
<p>双曲正切</p>
</td>
<td>
<p><img class="fm-editor-equation" height="29" src="img/192ad35b-1654-42ca-a0e3-03791abe2dcf.png" width="111"/></p>
</td>
<td>
<p><img class="fm-editor-equation" height="17" src="img/b618dcc0-9bb2-48ce-ae66-195c5e5df8c1.png" width="101"/></p>
</td>
</tr>
<tr>
<td>
<p>热卢</p>
</td>
<td>
<p><img class="fm-editor-equation" height="31" src="img/e23391cb-ce69-44a0-973c-bf016dd36c23.png" width="103"/></p>
</td>
<td>
<p><img class="fm-editor-equation" height="31" src="img/5d4756b5-3fbb-4b11-8361-8981bc3842f3.png" width="107"/></p>
</td>
</tr>
<tr>
<td>
<p>泄漏ReLU</p>
</td>
<td>
<p><img class="fm-editor-equation" height="33" src="img/d354df91-65c6-465d-8802-95c06d044a7c.png" width="118"/></p>
</td>
<td>
<p><img class="fm-editor-equation" height="33" src="img/7961fdd5-651e-4cf4-a5c9-4ab1c72317d1.png" width="115"/></p>
</td>
</tr>
<tr>
<td>
<p>Softplus</p>
</td>
<td>
<p><img class="fm-editor-equation" height="18" src="img/6e746d90-f7bb-4e70-8d67-b56443a3d445.png" width="115"/></p>
</td>
<td><div> <img class="fm-editor-equation" height="31" src="img/8322d860-dd07-459b-9e3b-daa1bc4e4337.png" width="99"/> </div></td>
</tr>
<tr>
<td>
<p>最大输出</p>
</td>
<td>
<p><img height="34" src="img/8c7cb1f0-40e5-432e-b5f3-f82488ce9a6f.png" width="102"/></p>
</td>
<td>
<p><img height="46" src="img/fad56601-3d9c-49f4-92db-ab8bd91abf9c.png" width="125"/></p>
</td>
</tr>
</tbody>
</table>
<p>表8.1:常用的激活功能</p>
<div><img height="453" src="img/e597a123-7360-4a9a-8026-2283e2d2b777.png" width="453"/></div>
<p>图8.3:常见激活函数的曲线图:左边一列为阶跃函数，右边一列为整流器函数</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Step-like activation functions</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">阶梯式激活函数</h1>
                
            
            
                
<p>heaviside阶跃函数(也称为<strong>单位阶跃函数</strong>或<strong>阈值函数</strong>)为所有小于<em>零</em>的值输出<em> 0 </em>，为所有其他值输出<em> 1 </em>。这是模拟生物神经元的自然选择，它产生电脉冲，或者保持沉默:<em> 0 </em>。不幸的是，由于在<em> 0 </em>处的不连续性，函数是不可微的，这使得无法使用梯度下降算法来训练这样的网络。这种网络中的每个单个神经元都是二元线性分类器的数学等价物，因此这种网络不能很好地执行非线性任务。</p>
<p>一个<strong>逻辑(sigmoid)函数</strong>是一个阶跃函数的连续近似。该功能将输入从范围(-∞，+∞)压缩到范围(<em> 0 </em>，<em> 1 </em>)。它允许使用梯度下降来训练神经网络，但是它也有两个问题:</p>
<ul>
<li>由于sigmoid的形状，使用它的nn容易出现<strong>消失梯度问题</strong>，这将在后面解释(参见<em>消失梯度问题</em>部分)。</li>
<li>sigmoid的输出不是以零为中心的。这在训练期间引入了权重值的不期望的曲折行为，并且网络通常训练较慢。</li>
</ul>
<p>利用sigmoid激活功能，每个神经元本质上执行逻辑回归。</p>
<p><strong>双曲正切</strong> ( <strong> tanh </strong>)是一个比例对数函数，所以函数的形状非常相似但其输出的范围是(<em> -1 </em>，<em> 1 </em>)。这意味着双曲正切函数仍然受到梯度消失的影响，但至少它的输出现在是零中心的。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Rectifier-like activation functions</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">类似整流器的激活功能</h1>
                
            
            
                
<p>整流器是分段线性函数，在NNs上下文之外很难见到。这类功能是专门为减轻传统的步进式激活功能的问题和局限性而设计的。整流器应用简单的阈值:<kbd>max(0, x)</kbd>。一个神经元使用一个整流器被称为<strong>整流线性单元</strong> ( <strong> ReLU </strong>)。</p>
<p>与sigmoids不同，整流器不会在上端饱和。这有助于神经元区分差的预测和非常差的预测，并在如此困难的情况下相应地更新权重。ReLU在计算上也非常便宜:与需要指数运算的sigmoids不同，ReLU可以作为阈值运算来实现。研究还表明，ReLU的网络收敛速度比使用sigmoids的网络快六倍，因此ReLU在发明后很快在深度学习社区中受到欢迎。</p>
<p>ReLU有它自己的缺点，所以提出了一些修改来解决它们:</p>
<ul>
<li><strong> Leaky ReLU </strong>:不是所有值都是<em> 0 </em>，而是小于<em> 0 </em>并且该激活返回输入的一小部分(例如<em> 0.01 </em>)。分数的大小由常数α决定。据推测，这应该可以防止ReLU在底端饱和，但实际操作中通常帮助不大。</li>
<li><strong>随机化ReLU </strong> : α在某些界限内是随机的。随机化是NN正则化的一种常用方法，我们将在本章后面看到。</li>
<li><strong>参数ReLU (PReLU) </strong> : α为可训练参数，通过梯度下降进行调整。</li>
<li><strong> Softplus </strong>:使用指数的ReLU近似值。这个函数的导数是sigmoid。</li>
<li><strong> Maxout单元</strong>:将ReLU和leaky ReLU组合在一个表达式中。这样，它允许maxout单元具有所有ReLU优点，即线性度，没有饱和，但没有垂死的ReLU问题。这里的缺点是maxout单元的参数数量是ReLU的两倍，所以计算起来更昂贵。</li>
</ul>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Building the network</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">构建网络</h1>
                
            
            
                
<p>单个神经元可以组织成一个网络(见<em>图8.4 </em>)，通常是将几个神经元并联在一层，然后一层层堆叠起来。这样的网络被称为<strong>前馈神经网络</strong>或<strong>多层感知器(MLP) </strong>。第一层是输入层，最后一层是输出层，所有内层称为<em>隐藏层</em>。如果一层的每个神经元都连接到下一层的所有神经元，这样的网络称为<strong>全连接NN </strong>。</p>
<p>具有一种激活类型(通常是sigmoid)的全连接前馈多层感知器是传统(规范)类型的NN。它主要用于分类目的。在以下章节中，我们将讨论其他类型的神经网络，但在本章中，我们将坚持MLP:</p>
<div><img height="183" src="img/6b74f59b-3305-4e86-a7cb-50606a3c31ab.png" width="282"/></div>
<p>图8.4:五层全连接前馈神经网络</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Building a neural layer in Swift</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">在Swift中构建神经层</h1>
                
            
            
                
<p>全连接层很容易实现，因为它可以表示为两个操作:</p>
<ul>
<li>权重矩阵<em> W </em>和输入向量<em> x. </em>之间的矩阵乘法</li>
<li>激活功能<em> f </em>的逐点应用:</li>
</ul>
<div><img class="fm-editor-equation" height="18" src="img/3e7a3e6a-eb8a-4a64-8193-a2d0f6a7f221.png" width="99"/></div>
<div><img height="244" src="img/25ba9d56-9b30-430d-887f-c9d022b26a46.png" width="290"/></div>
<p>图8.5:一层的细节</p>
<p>在许多框架中，这两个操作是分开的，因此矩阵乘法发生在全连接层，而激活发生在下一个非线性层。这很方便，因为这样我们可以很容易地用卷积代替加权和。在下一章，我们将讨论卷积神经网络。</p>
<p>但是现在，让我们看看NNs如何执行逻辑运算。一个神经元足以模拟任何逻辑门，除了XOR。这一发现导致了20世纪60年代的第一次人工智能冬天；然而，XOR对于具有两层网络的模型来说是微不足道的。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Using neurons to build logical functions</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">使用神经元构建逻辑功能</h1>
                
            
            
                
<p>在iOS和macOS SDK的其他模糊部分中，有一个有趣的库叫做SIMD。它是直接访问向量指令和向量类型的接口，向量指令和向量类型直接映射到CPU中的向量单元，无需编写汇编代码。从2.0版本开始，您可以直接从Swift代码中引用该标题中定义的向量和矩阵类型以及线性代数运算符。</p>
<p><strong>通用逼近</strong>定理指出，如果找到合适的权重，具有一个隐藏层的简单NN可以逼近各种各样的连续函数。这也通常被重新表述为NNs，即通用函数逼近器。然而，这个定理并没有告诉我们是否有可能找到这样合适的权重。</p>
<p>要访问这些好东西，您需要在Swift文件中使用<kbd>import simd</kbd>，或者在C/C++/Objective-C文件中使用<kbd>#include &lt;simd/simd.h&gt;</kbd>。GPU中也有SIMD单位，所以你也可以将SIMD导入到你的金属着色器代码中。</p>
<p>根据iOS 10.3/Xcode 8.2.1，C SIMD的部分功能在Swift版本中不可用；例如，逻辑和三角运算。要查看它们，创建Objective-C文件，<kbd>#import &lt;simd/simd.h&gt;</kbd>并点击<em>命令</em>，点击<kbd>simd.h</kbd>转到头文件。</p>
<p>我最喜欢SIMD的部分是，其中所有的向量和矩阵都明确地将大小作为其类型的一部分。例如，函数<kbd>float 4()</kbd>返回大小为4 x 4的矩阵。但是这也使得SIMD不灵活，因为只有大小从2到4的矩阵可用。</p>
<p>看看SIMD游乐场的一些SIMD用法的例子:</p>
<pre>let firstVector = float4(1.0, 2.0, 3.0, 4.0) 
let secondVector = firstVector 
let dotProduct = dot(firstVector, secondVector) </pre>
<p>结果如下:</p>
<div><img height="220" src="img/a826b794-94e8-4fda-913a-002e4e4474b1.png" width="282"/></div>
<p>图8.6:实现异或功能的神经网络</p>
<p>为了说明SIMD可用于ML算法，让我们在SIMD实现一个简单的XOR NN:</p>
<pre>func xor(_ a: Bool, _ b: Bool) -&gt; Bool { 
    let input = float2(Float(a), Float(b)) 
 
    let weights1 = float2(1.0, 1.0) 
    let weights2 = float2(-1.0, -1.0) 
     
    let matrixOfWeights1 = float2x2([weights1, weights2]) 
    let weightedSums = input * matrixOfWeights1 
 
    let stepLayer = float2(0.5, -1.5) 
     
    let secondLayerOutput = step(weightedSums, edge: stepLayer) 
     
    let weights3 = float2(1.0, 1.0) 
    let outputStep: Float = 1.5 
     
    let weightedSum3 = reduce_add(secondLayerOutput * weights3) 
     
    let result = weightedSum3 &gt; outputStep 
    return result 
} </pre>
<p>SIMD的好处是，它明确告诉CPU一步计算点积，而不是循环遍历向量，而是利用SIMD指令。</p>
<p> </p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Implementing layers in Swift</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">在Swift中实施层</h1>
                
            
            
                
<p>当您想要在Swift中实施NN时，至少有三个选项可供考虑:</p>
<ul>
<li>在纯Swift中实现它(这可能主要用于研究目的)。在GitHub上可以找到许多不同复杂性和功能的实现。看起来好像每个程序员在她/他生命的某个阶段开始用她/他最喜欢的编程语言编写一个神经网络库。</li>
<li>使用低级加速库(金属性能着色器或bnn)来实现它。</li>
<li>使用一些通用NN框架实现它—Keras、TensorFlow、PyTorch等等—然后将其转换为核心ML格式。</li>
</ul>
<p>金属性能着色器库包括三种类型的NNs激活:ReLU、sigmoid和TanH ( <kbd>MPSCNNNeuronReLU</kbd>、<kbd>MPSCNNNeuronSigmoid</kbd>、<kbd>MPSCNNNeuronTanH</kbd>)。更多信息请参考:<a href="https://developer.apple.com/reference/metalperformanceshaders" target="_blank">https://developer . apple . com/reference/metalperformanceshaders</a>。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Training the network</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">训练网络</h1>
                
            
            
                
<p>目前训练神经网络最常见的方法是误差反向传播算法，或反向传播(通常简称为<em>反向传播</em>)。正如我们已经看到的，单个神经元经常让我们想起线性或逻辑回归，所以反向传播通常与我们的老朋友梯度下降算法一起出现也就不足为奇了。NN培训的工作方式如下:</p>
<ul>
<li>正向传递-输入显示在图层上，变换逐层应用到图层上，直到预测在最后一个图层上输出。</li>
<li>损失计算-将预测与地面真实值进行比较，并使用损失函数<em> J </em>为输出层的每个神经元计算误差值。</li>
<li>然后，误差被向后传播(反向传播)，使得每个神经元具有与其相关联的误差，该误差与其对输出的贡献成比例。</li>
<li>使用梯度下降的一个步骤来更新权重(<em> w) </em>。使用误差值针对每个神经元的权重计算损失函数<img class="fm-editor-equation" height="18" src="img/00d91c0f-78c1-463e-8f9b-240cec4bd380.png" width="12"/>的梯度。然后，通常的梯度下降步骤发生在线性回归中。</li>
</ul>
<p>只有当正向传播中的所有变换都是可微的(在最简单的情况下，点积和激活函数)时，反向传播才是可能的，因为它本质上是来自微积分的链规则的应用。</p>
<p>如需进一步阅读，请转至<a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank">https://en.wikipedia.org/wiki/back传播</a>。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Vanishing gradient problem</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">消失梯度问题</h1>
                
            
            
                
<p>sigmoid在一端渐近地接近零，而在另一端是<em> 1 </em>。在这些尾部，函数的导数非常小。这对反向传播算法来说是个坏消息，因为当信号通过网络传播回来更新权重时，这些几乎为零的值会杀死信号。</p>
<p>死神经元的问题是:如果你随机初始化网络权重，权重大的sigmoidal神经元从一开始就死了(几乎不传输信号)。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Seeing biological analogies</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">看到生物类比</h1>
                
            
            
                
<p>每个人都听说过人工神经网络模仿大脑的工作方式。这实际上与事实相差甚远。真实的情况是，神经网络作为一个领域是从模拟大脑如何工作的尝试中发展出来的。大脑的基本单位是神经元(神经细胞)。人脑包含大约860亿个神经元。神经元可以在其体内产生电位(动作电位)。神经元有两种类型的分支投射。一种是短投影，称为树突(来自希腊语<em> δεvεδpov </em>，tree)。通常，它们的功能是接收来自其他神经元的电脉冲。另一种类型是更长的投射，称为轴突(来自希腊语αξοv，axis)。有些神经元没有轴突，但没有神经元有一个以上的轴突。轴突的功能是将电脉冲从神经元传递到其他细胞。</p>
<p>通过其轴突，神经元连接到其他神经元的体(或树突)，并向它们传输电信号。不是所有的神经元都向其他神经元传递信号；其中一些刺激肌肉和腺体。</p>
<p>轴突末端有称为突触的结构——连接细胞体或树突。为了向下一个神经元传递信号，突触会发出化学神经介质(或很少发出电信号)。人脑中大约有1014-1015个突触。你可以自己计算存储如此大量的信息需要多少磁盘空间。在我们的人造大脑中，人造神经元要少得多。即使是最大的现代神经网络也只能与水母或蜗牛的大脑相提并论。然而，神经元或突触的数量并不是全部，因为有些动物的大脑比人类的大脑包含更多的神经元。如果你对这个话题感兴趣，请访问维基百科页面:<a href="https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons" target="_blank">https://en . Wikipedia . org/wiki/List _ of _ animals _ by _ number _ of _ neurons</a>。</p>
<p>尽管神经网络的概念是从生物学中借用的，但人们应该有强大的想象力才能看出生物原型与现代人工神经网络是如何相似的。这就是为什么一些研究人员认为一些其他的名字，像<em>计算图</em>更合适。生物学术语曾经在这个领域更受欢迎，但现在字面上唯一广泛使用的生物学术语是<em>神经元</em>。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Basic neural network subroutines (BNNS)</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">基本神经网络子程序</h1>
                
            
            
                
<p>BNNS是Accelerate的子模块，包含为在CPU上运行推理而优化的卷积NN原语。它是在iOS 10和macOS 10.12中引入的。请注意，它只包含用于推理的函数，不包含用于训练的函数。</p>
<p>这个库背后的动机是为公共例程提供统一的API，这样应用程序开发人员就不需要每次都从头开始重新实现卷积和其他原语(这很难，我们已经在CNN一章中看到过)。在典型的CNN中，大部分能量都花在卷积层上。完全连接的层在计算上更昂贵，但通常CNN在最末端包含一个或几个层，所以卷积仍然消耗大约70%的能量。这就是拥有高度优化的卷积层非常重要的原因。与MPS不同，CNN可以在iOS、macOS、tvOS甚至watchOS上使用。因此，如果你想在电视机或手表上运行深度学习(只是因为你可以)，这是你的首选工具。</p>
<p>更严肃地说，当你为没有金属性能着色器支持的设备实现NNS时，BNNS是有用的(旧的iOS设备和现在所有的macOS设备)。在所有其他情况下，您仍然希望使用MPS CNNs来利用GPU大规模并行性。</p>
<p>要检查金属功能的可用性，请查看<a href="https://developer.apple.com/metal/availability/" target="_blank">https://developer.apple.com/metal/availability/</a>。</p>
<p>BNNS包含三种类型的层:卷积、池化和全连接层，以及几种激活:同一性、校正线性、泄漏校正线性、sigmoid、tanh、缩放tanh和abs。</p>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>BNNS example</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">BNNS示例</h1>
                
            
            
                
<p>在以下示例中，输入图像的大小为224 x 224 x 64，输出图像的大小为222 x 222 x 96。卷积权重的维数是3×3×64×96。也就是54.5亿次浮点运算(gigaFloPS)。在整个MNIST识别网络中，每次转发大约有1-2万亿次运算。</p>
<p>BNNS是Accelerate的一部分，因此您需要导入Accelerate来访问神经网络构建块。您要做的第一件事是描述输入堆栈:</p>
<pre>var inputStack = BNNSImageStackDescriptor(<br/>  width: 224, height: 224, channels: 64,<br/>  row_stride: 224, image_stride: 224*224,<br/>  data_type: BNNSDataTypeFloat32,<br/>  data_scale: 1.0, data_bias: 0.0)</pre>
<p>大多数参数是不言自明的；<kbd>row_stride</kbd>是到下一行的像素增量，<kbd>image_stride</kbd>类似地是到下一通道的像素增量，<kbd>data_type</kbd>是一种存储类型。</p>
<p>输出堆栈看起来应该类似:</p>
<pre>var outputStack = BNNSImageStackDescriptor(<br/>  width: 1, height: 10, channels: 1,<br/>  row_stride: 1, image_stride: 10,<br/>  data_type: BNNSDataTypeFloat32,<br/>  data_scale: 1.0, data_bias: 0.0)</pre>
<p>现在让我们创建一个卷积层。<kbd>BNNSConvolutionLayerParameters</kbd>包含卷积层的描述:</p>
<pre>let activation = BNNSActivation(function: BNNSActivationFunctionIdentity, alpha: 0, beta: 0)<br/><br/>var convolutionParameters = BNNSConvolutionLayerParameters(<br/>  x_stride: 1, y_stride: 1,<br/>  x_padding: 0, y_padding: 0,<br/>  k_width: 3, k_height: 3,<br/>  in_channels: 64, out_channels: 96,<br/>  weights: convolutionWeights,<br/>  bias: convolutionBias,<br/>  activation: activation)</pre>
<p><kbd>k_width</kbd>和<kbd>k_height</kbd>分别是籽粒宽度和高度。</p>
<p>创建层本身:</p>
<pre>let convolutionLayer = BNNSFilterCreateConvolutionLayer(&amp;inputStack, &amp;outputStack, &amp;convolutionParameters, nil)</pre>
<p><kbd>nil</kbd>代表默认<kbd>BNNSFilterParameters</kbd>。</p>
<p>现在你可以使用过滤器，并在不再需要时通过调用<kbd>BNNSFilterDestroy(convolutionLayer)</kbd>将其销毁。</p>
<p>池层:</p>
<pre>// Describe pooling layer<br/>BNNSPoolingLayerParameters pool = {<br/>    .k_width = 3,<br/>// kernel height<br/>// kernel width<br/>// X padding<br/>// Y padding<br/>    .k_height = 3,<br/>    .x_padding = 1,<br/>    .y_padding = 1,<br/>    .x_stride = 2,<br/>    .y_stride = 2,<br/>    .in_channels = 64,<br/>    .out_channels = 64,<br/>    .pooling_function = BNNSPoolingFunctionMax  // pooling function<br/>};<br/>// Create pooling layer filter<br/>BNNSFilter filter = BNNSFilterCreatePoolingLayer(<br/>    &amp;in_stack,     // BNNSImageStackDescriptor for input stack<br/>    &amp;out_stack,    // BNNSImageStackDescriptor for output stack<br/>    &amp;pool,         // BNNSPoolingLayerParameters<br/>    NULL);         // BNNSFilterParameters (NULL = defaults)<br/>// Use the filter ...<br/>// Destroy filter<br/>BNNSFilterDestroy(filter);<br/><br/>// Describe input vector<br/>BNNSVectorDescriptor in_vec = {<br/>    .size = 3000,<br/>// size<br/>// storage type<br/>};<br/>// Describe fully connected layer<br/>BNNSFullyConnectedLayerParameters full = {<br/>    .in_size = 3000,<br/>    .out_size = 20000,<br/>    .weights = {<br/>        .data_type = BNNSDataTypeFloat16,<br/>        .data = weights<br/>// input vector size<br/>// output vector size<br/>// weights storage type<br/>// pointer to weights data<br/>} };<br/>// Create fully connected layer filter<br/>BNNSFilter filter = BNNSFilterCreateFullyConnectedLayer(<br/>    &amp;in_vec,       // BNNSVectorDescriptor for input vector<br/>    &amp;out_vec,      // BNNSVectorDescriptor for output vector<br/>    &amp;full,         // BNNSFullyConnectedLayerParameters<br/>    NULL);         // BNNSFilterParameters (NULL = defaults)// Use the filter ...<br/>// Destroy filter<br/>BNNSFilterDestroy(filter);<br/>// Apply filter to one pair of (in,out)<br/>int status = BNNSFilterApply(filter,<br/>                             in,<br/>out);<br/>// BNNSFilter<br/>// pointer to input data<br/>// pointer to output data<br/>// Apply filter to N pairs of (in,out)<br/>int status = BNNSFilterApplyBatch(filter,<br/>                                  20,<br/>                                  in,<br/>                                  3000,<br/>                                  out,<br/>                                  20000);<br/>// BNNSFilter<br/>// batch size (N)<br/>// pointer to input data<br/>// input stride (values)<br/>// pointer to output data<br/>// output stride (values)</pre>


            

            
        
    </body>

</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">
    <head>
        <title>Summary</title>
        
        <meta charset="utf-8"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000005314597" name="Adept.expected.resource"/>
    </head>

    <body>
        

                            
                    <h1 class="header-title">摘要</h1>
                
            
            
                
<p>在这一章中，我们已经熟悉了人工神经网络及其主要组成部分。神经网络是由通常组织成层的神经元构成的。典型的神经元对输入进行加权求和，然后对其应用非线性激活函数来计算其输出。有许多不同的激活函数，但目前最流行的是ReLU及其修改，因为它们的计算属性。</p>
<p>通常使用基于随机梯度下降的反向传播算法来训练神经网络。多层前馈神经网络也称为多层感知器。MLP可用于分类任务。</p>
<p>在下一章，我们将继续讨论神经网络，但这次我们将集中讨论卷积神经网络，它在计算机视觉领域特别流行。</p>
<p class="mce-root"/>


            

            
        
    </body>

</html></body></html>