<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Applying Neural Style Transfer on Photos</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">在照片上应用神经风格转移</h1>

                

            

            

                

<p class="graf graf--p graf-after--figure">在这一章中，我们将构建一个完整的iOS和Android应用程序，其中图像转换以类似于Instagram应用程序的方式应用于我们自己的图像。对于这个应用程序，我们将在TensorFlow的帮助下再次使用Core ML和TensorFlow模型。为了使这个工作，我们将不得不执行一些小的黑客。</p>

<p class="graf graf--p graf-after--figure">本章的最佳用例是基于一个名为<strong> Prisma </strong>的照片编辑应用程序，通过它你可以使用神经网络将你的图像转换为绘画。你可以把你的图像转换成一种艺术形式，看起来像是毕加索或萨尔瓦多·达利画的。</p>

<p>在本章中，我们将讨论以下主题:</p>

<ul>

<li>艺术神经风格转移</li>

<li>使用神经类型转移构建应用程序</li>

</ul>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Artistic neural style transfer</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">艺术神经风格转移</h1>

                

            

            

                

<p>图像变换通过快速样式转换来应用。在实现我们的移动应用程序之前，让我们更深入地了解一下样式转换是如何工作的。每个人都喜欢看到自己的作品具有艺术风格。艺术神经风格转移帮助我们以一种艺术形式看到我们自己的图像，这种艺术形式涉及混合你的内容和风格，以便引入一种独特的视觉体验。在此之前，没有基于人工智能的系统能够实现这样的系统。</p>

<p>请看下面的截图，这是一个如何将艺术风格应用于普通图像的示例:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-412 image-border" src="img/6c690a4f-1485-4340-9b28-c65ee59e9849.png" style="width:30.83em;height:8.25em;"/></p>

<p>我们将在本章中创建的应用程序是基于一个类似于前面屏幕截图所示的系统的实现。已经有多篇论文发表，证明它在面部和物体识别方面的表现接近人类。深度神经网络有助于实现人工人类视觉。在这个应用中，我们使用的算法实现了一个深度神经网络，可以创建高视觉质量的图像。神经网络用于对输入图像的内容和风格进行分叉和重新排列。通过这种方式，它提供了一种创造艺术形象的神经逻辑。</p>

<p>这个项目基于以下论文:<a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank">https://arxiv.org/pdf/1508.06576.pdf</a>和这里的GitHub项目:<a href="https://github.com/lengstrom/fast-style-transfer" target="_blank">https://github.com/lengstrom/fast-style-transfer</a>。另一篇关于近似人类面部识别的论文可以在以下链接找到:<a href="https://ieeexplore.ieee.org/document/6909616/" target="_blank">https://ieeexplore.ieee.org/document/6909616/</a>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Background</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">背景</h1>

                

            

            

                

<p>在神经网络中，<strong>卷积神经网络</strong>(<strong>CNN</strong>)是一种被广泛用于图像分类、对象检测、面部识别等的主要技术。典型的CNN算法将图像作为数组格式的输入，并产生其分类作为输出，例如，256 * 256 * 3(高*宽*尺寸)的人的头像照片，RGB值的矩阵数组。这里的数字<em> 3 </em>指的是RGB值。</p>

<p>在典型的转移算法中，更多的焦点将被给予在对象识别上，并且CNN将被训练在对象识别上。在CNN处理层次中，当处理级别增加时，对象信息在更高层变得更加明确。每个处理层中的转换通过查找图像中的实际内容而不是计算详细的像素值来改进对象识别。在CNN的处理中，较高层总是表示所识别的对象及其排列，而较低层表示关于更深像素值的更多信息，以便再现图像。</p>

<p>如下所示，为了找出输入图像的风格，我们设计了一个特征空间来帮助我们捕捉纹理相关的信息。该空间基于网络每层中的滤波器响应，而多层包含有助于在不同尺度上构建输入图像的多种变化的特征相关性。这样，它捕捉纹理信息，但不涉及对象的全局排列:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-413 image-border" src="img/51503cd8-43aa-48d1-9e54-ce5db017742a.png" style="width:57.42em;height:33.50em;"/></p>

<p>在前面描述CNN流程的屏幕截图中，每个步骤都应用了过滤器。过滤器的数量随着过滤图像尺寸的减小而增加。大小的减小是通过下采样机制(如池化)实现的。在CNN层中，可以在特定阶段重建图像，这取决于网络的输入。该信息也可以在CNN的不同处理阶段被可视化。这是通过基于特定层中的网络响应重建输入图像来实现的。如前面的屏幕截图所示，可以在每一步重新创建内容，而不必等到最后一步。让我们仔细看看<strong>视觉几何组</strong> ( <strong> VGG </strong>)网络数据集。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>VGG network</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">VGG网络</h1>

                

            

            

                

<p>一个VGG网络可以检测给定输入图像中的1000个对象。这需要224 * 244 * 3大小的图像输入(数字<em> 3 </em>代表RGB值)。这是使用总共16层的3×3大小的卷积层构建的。它还提供了70.5%的前1名准确度和90%的前5名准确度。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Layers in the VGG network</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">VGG网络中的层</h1>

                

            

            

                

<p>VGG网络有16层，解释如下:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/f22877f4-4ef0-4950-8bfb-64b0d1748384.png" style="width:13.42em;height:39.83em;"/></p>

<p>前期培训的VGG TensorFlow模型链接可以在这里找到:<a href="https://www.cs.toronto.edu/~frossard/vgg16/vgg16_weights.npz" target="_blank">https://www . cs . Toronto . edu/~ frossard/vgg 16/vgg 16 _ weights . npz</a>。</p>

<p>我们可以用每一层上的样式特征输出所捕获的信息来可视化和构建图像。我们可以根据局部结构和颜色找到纹理化的图像。正如您在前面的图像中所看到的，本地化图像结构的复杂性和可视化程度随着层次的增加而增加，而像素清晰度却降低了。</p>

<p>神经风格转移表明，CNN中的风格和内容是可以分离的，允许我们操纵和产生有意义的输出。它使用的示例是一张照片的内容表示，该照片显示了取自不同艺术时期的几幅著名画作，如下面的屏幕截图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-414 image-border" src="img/5d0857f2-ff93-4ed8-be7a-e2f9588f13a6.png" style="width:45.50em;height:29.50em;"/></p>

<p>当然，图像内容和风格不能完全相互独立。当应用CNN时，一个图像的内容用另一个图像的风格处理，我们不会看到一个完美的图像同时满足两个约束。我们可以在内容和风格之间进行权衡，以创建吸引人的输出图像。</p>

<p>在我们的例子中，我们用著名的绘画作品来渲染照片。这种方法叫做<strong>非照片真实感渲染</strong>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building the applications</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">构建应用程序</h1>

                

            

            

                

<div><p style="font-size: 16px">接下来是本章的应用程序构建部分。在构建这个应用程序时，我们将在预先训练好的模型上使用快速风格转换。我们还可以使用经过调整的定制模型，使应用程序在iOS平台上工作。</p>

</div>

<p>下面是两个使用TensorFlow实现风格转换的例子:<a href="https://github.com/titu1994/Neural-Style-Transfer" target="_blank">https://github.com/titu1994/Neural-Style-Transfer</a>和<a href="https://github.com/yining1023/fast_style_transfer_in_ML5/" target="_blank">https://github.com/yining1023/fast_style_transfer_in_ML5/</a>。</p>

<div><p class="graf graf--p graf-after--p">因此，我们将在本教程中使用TensorFlow-to-Core ML库1.1.0+。GitHub链接如下:<a href="https://github.com/tf-coreml/tf-coreml" target="_blank">https://github.com/tf-coreml/tf-coreml</a>，依赖项如下:<a href="https://github.com/tf-coreml/tf-coreml" target="_blank"/></p>

<ul>

<li><kbd>tensorflow</kbd> &gt; = 1.5.0</li>

<li><kbd>coremltools</kbd> &gt; = 0.8</li>

<li><kbd>numpy</kbd> &gt; = 1.6.2</li>

<li><kbd>protobuf</kbd> &gt; = 3.1.0</li>

<li><kbd>six</kbd> &gt; = 1.10.0</li>

</ul>

</div>

<p>要获得TensorFlow-to-Core ML转换器的最新版本，请克隆此存储库并从源安装它，如下所示:</p>

<div><pre><strong>git clone https://github.com/tf-coreml/tf-coreml.git

cd tf-coreml </strong><br/><strong>pip install -e</strong> </pre></div>

<p>或者，您可以运行以下命令:</p>

<div><pre><strong>python setup.py bdist_wheel</strong></pre></div>

<p>要安装PyPI包，请运行以下命令:</p>

<div><pre><strong>pip install -U tfcoreml</strong></pre></div>

<p class="mce-root"/>

<p class="graf graf--p graf-after--h3">现在，让我们从初步步骤开始。这是必要的，因为快速样式转移并不意味着生产级应用:</p>

<ol>

<li class="graf graf--p graf-after--p">作为第一步，我们需要为我们的图找出输出节点的名称。TensorFlow自动生成这个，我们可以通过打印<kbd>evaluate.py</kbd>脚本中的<kbd>net</kbd>来获得它。我们使用下面的块来获取输出节点名:</li>

</ol>

<pre style="padding-left: 60px"># function ffwd,line 93

# https://github.com/lengstrom/fast-style-   transfer/blob/master/evaluate.py#L93<br/>preds = transform.net(img_placeholder)

# Printing the output node name

print(preds)</pre>

<ol start="2">

<li>完成这些后，我们可以运行脚本来查看打印输出。我们在这里使用预先训练好的<kbd>wave</kbd>模型。下面的代码块给出了输出节点名:</li>

</ol>

<pre class="graf graf--pre graf-after--p" style="padding-left: 60px">$ python evaluate.py --checkpoint wave.ckpt --in-path inputs/ --out-<br/>  path outputs/ <br/><br/>&gt; Tensor("add_37:0", shape=(20, 720, 884, 3), dtype=float32, <br/>  device=/device:GPU:0)</pre>

<p class="graf graf--p graf-after--pre" style="padding-left: 60px">输出节点名是这里的重要数据，是<kbd>add_37</kbd>。这是有意义的，因为网络中最后一个未命名的运算符是加法，如前面的代码块所示:</p>

<pre class="graf graf--p graf-after--pre" style="padding-left: 60px">#https://github.com/lengstrom/fast-style-transfer/blob/master/src/transform.py#L17<br/>preds = tf.nn.tanh(conv_t3) * 150 + 255./2</pre>

<ol start="3">

<li>让我们对<kbd>evaluate.py</kbd>再做一些修改，并将图形保存到磁盘:</li>

</ol>

<pre style="padding-left: 60px"># https://github.com/lengstrom/fast-style-transfer/blob/master/evaluate.py#L98

if os.path.isdir(checkpoint_dir):

    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)

    if ckpt and ckpt.model_checkpoint_path:

        saver.restore(sess, ckpt.model_checkpoint_path)

        ########## for pre-trained models ###########

        frozen_graph_def = <br/>   tf.graph_util.convert_variables_to_constants(sess,sess.graph_def, <br/>                                                ['add_37'])

        with open('output_graph.pb', 'wb') as f:

            f.write(frozen_graph_def.SerializeToString())

        #####################################################

  else:

    raise Exception("No checkpoint found...")

else:

       saver.restore(sess, checkpoint_dir)

       ########## for custom models ###########

       frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess,sess.graph_def,<br/>                                             ['add_37'])

       with open('output_graph.pb', 'wb') as f:

           f.write(frozen_graph_def.SerializeToString())

       #####################################################</pre>

<ol start="4">

<li class="graf graf--p graf-after--figure">现在，我们在一个模型上运行<kbd>evaluate.py</kbd>，我们将保存我们的图形文件:</li>

</ol>

<pre class="graf graf--pre graf-after--p" style="padding-left: 60px"><strong>$ python evaluate.py --checkpoint wave/wave.ckpt --in-path inputs/ - </strong><br/><strong>         -out-path outputs/ --device "/cpu:0" --batch-size 1</strong></pre>

<p class="graf graf--p graf-after--pre">我们将得到作为输出的<kbd>output_graph.pb</kbd>文件，然后我们可以继续进行核心的ML转换部分。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>TensorFlow-to-Core ML conversion</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">张量流至核心ML转换</h1>

                

            

            

                

<p>TensorFlow到CoreML的转换是使用<kbd>tf-coreml</kbd>库完成的。</p>

<p>点击以下链接查看张量流至堆芯ML转换器库:<a href="https://github.com/tf-coreml/tf-coreml" target="_blank">https://github.com/tf-coreml/tf-coreml</a>。</p>

<p>我们有一个将张量流模型转换为核心ML的三步过程。</p>

<ol>

<li class="graf graf--p graf-after--p">我们在上一步中生成的模型(<kbd>output_graph.pb</kbd>)需要支持<em>能力。</em>苹果的核心ML工具提供了一元转换，支持<em>幂</em>。我们需要将这段代码添加到TensorFlow实现中，如下所示:</li>

</ol>

<pre style="padding-left: 60px"># tfcoreml src



# file1 : _interpret_shapes.py



# in the _SHAPE_TRANSLATOR_REGISTRY we need to add the Pow operation

_SHAPE_TRANSLATOR_REGISTRY = {

     ... previous keys ...

    # add this:

    'Pow': _identity,

}





# file 2: _ops_to_layers.py



# in the _OP_REGISTRY to add the Pow operation

_OP_REGISTRY = {

     ... previous keys ...

    # add this:

    'Pow': _layers.pow

}





# file 3: _layers.py



# in the _layers we need to define the conversion

def pow(op, context):

    const_name = compat.as_bytes(op.inputs[1].name)

    const_val = context.consts[const_name]    <br/>## Note: this is .5 here, you can play around with this 

    input_name = compat.as_bytes(op.inputs[0].name)

    output_name = compat.as_bytes(op.outputs[0].name)

    context.builder.add_unary(output_name, input_name, output_name, <br/>                              'power', alpha=const_val)

    context.translated[output_name] = True</pre>

<ol start="2">

<li>创建并运行转换脚本:</li>

</ol>

<pre style="padding-left: 60px">import tfcoreml as tf_converter<br/>tf_converter.convert(tf_model_path = 'output_graph.pb',

                     mlmodel_path = 'model_name.mlmodel',

                     output_feature_names = ['add_37:0'],

                     image_input_names = ['img_placeholder__0'])  <br/><br/>$ python convert.py</pre>

<p>到目前为止，实际的Core ML转换器不提供从模型输出图像的能力。</p>

<ol start="3">

<li>在模型(<kbd>own_model.mlmodel</kbd>)上创建并运行输出转换脚本，它是前面的输出:</li>

</ol>

<pre style="padding-left: 60px">import coremltools



def convert_multiarray_output_to_image(spec, feature_name,   <br/>                                       is_bgr=False):  

    """  

    Convert an output multiarray to be represented as an image  

    This will modify the Model spec passed in.  

    """ <br/><br/><br/>    for output in spec.description.output: <br/>        if output.name != feature_name: <br/>            continue <br/>        if output.type.WhichOneof('Type') != 'multiArrayType': <br/>            raise ValueError("%s is not a multiarray type" % <br/>                              output.name) <br/>        array_shape = tuple(output.type.multiArrayType.shape) <br/>        channels, height, width = array_shape <br/>        from coremltools.proto import FeatureTypes_pb2 as ft <br/>        if channels == 1: <br/>            output.type.imageType.colorSpace = <br/>               ft.ImageFeatureType.ColorSpace.Value('GRAYSCALE') <br/>        elif channels == 3: <br/>            if is_bgr: <br/>                output.type.imageType.colorSpace = <br/>                    ft.ImageFeatureType.ColorSpace.Value('BGR') <br/>            else: <br/>                output.type.imageType.colorSpace = <br/>                    ft.ImageFeatureType.ColorSpace.Value('RGB') <br/>        else: <br/>            raise ValueError("Channel Value %d not supported for <br/>                              image inputs" % channels) <br/>        output.type.imageType.width = width <br/>        output.type.imageType.height = height <br/><br/><br/>model = coremltools.models.MLModel('own_model.mlmodel') <br/>spec = model.get_spec() <br/>convert_multiarray_output_to_image(spec,'add_37__0',is_bgr=False) <br/>newModel = coremltools.models.MLModel(spec) <br/>newModel.save('wave.mlmodel') </pre>

<p class="graf graf--pre graf-after--figure">现在运行以下代码:</p>

<pre class="graf graf--pre graf-after--figure"><strong>$ python output.py</strong></pre>

<p>现在我们有了自己的ML模型。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>iOS application</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">iOS应用程序</h1>

                

            

            

                

<p class="graf graf--p graf-after--h3">在iOS应用程序中，我们将在此介绍重要的细节:</p>

<ol>

<li>将模型导入到Xcode项目中。确保将它们添加到目标中。</li>

<li class="graf graf--p graf-after--p">导入后，您将能够像这样实例化模型:</li>

</ol>

<div><div><pre style="padding-left: 60px">private let models = [

    wave().model,

    udnie().model,

    rain_princess().model,

    la_muse().model

]</pre></div>

<ol start="3">

<li class="iframeContainer">为模型输入参数创建一个类，这个类是<kbd>MLFeatureProvider</kbd>。<kbd>img_placeholder</kbd>是评估脚本中定义的输入:</li>

</ol>

</div>

<div><div><div><pre style="padding-left: 60px">//  StyleTransferInput.swift

//  StyleTransfer



import CoreML



class StyleTransferInput : MLFeatureProvider {

    

    var input: CVPixelBuffer

    

    var featureNames: Set&lt;String&gt; {

        get {

            return ["img_placeholder__0"]

        }

    }

    

    func featureValue(for featureName: String) -&gt; MLFeatureValue? {

        if (featureName == "img_placeholder__0") {

            return MLFeatureValue(pixelBuffer: input)

        }

        return nil

    }

    

    init(input: CVPixelBuffer) {

        self.input = input

    }

}</pre></div>

</div>

</div>

<ol start="4">

<li class="graf graf--p graf-after--figure">现在，调用模型以获得所需的输出:</li>

</ol>

<div><div><div><pre style="padding-left: 60px">private func stylizeImage(cgImage: CGImage, model: MLModel) -&gt; CGImage {

    

    // size can change here if you want, remember to run right sizes <br/>       in the fst evaluating script

    let input = StyleTransferInput(input: pixelBuffer(cgImage: <br/>                               cgImage, width: 883, height: 720))



    // model.prediction will run the style model on input image

    let outFeatures = try! model.prediction(from: input)

  

    // we get the image buffer after

    let output = outFeatures.featureValue(for: <br/>                               "add_37__0")!.imageBufferValue!

  

    // remaining code to convert image buffer here .....

}</pre></div>

</div>

</div>

<p>您可以直接从GitHub资源库中提取代码，如下:<a href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter3" target="_blank">https://GitHub . com/intrepidkarthi/MLmobileapps/tree/master/chapter 3</a>和<a href="https://github.com/PacktPublishing/Machine-Learning-Projects-for-Mobile-Applications" target="_blank">https://GitHub . com/packt publishing/Machine-Learning-Projects-for-Mobile-Applications</a>。<a href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter3" target="_blank"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Android application</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">Android应用程序</h1>

                

            

            

                

<p>让我们开始使用TensorFlow模型构建一个Android应用程序。在这种情况下，我们将使用预先建立的模型从谷歌特色的风格转移。</p>

<p>该应用程序的基本功能将类似于在Instagram上应用过滤器。我们将使用相机拍摄一张照片，或者从移动画廊中选择一个文件，并在可用设计列表中的图像上应用艺术风格转移。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Setting up the model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">建立模型</h1>

                

            

            

                

<p>该模型是名为<strong> Magenta </strong>的TensorFlow研究项目的一部分。它主要涉及将<strong>机器学习</strong> ( <strong> ML </strong>)应用到音乐和艺术创作的过程中，这涉及到使用强化学习和深度学习开发新的算法，这些算法可以应用在音乐文件和图像上，构建将帮助艺术家和音乐家的工具。</p>

<p>张量流研究项目的资源库可以在这里找到:【https://github.com/tensorflow/magenta】T2。</p>

<p>如果您正在下载我们的存储库，它已经准备好了模型文件，那么您可以跳过设置模型这一节。</p>

<p>风格转移是内容图像和风格图像的组合产生结果仿作图像的过程。这在文森特·杜默林、乔恩·史伦斯和曼朱纳斯·库德鲁尔(<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://arxiv.org/abs/1610.07629" target="_blank">https://arxiv.org/abs/1610.07629</a>)的论文《艺术风格的学术代表中有详细讨论。</p>

<p>我们既可以使用现有的模型，也可以在我们的应用程序中建立自己的模型。为此，我们需要首先设置Magenta环境。使用自动化脚本在Mac上安装它很简单。如果您在另一个环境中设置，请检查Magenta项目中的手动安装细节。</p>

<p>在终端上运行以下命令以安装Magenta:</p>

<pre><strong>curl <br/>  https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/<br/>          too</strong><strong>ls/magenta-install.sh &gt; /tmp/magenta-install.sh<br/></strong><br/><strong>bash /tmp/magenta-install.sh</strong></pre>

<p>现在，打开一个新的终端窗口并运行以下命令:</p>

<pre class="mce-root"><strong>source activate magenta</strong></pre>

<p class="mce-root">我们现在可以使用洋红色了！</p>

<p class="mce-root">有两种预先训练好的模型可用。还是用那个叫<strong>莫奈</strong>的吧。</p>

<p>要下载这个，下面是模型的链接:<a href="http://download.magenta.tensorflow.org/models/multistyle-pastiche-generator-monet.ckpt" target="_blank">http://download . magenta . tensor flow . org/models/multi style-pastiche-generator-Monet . ckpt</a>。</p>

<p>运行以下命令:</p>

<pre><strong>$ image_stylization_transform \

      --num_styles=&lt;NUMBER_OF_STYLES&gt; \<br/>      </strong><strong>--which_styles="[0,1,2,5,14]" \<br/></strong><strong>      --checkpoint=/path/to/model.ckpt \ <br/>      --input_image=/path/to/image.jpg \ <br/>      --output_dir=/tmp/image_stylization/output \ <br/>      --output_basename="stylized"</strong></pre>

<p>您应该在参数中传递正确的型号。对于莫奈来说，这就是<kbd>10</kbd>。<kbd>which_styles</kbd> <em> </em>参数指定应用于单个图像的线性样式组合列表。以下是应用莫奈风格的示例:</p>

<pre><strong>$ image_stylization_transform \

      --num_styles=10 \

      --checkpoint=multistyle-pastiche-generator-monet.ckpt \</strong><br/><strong>      --which_styles=" <br/>                 {0:0.1,1:0.1,2:0.1,3:0.1,4:0.1,5:0.1,6:0.1,<br/>                  7:0.1,8:0.1,9:0.1}" \</strong><br/><strong>      --input_image=photo.jpg \ </strong><br/><strong>      --output_dir=/tmp/image_stylization/output \ </strong><br/><strong>      --output_basename="all_monet_styles"</strong></pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Training your own model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">训练你自己的模型</h1>

                

            

            

                

<p>你可以用你自己的风格图像训练你的模型。这可以分三步完成。</p>

<p>准备好自己风格的图片放在一个目录里，从这里下载训练好的VGG模型关卡:<a href="http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz" target="_blank">http://download . tensor flow . org/models/vgg _ 16 _ 2016 _ 08 _ 28 . tar . gz</a>:</p>

<pre>//Setting up your own images<br/><strong>$ image_stylization_create_dataset \</strong><br/><strong> --vgg_checkpoint=/path/to/vgg_16.ckpt \</strong><br/><strong> --style_files=/path/to/style/images/*.jpg \</strong><br/><strong> --output_file=/tmp/image_stylization/style_images.tfrecord</strong></pre>

<p>然后，您可以开始训练模型:</p>

<pre>//Training a model<br/><strong>$ image_stylization_train \</strong><br/><strong> --train_dir=/tmp/image_stylization/run1/train</strong><br/><strong> --style_dataset_file=/tmp/image_stylization/style_images.tfrecord \</strong><br/><strong> --num_styles=&lt;NUMBER_OF_STYLES&gt; \</strong><br/><strong> --vgg_checkpoint=/path/to/vgg_16.ckpt \</strong><br/><strong> --imagenet_data_dir=/path/to/imagenet-2012-tfrecord</strong></pre>

<p>之后，评估你的训练:</p>

<pre><strong>$ image_stylization_evaluate \</strong><br/><strong> --style_dataset_file=/tmp/image_stylization/style_images.tfrecord \</strong><br/><strong> --num_styles=&lt;NUMBER_OF_STYLES&gt; \</strong><br/><strong> --train_dir=/tmp/image_stylization/run1/train \</strong><br/><strong> --eval_dir=/tmp/image_stylization/run1/eval \</strong><br/><strong> --vgg_checkpoint=/path/to/vgg_16.ckpt \</strong><br/><strong> --imagenet_data_dir=/path/to/imagenet-2012-tfrecord \</strong><br/><strong> --style_grid</strong></pre>

<p>或者，如果您想要微调现有模型，请输入以下内容:</p>

<pre><strong>$ image_stylization_finetune \</strong><br/><strong> --checkpoint=/path/to/model.ckpt \</strong><br/><strong> --train_dir=/tmp/image_stylization/run2/train</strong><br/><strong> --style_dataset_file=/tmp/image_stylization/style_images.tfrecord \</strong><br/><strong> --num_styles=&lt;NUMBER_OF_STYLES&gt; \</strong><br/><strong> --vgg_checkpoint=/path/to/vgg_16.ckpt \</strong><br/><strong> --imagenet_data_dir=/path/to/imagenet-2012-tfrecord</strong></pre>

<p>将这些放在一起，我们剩下以下内容:</p>

<pre># Select an image (any jpg or png).

input_image = 'evaluation_image/hero.jpg'



image = np.expand_dims(image_utils.load_np_image(

                        os.path.expanduser(input_image)), 0)



checkpoint = 'checkpoints/multistyle-pastiche-generator-monet.ckpt'

              num_styles = 10  <br/># Number of images in checkpoint file. Do not change.

    

# Styles from checkpoint file to render. They are done in batch, so the # more rendered, the longer it will take and the more memory will be  <br/># used.These can be modified as you like. Here we randomly select six  <br/># styles.

styles = range(num_styles)

random.shuffle(styles)

which_styles = styles[0:6]

num_rendered = len(which_styles)  



with tf.Graph().as_default(), tf.Session() as sess:

    stylized_images = model.transform(

        tf.concat([image for _ in range(len(which_styles))], 0),

        normalizer_params={

            'labels': tf.constant(which_styles),

            'num_categories': num_styles,

            'center': True,

            'scale': True})

    model_saver = tf.train.Saver(tf.global_variables())

    model_saver.restore(sess, checkpoint)

    stylized_images = stylized_images.eval()

    

    # Plot the images.

    counter = 0

    num_cols = 3

    f, axarr = plt.subplots(num_rendered // num_cols, num_cols, <br/>                                            figsize=(25, 25))

    for col in range(num_cols):

        for row in range( num_rendered // num_cols):

            axarr[row, col].imshow(stylized_images[counter])

            axarr[row, col].set_xlabel('Style %i' % which_styles[counter])

            counter += 1</pre>

<p>现在，让我们开始构建Android应用程序。在本应用中，我们将使用TensorFlow模型，该模型使用Magenta项目中的以下网络代码构建:</p>

<pre>"""Style transfer network code."""



from __future__ import absolute_import

from __future__ import division

from __future__ import print_function





import tensorflow as tf



from magenta.models.image_stylization import ops



slim = tf.contrib.slim





def transform(input_, normalizer_fn=ops.conditional_instance_norm,

              normalizer_params=None, reuse=False):

  """Maps content images to stylized images.



  Args:

    input_: Tensor. Batch of input images.

    normalizer_fn: normalization layer function.  Defaults to

        ops.conditional_instance_norm.

    normalizer_params: dict of parameters to pass to the conditional <br/>                       instance

        normalization op.

    reuse: bool. Whether to reuse model parameters. Defaults to False.



  Returns:

    Tensor. The output of the transformer network.

  """

  if normalizer_params is None:

    normalizer_params = {'center': True, 'scale': True}

  with tf.variable_scope('transformer', reuse=reuse):

    with slim.arg_scope(

        [slim.conv2d],

        activation_fn=tf.nn.relu,

        normalizer_fn=normalizer_fn,

        normalizer_params=normalizer_params,

        weights_initializer=tf.random_normal_initializer(0.0, 0.01),

        biases_initializer=tf.constant_initializer(0.0)):

      with tf.variable_scope('contract'):

        h = conv2d(input_, 9, 1, 32, 'conv1')

        h = conv2d(h, 3, 2, 64, 'conv2')

        h = conv2d(h, 3, 2, 128, 'conv3')

      with tf.variable_scope('residual'):

        h = residual_block(h, 3, 'residual1')

        h = residual_block(h, 3, 'residual2')

        h = residual_block(h, 3, 'residual3')

        h = residual_block(h, 3, 'residual4')

        h = residual_block(h, 3, 'residual5')

      with tf.variable_scope('expand'):

        h = upsampling(h, 3, 2, 64, 'conv1')

        h = upsampling(h, 3, 2, 32, 'conv2')

        return upsampling(h, 9, 1, 3, 'conv3', activation_fn=tf.nn.sigmoid)</pre>

<p>让我们探索用镜像填充代替零填充的相同填充卷积。<kbd>conv2d</kbd>函数期望<kbd>'kernel_size'</kbd>为奇数:</p>

<pre>def conv2d(input_,<br/>           kernel_size,<br/>           stride,<br/>           num_outputs,<br/>           scope,<br/> activation_fn=tf.nn.relu):<br/> """<br/> Args:<br/> input_: 4-D Tensor input.<br/> kernel_size: int (odd-valued) representing the kernel size.<br/> stride: int representing the strides.<br/> num_outputs: int. Number of output feature maps.<br/> scope: str. Scope under which to operate.<br/> activation_fn: activation function.<br/> Returns:<br/> 4-D Tensor output.<br/> Raises:<br/> ValueError: if `kernel_size` is even.<br/> """<br/> if kernel_size % 2 == 0:<br/> raise ValueError('kernel_size is expected to be odd.')<br/> padding = kernel_size // 2<br/> padded_input = tf.pad(<br/> input_, [[0, 0], [padding, padding], [padding, padding], [0, 0]],<br/> mode='REFLECT')<br/> return slim.conv2d(<br/> padded_input,<br/> padding='VALID',<br/> kernel_size=kernel_size,<br/> stride=stride,<br/> num_outputs=num_outputs,<br/> activation_fn=activation_fn,<br/> scope=scope)</pre>

<p>现在，让我们来看一个相同填充的转置卷积的平滑替换。该函数首先通过因子<kbd>'stride'</kbd>计算输入的最近邻上采样，然后应用镜像填充、相同填充卷积。它期望<kbd>'kernel_size'</kbd>是奇数:</p>

<pre>def upsampling(input_,<br/>               kernel_size,<br/>               stride,<br/>               num_outputs,<br/>               scope,<br/> activation_fn=tf.nn.relu):<br/> """<br/> Args:<br/> input_: 4-D Tensor input.<br/> kernel_size: int (odd-valued) representing the kernel size.<br/> stride: int representing the strides.<br/> num_outputs: int. Number of output feature maps.<br/> scope: str. Scope under which to operate.<br/> activation_fn: activation function.<br/> Returns:<br/> 4-D Tensor output.<br/> Raises:<br/> ValueError: if `kernel_size` is even.<br/> """<br/> if kernel_size % 2 == 0:<br/> raise ValueError('kernel_size is expected to be odd.')<br/> with tf.variable_scope(scope):<br/> shape = tf.shape(input_)<br/> height = shape[1]<br/> width = shape[2]<br/> upsampled_input = tf.image.resize_nearest_neighbor(<br/> input_, [stride * height, stride * width])<br/> return conv2d(<br/> upsampled_input,<br/> kernel_size,<br/> 1,<br/> num_outputs,<br/> 'conv',<br/> activation_fn=activation_fn)</pre>

<p>由两个镜像填充、相同填充的卷积构成的剩余块。<kbd>residual_block</kbd>函数期望<kbd>'kernel_size'</kbd>为奇数:</p>

<pre>def residual_block(input_, kernel_size, scope, activation_fn=tf.nn.relu):<br/> """<br/> Args:<br/> input_: 4-D Tensor, the input.<br/> kernel_size: int (odd-valued) representing the kernel size.<br/> scope: str, scope under which to operate.<br/> activation_fn: activation function.<br/> Returns:<br/> 4-D Tensor, the output.<br/> Raises:<br/> ValueError: if `kernel_size` is even.<br/> """<br/> if kernel_size % 2 == 0:<br/> raise ValueError('kernel_size is expected to be odd.')<br/> with tf.variable_scope(scope):<br/> num_outputs = input_.get_shape()[-1].value<br/> h_1 = conv2d(input_, kernel_size, 1, num_outputs, 'conv1', activation_fn)<br/> h_2 = conv2d(h_1, kernel_size, 1, num_outputs, 'conv2', None)<br/> return input_ + h_2</pre>

<p>现在，我们可以开始构建Android应用程序了。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building the application</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">构建应用程序</h1>

                

            

            

                

<p>此应用程序使用TensorFlow的以下依赖项进行编译，这将获取最新版本:</p>

<pre>allprojects {<br/> repositories {<br/> jcenter()<br/> }<br/>}<br/>dependencies {<br/> compile 'org.tensorflow:tensorflow-android:+'<br/>}</pre>

<p>我们将创建一个<kbd>.CameraActivity</kbd>，它将是应用程序的启动器活动。这在<kbd>AndroidManifest.xml</kbd>文件中定义:</p>

<pre>&lt;activity android:name=".CameraActivity"<br/>    android:icon="@mipmap/ic_launcher"&gt;<br/>    &lt;intent-filter&gt;<br/>        &lt;action android:name="android.intent.action.MAIN" /&gt;<br/>        &lt;category android:name="android.intent.category.LAUNCHER" /&gt;<br/>    &lt;/intent-filter&gt;<br/>&lt;/activity&gt;</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Setting up the camera and an image picker </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">设置相机和图像拾取器</h1>

                

            

            

                

<p>在<kbd>.CameraActivity</kbd>中，我们将使用在以下链接找到的<kbd>CameraKit</kbd>库中的摄像头模块:<a href="https://github.com/CameraKit/camerakit-android" target="_blank">https://github.com/CameraKit/camerakit-android</a>。</p>

<p>为了开始捕捉图像，我们将从初始化<kbd>CameraView</kbd>对象开始:</p>

<pre>myCamera = findViewById(R.id.camera);<br/>myCamera.setPermissions(CameraKit.Constants.PERMISSIONS_PICTURE);<br/>myCamera.setMethod(CameraKit.Constants.METHOD_STILL);<br/>myCamera.setJpegQuality(70);<br/>myCamera.setCropOutput(true);</pre>

<p>当您使用相机拍摄照片时，回调方法将启动样式转换:</p>

<pre>findViewById(R.id.picture).setOnClickListener(new View.OnClickListener() {<br/>    @Override<br/>    public void onClick(View v) {<br/>        captureStartTime = System.currentTimeMillis();<br/>        mCameraView.captureImage(new <br/>                    CameraKitEventCallback&lt;CameraKitImage&gt;() {<br/>            @Override<br/>            public void callback(CameraKitImage cameraKitImage) {<br/>                byte[] jpeg = cameraKitImage.getJpeg();<br/><br/>                // Get the dimensions of the bitmap<br/>                BitmapFactory.Options bmOptions = new <br/>                                          BitmapFactory.Options();<br/><br/>                // Decode the image file into a Bitmap sized to fill <br/>                //  the View<br/>                //bmOptions.inJustDecodeBounds = false;<br/>                  bmOptions.inMutable = true;<br/><br/>                long callbackTime = System.currentTimeMillis();<br/>                Bitmap bitmap = BitmapFactory.decodeByteArray(jpeg, 0, <br/>                                jpeg.length, bmOptions);<br/>                ResultHolder.dispose();<br/>                ResultHolder.setImage(bitmap);<br/>                ResultHolder.setNativeCaptureSize(mCameraView.getCaptureSize());<br/>                ResultHolder.setTimeToCallback(callbackTime - <br/>                captureStartTime);<br/>                Intent intent = new Intent(getApplicationContext(), <br/>                                           ShowImageActivity.class);<br/>                startActivity(intent);<br/>            }<br/>        });<br/>    }<br/>});</pre>

<p>或者，您可以选择使用意向从移动图库中挑选图像:</p>

<pre>mFile = new File(getExternalFilesDir(null), "pic.jpg");<br/>Intent intent = new Intent();<br/>intent.setType("image/*");<br/>intent.setAction(Intent.ACTION_GET_CONTENT);<br/>startActivityForResult(Intent.createChooser(intent, "Select Picture"), <br/>                       1);</pre>

<p class="CDPAlignLeft CDPAlign">上述代码生成以下输出:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-784 image-border" src="img/e2583528-b8f5-4b15-a9bd-4cee778bfd13.png" style="width:21.92em;height:39.00em;"/></p>

<p>您将在<kbd>onActivityResult</kbd>方法中得到结果，如下所示:</p>

<pre>@Override<br/>public void onActivityResult(int requestCode, int resultCode, Intent data) {<br/>    super.onActivityResult(requestCode, resultCode, data);<br/>    if (requestCode == 1 &amp;&amp; resultCode == Activity.RESULT_OK) {<br/>        if (data == null) {<br/>            //Display an error<br/>            return;<br/>        }<br/>        try {<br/>            InputStream inputStream =   <br/>                 getContentResolver().openInputStream(data.getData());<br/><br/>            byte[] buffer = new byte[inputStream.available()];<br/>            inputStream.read(buffer);<br/><br/>            // Get the dimensions of the bitmap<br/>            BitmapFactory.Options bmOptions = new <br/>                          BitmapFactory.Options();<br/><br/>            // Decode the image file into a Bitmap sized to fill the <br/>            // View<br/>            //bmOptions.inJustDecodeBounds = false;<br/>            bmOptions.inMutable = true;<br/><br/>            Bitmap bitmap = BitmapFactory.decodeByteArray(buffer, 0,   <br/>                                          buffer.length, bmOptions);<br/>            ResultHolder.dispose();<br/>            ResultHolder.setImage(bitmap);<br/>            Intent intent = new Intent(getApplicationContext(), <br/>                                       ShowImageActivity.class);<br/>            startActivity(intent);<br/>        } catch (FileNotFoundException e) {<br/>            e.printStackTrace();<br/>        } catch (IOException e) {<br/>            e.printStackTrace();<br/>        }<br/>        //Now you can do whatever you want with your inpustream, save       <br/>          it as file, upload to a server, decode a bitmap...<br/>    }<br/>    finish();<br/>}</pre>

<p>在<kbd>.ShowImageActivity</kbd>中，内置了一个类似Instagram的用户界面，由此你可以从我们视图底部的横向列表中选择多种风格，并将其中一种应用到选中的图片上。这是用<kbd>RecyclerView</kbd>上的<kbd>HorizontalListAdapter</kbd>设置的。</p>

<p>从<kbd>Assets</kbd>文件夹加载每种风格的缩略图，如下所示:</p>

<pre>private void loadStyleBitmaps(){<br/>    for(int i=0;i&lt;NUM_STYLES;i++){<br/>        try{<br/>            myStylesBmList.add(i,BitmapFactory.decodeStream(getAssets().open("thumb<br/>                   nails/style"+i+".jpg")));<br/>        }<br/>        catch(IOException e){<br/>            e.printStackTrace();<br/>            Toast.makeText(ShowImageActivity.this,"Alert! there is an <br/>                issue while loading images",Toast.LENGTH_SHORT).show();<br/>            finish();<br/>        }<br/>    }<br/>}</pre>

<p>上述代码的输出如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-785 image-border" src="img/f86a487d-f53d-47ed-9b10-d87e00441549.png" style="width:20.50em;height:36.50em;"/></p>

<p>要在选取特定样式时将样式应用于图像，请输入以下内容:</p>

<pre>mRecyclerView.addOnItemTouchListener(new <br/>   RecyclerItemClickListener(getApplicationContext(),mRecyclerView,new    <br/>   RecyclerItemClickListener.OnItemClickListener(){<br/><br/>    @Override<br/>    public void onItemClick(View view, int position) {<br/>        mSelectedStyle = position;<br/>        progress = new ProgressDialog(ShowImageActivity.this);<br/>        progress.setTitle("Loading");<br/>        progress.setMessage("Applying your awesome style! Please <br/>                             wait!");<br/>        progress.setCancelable(false); // disable dismiss by tapping <br/>                                          outside of the dialog<br/>        progress.show();<br/>        runInBackground(<br/>                new Runnable() {<br/>                    @Override<br/>                    public void run() {<br/>                        try {<br/>                            stylizeImage();<br/>                        }<br/>                        catch(Exception e){<br/>                            e.printStackTrace();<br/>                            runOnUiThread(new Runnable() {<br/>                                @Override<br/>                                public void run() {<br/>                                    Toast.makeText(getApplicationContext(),"Oops! Some error occurred!",Toast.LENGTH_SHORT).show();<br/>                                    if(progress!=null){<br/>                                        progress.dismiss();<br/>                                    }<br/>                                }<br/>                            });<br/>                        }<br/>                    }<br/>                });<br/>    }<br/><br/>    @Override<br/>    public void onLongItemClick(View view, int position) {<br/>    }<br/>}));</pre>

<p>然后，调用样式方法来应用样式:</p>

<pre>private void stylizeImage() {<br/>    if(bitmapCache.get("style_"+String.valueOf(mSelectedStyle))==null) {<br/>        ActivityManager actManager = (ActivityManager) getApplication().getSystemService(Context.ACTIVITY_SERVICE);<br/>        ActivityManager.MemoryInfo memInfo = new <br/>                                  ActivityManager.MemoryInfo();<br/>                                  actManager.getMemoryInfo(memInfo);<br/><br/>        mImgBitmap = Bitmap.createBitmap(mOrigBitmap);<br/>        for (int i = 0; i &lt; NUM_STYLES; i++) {<br/>            if (i == mSelectedStyle) {<br/>                styleVals[i] = 1.0f;<br/>            } else styleVals[i] = 0.0f;<br/>        }<br/>        mImgBitmap.getPixels(intValues, 0, mImgBitmap.getWidth(), 0, 0,   <br/>                        mImgBitmap.getWidth(), mImgBitmap.getHeight());<br/><br/>        for(int i=0;i&lt;MY_DIVISOR;i++) {<br/>            float[] floatValuesInput = new  float[floatValues.length/MY_DIVISOR];<br/>            int myArrayLength = intValues.length/MY_DIVISOR;<br/>            for(int x=0;x &lt; myArrayLength;++x){<br/>                final int myPos = x+i*myArrayLength;<br/>                final int val = intValues[myPos];<br/>                floatValuesInput[x * 3] = ((val &gt;&gt; 16) &amp; 0xFF) / <br/>                                                         255.0f;<br/>                floatValuesInput[x * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / <br/>                                                         255.0f;<br/>                floatValuesInput[x * 3 + 2] = (val &amp; 0xFF) / 255.0f;<br/>            }<br/>            Log.i(ShowImageActivity.class.getName(),"Sending following data to tensorflow : floarValuesInput length : " + floatValuesInput.length+" image bitmap height :" + mImgBitmap.getHeight() + " image bitmap width : " + mImgBitmap.getWidth());<br/>            // Copy the input data into TensorFlow.<br/>            inferenceInterface.feed(<br/>                    INPUT_NODE, floatValuesInput, 1, mImgBitmap.getHeight()/MY_DIVISOR, mImgBitmap.getWidth(), 3);<br/>            inferenceInterface.feed(STYLE_NODE, styleVals, NUM_STYLES);<br/>            inferenceInterface.run(new String[]{OUTPUT_NODE}, <br/>                                   isDebug());<br/>            float[] floatValuesOutput = new float[floatValues.length/MY_DIVISOR];<br/>            //floatValuesOutput  = new float[mImgBitmap.getWidth() * (mImgBitmap.getHeight() + 10) * 3];//add a little buffer to the float array because tensorflow sometimes returns larger images than what is given as input<br/>            inferenceInterface.fetch(OUTPUT_NODE, floatValuesOutput);<br/><br/>            for (int j = 0; j &lt; myArrayLength; ++j) {<br/>                intValues[j+i*myArrayLength] =<br/>                        0xFF000000<br/>                                | (((int) (floatValuesOutput [(j) * 3]  <br/>                                                       * 255)) &lt;&lt; 16)<br/>                                | (((int) (floatValuesOutput [(j) * 3 + <br/>                                                     1] * 255)) &lt;&lt; 8)<br/>                                | ((int) (floatValuesOutput [(j) * 3 + <br/>                                                          2] * 255));<br/>            }<br/>            //floatValues = new float[mImgBitmap.getWidth() * <br/>                                     (mImgBitmap.getHeight()) * 3];<br/>            mImgBitmap.setPixels(intValues, 0, mImgBitmap.getWidth(), <br/>                0, 0, mImgBitmap.getWidth(), mImgBitmap.getHeight());<br/>            runOnUiThread(new Runnable() {<br/>                @Override<br/>                public void run() {<br/>                    mPreviewImage.setImageBitmap(mImgBitmap);<br/>                }<br/>            });<br/>        }<br/>    }<br/>    else{<br/>        mImgBitmap = bitmapCache.get("style_"+String.valueOf(mSelectedStyle));<br/>    }<br/>    runOnUiThread(new Runnable() {<br/>        @Override<br/>        public void run() {<br/>            if(mPreviewImage!=null){<br/>                mPreviewImage.setImageBitmap(mImgBitmap);<br/>                bitmapCache.put("style_"+String.valueOf(mSelectedStyle),mImgBitmap);<br/>                if(progress!=null){<br/>                    progress.dismiss();<br/>                }<br/>            }<br/>        }<br/>    });<br/>}</pre>

<p>上述代码的输出如下:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-786 image-border" src="img/5ea0ae19-de6d-482a-b884-a87845540abb.png" style="width:21.58em;height:38.33em;"/></p>

<p>应用样式后，可以共享图像:</p>

<pre>shareButton.setOnClickListener(new View.OnClickListener() {<br/>    @Override<br/>    public void onClick(View view) {<br/>        if(ContextCompat.checkSelfPermission(ShowImageActivity.this, <br/>                         Manifest.permission.WRITE_EXTERNAL_STORAGE)<br/>                != PackageManager.PERMISSION_GRANTED)<br/>        {<br/>            requestStoragePermission();<br/>            return;<br/>        }<br/>        if(mImgBitmap!=null) {<br/>            try{<br/>                Bitmap newBitmap = Bitmap.createBitmap(mImgBitmap.getWidth(), mImgBitmap.getHeight(), <br/>                                         Bitmap.Config.ARGB_8888);<br/>                // create a canvas where we can draw on<br/>                Canvas canvas = new Canvas(newBitmap);<br/>                // create a paint instance with alpha<br/>                canvas.drawBitmap(mOrigBitmap,0,0,null);<br/>                Paint alphaPaint = new Paint();<br/>                alphaPaint.setAlpha(mSeekBar.getProgress()*255/100);<br/>                // now lets draw using alphaPaint instance<br/>                canvas.drawBitmap(mImgBitmap, 0, 0, alphaPaint);<br/><br/>                String path = MediaStore.Images.Media.insertImage(ShowImageActivity.this.getContentRe                                   solver(), newBitmap, "Title", null);<br/>                final Intent intent = new Intent(android.content.Intent.ACTION_SEND);<br/>                intent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);<br/>                intent.putExtra(Intent.EXTRA_STREAM, Uri.parse(path));<br/>                intent.setType("image/png");<br/>                startActivity(intent);<br/>            }<br/>            catch(Exception e){<br/>                e.printStackTrace();<br/>                Toast.makeText(ShowImageActivity.this,"Error occurred while trying to share",Toast.LENGTH_SHORT).show();<br/>            }<br/><br/>        }<br/>    }<br/>});</pre>

<p>然后，在对用户的数据执行任何操作之前，我们需要获得用户的适当许可:</p>

<pre>private void requestStoragePermission() {<br/>    if (ActivityCompat.shouldShowRequestPermissionRationale(ShowImageActivity.<br/>         this, Manifest.permission.WRITE_EXTERNAL_STORAGE)) {<br/>        Toast.makeText(ShowImageActivity.this,"Write permission required to share",Toast.LENGTH_SHORT).show();<br/>    }<br/>    ActivityCompat.requestPermissions(this, new String[]{Manifest.permission.WRITE_EXTERNAL_STORAGE},<br/>            REQUEST_STORAGE_PERMISSION);<br/>}<br/><br/>@Override<br/>public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions,<br/>                                       @NonNull int[] grantResults) {<br/>    if (requestCode == REQUEST_STORAGE_PERMISSION) {<br/>        if (grantResults.length != 1 || grantResults[0] != PackageManager.PERMISSION_GRANTED) {<br/>            Camera2BasicFragment.ErrorDialog.newInstance(getString(R.string.request_permission_storage)).show(getFragmentManager(),"dialog");<br/>        }<br/>        else{<br/>            shareButton.performClick();<br/>        }<br/>    } else {<br/>        shareButton.performClick();<br/>        super.onRequestPermissionsResult(requestCode, permissions, grantResults);<br/>    }<br/>}</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>在这一章中，我们学习了从一种艺术形式构建一个风格转换应用程序。现在，我们非常熟悉CNN如何深入工作以及各层如何处理数据。我们也已经熟悉了构建iOS应用和Android应用的基础。</p>

<p>在下一章，我们将更详细地讨论使用Firebase的ML Kit框架应用机器学习。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>References</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">参考</h1>

                

            

            

                

<ul>

<li>【https://arxiv.org/abs/1508.06576 T2】号</li>

<li><a href="https://harishnarayanan.org/writing/artistic-style-transfer/" target="_blank">https://harishnarayanan . org/writing/artistic-style-transfer/</a></li>

<li><a href="https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398" target="_blank">https://medium . com/tensor flow/neural-style-transfer-creating-art-with-deep-learning-using-TF-keras-and-eager-execution-7d 541 AC 31398</a></li>

<li><a href="https://towardsdatascience.com/artistic-style-transfer-b7566a216431" target="_blank">https://towards data science . com/artistic-style-transfer-b 7566 a 216431</a></li>

<li><a href="https://github.com/anishathalye/neural-style" target="_blank">https://github.com/anishathalye/neural-style</a></li>

<li><a href="https://shafeentejani.github.io/2016-12-27/style-transfer/" target="_blank">https://shafeentejani.github.io/2016-12-27/style-transfer/</a></li>

<li><a href="https://reiinakano.github.io/fast-style-transfer-deeplearnjs/" target="_blank">https://reiinakano . github . io/fast-style-transfer-deeplearnjs/</a></li>

<li><a href="https://arxiv.org/abs/1603.08155" target="_blank">https://arxiv.org/abs/1603.08155</a></li>

</ul>





            



            

        

    </body>



</html></body></html>