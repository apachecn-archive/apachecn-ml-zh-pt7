<html><head/><body>

    

        <title>Mobile Landscapes in Machine Learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">机器学习中的移动景观</h1>

                

            

            

                

<p>计算机每天都在进步，设备的外形也在发生巨大的变化。过去，我们只会在办公室看到电脑，但现在我们在家里的桌子上、腿上、口袋里和手腕上都可以看到电脑。随着机器配备越来越多的智能，市场变得越来越多样化。</p>

<p>目前，几乎每个成年人都随身携带一个设备，据估计，无论是否有必要，我们每天至少要看50次智能手机。这些机器影响着我们的日常决策过程。设备现在配备了Siri、谷歌助手、Alexa或Cortana等应用程序，这些功能旨在模仿人类智能。回答任何问题的能力将这些类型的技术呈现为人类大师。在后端，这些系统利用从所有用户那里获得的集体智慧进行改进。你与虚拟助手互动得越多，他们给出的结果就越好。</p>

<p>尽管有这些进步，我们离通过机器创造人脑还有多远？我们现在是2018年。如果科学发现了一种控制我们大脑神经元的方法，这可能在不久的将来成为可能。模仿人类能力的机器正在帮助解决复杂的文本、视觉和听觉问题。它们类似于人类大脑每天执行的任务——平均而言，人类大脑每天做出大约35，000个决定。</p>

<p>虽然我们将来能够模仿人脑，但这是有代价的。目前我们没有更便宜的解决方案。与人脑相比，人脑模拟程序的功耗量限制了它。人脑耗电约20 W，而一个模拟程序耗电约1 MW以上。人类大脑中的神经元运行速度为200 Hz，而典型的微处理器运行速度为2 GHz，是这个速度的1000万倍。</p>

<p>虽然我们离克隆人脑还很远，但我们可以实现一种算法，根据以前的数据以及来自类似设备的数据做出有意识的决定。这就是<strong>人工智能</strong> ( <strong> AI </strong>)的子集派上用场的地方。通过预定义的算法从我们拥有的复杂数据中识别模式，这些类型的智能可以为我们提供有用的信息。</p>

<p class="mce-root">当计算机每次都在没有明确指令的情况下开始做决策时，我们就达到了<strong>机器学习</strong> ( <strong> ML </strong>)的能力。ML现在无处不在，包括通过识别垃圾邮件、推荐在电子商务网站上购买的最佳产品、在社交媒体照片上自动标记你的面部等功能。所有这些都是使用历史数据中确定的模式，以及通过减少数据中不必要的噪声并产生高质量输出的算法来完成的。当数据积累得越来越多时，计算机可以做出更好的决策。</p>

<p class="mce-root">由于我们可以更广泛地使用移动设备，我们在这些设备上花费的时间也在迅速增加，因此在手机上运行ML模型是有意义的。在手机市场，Android和iOS平台率先覆盖了整个智能手机频谱。我们将探索TensorFlow Lite和Core ML如何在这些移动平台上工作。</p>

<p>本章将涉及的主题如下:</p>

<ul>

<li>ML基础知识(带示例)</li>

<li>张量流和核心ML基础</li>

</ul>





            



            

        

    






    

        <title>Machine learning basics</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">机器学习基础</h1>

                

            

            

                

<p>ML是一个概念，它描述了一组通用算法分析你的数据的过程，并为你提供有趣的数据，而无需为你的问题编写任何特定的代码。</p>

<p>或者，我们可以将ML视为一个黑匣子，尖端科学家如何使用它来做一些疯狂的事情，如检测癫痫或癌症疾病，而您简单的电子邮件收件箱每天都在使用它来过滤垃圾邮件。</p>

<p>在更大的层面上，洗钱可分为以下两类:</p>

<ul>

<li>监督学习</li>

<li>无监督学习</li>

</ul>





            



            

        

    






    

        <title>Supervised learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">监督学习</h1>

                

            

            

                

<p>有了监督学习，你的主要任务就是开发一个将输入映射到输出的函数。例如，如果您有输入变量(<em> x </em>)和输出变量(<em> y </em>)，那么您可以使用一种算法来学习从输入到输出的映射函数:</p>

<p class="CDPAlignCenter CDPAlign"><em> y = f(x) </em></p>

<p>目标是很好地逼近映射函数，以便当您有新的输入数据(<em> x </em>)时，您可以预测它的输出变量(<em> y </em>)。</p>

<p>例如，你有一堆水果和篮子。你开始给水果和篮子贴上苹果、香蕉、草莓等标签。当你把所有的水果都放进相应的篮子里后，现在你的工作是给新进的水果贴上标签。通过给它们贴标签，你已经学会了所有的水果和它们的细节。根据之前的经验，你现在可以根据颜色、大小和图案等属性给新水果贴上标签。</p>





            



            

        

    






    

        <title>Unsupervised learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">无监督学习</h1>

                

            

            

                

<p>在这种情况下，您只有输入数据(<em> x </em>)而没有相应的输出变量。无监督学习的目标是对数据中的底层结构或分布进行建模，以便了解更多信息。</p>

<p>在无监督学习中，你可能一开始没有任何数据。例如，在上面讨论的监督学习的相同场景中，你有一个装满水果的篮子，你被要求将它们分成相似的组。但你没有任何以前的数据，或者没有训练或标签是早些时候做的。在这种情况下，您需要首先了解领域，因为您不知道输入是否是水果。在这种情况下，您需要首先理解每个输入的所有特征，然后尝试与每个新输入匹配。可能在最后一步，你会把所有红色的水果放在一个篮子里，绿色的水果放在另一个篮子里。但不是一个准确的分类。这被称为无监督学习。</p>





            



            

        

    






    

        <title>Linear regression - supervised learning</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">线性回归-监督学习</h1>

                

            

            

                

<p>让我们来看一个简单的线性回归示例及其在TensorFlow上的实现。</p>

<p>让我们预测一所房子的价格，查看同一地区其他房子的价格及其大小信息:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-745 image-border" src="img/4376f3e9-7385-4611-b256-833f82a2371a.png" style="width:35.42em;height:11.50em;"/></p>

<div><div><p>我们有一栋价值82，000美元的房子，另一栋价值55，000美元。现在，我们的任务是找出第三所房子的价格。我们知道所有房子的大小和价格，我们可以将这些数据绘制成图表。让我们根据另外两个数据点找出第三栋房子的价格:</p>

</div>

<div><img src="img/0013903f-cb5e-4503-8883-93f3434184f1.png" style="width:38.50em;height:26.92em;"/></div>

<p>你可能想知道现在如何划清界限。靠近图上标记的所有点随机画一条线。现在，计算每个点到直线的距离，并将它们相加。这样做的结果是误差值。我们的算法应该朝着最小化误差的方向发展，因为最佳拟合线具有较低的误差值。这个过程被称为<strong>梯度下降</strong>。</p>

<p>特定地区所有房屋的价格都相应地标在图上。现在，让我们画出我们已经知道的两套房子的价值:<br/></p>

<div><img src="img/0eaf3823-999a-461d-bf98-5c4a6366d2f4.png" style="width:46.25em;height:32.17em;"/></div>

<p>之后，让我们画一条更接近大多数值的线。这条线与数据完全吻合。由此，我们应该能够确定三号房屋的价格:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-748 image-border" src="img/e3d5485e-6b14-470b-bbec-a19ae5add748.png" style="width:38.33em;height:27.67em;"/></p>

<p>基于为住宅3号提供的数据的大小，我们可以在图表上映射为住宅3号提供的数据的大小。这将允许我们计算出通过所有点画的线上的连接点。这对应于y轴上的98，300美元。这就是所谓的<strong>线性回归</strong>:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-746 image-border" src="img/17a5fbf1-491c-4fa2-8e06-16b3ed0cd75b.png" style="width:28.58em;height:9.17em;"/></p>

<div><p>让我们试着把我们的问题转化成伪代码的形式:</p>

<pre>def estimate_house_price(sqft, location): <br/> price = 0 <br/> #In my area, the average house costs 2000 per sq.ft <br/> price_per_sqft = 2000 <br/> if location == "vegas": <br/>     #but some areas cost a bit more <br/>     price_per_sqft = 5000 <br/> elif location == "newyork": <br/>     #and some areas cost less <br/>     price_per_sqft = 4000 <br/> #start with a base price estimate based on how big the place is <br/> price = price_per_sqft * sqft <br/> return price</pre>

<p>这将是你估计房价的典型方法。我们可以继续添加越来越多的条件检查，但它不能被控制超过位置或参数增加的点。对于一个典型的房价估计，还有很多其他因素也要考虑，如房间的数量，附近的位置，学校，加油站，医院，水位，交通，等等。我们可以把这个函数概括成非常简单的东西，并猜出正确的答案:</p>

<pre>def estimate_house_price(sqft, location): <br/> price = &lt; DO MAGIC HERE &gt;<br/> return price</pre>

<p>我们如何在不写条件检查的情况下确定一条完全符合的线？通常，线性回归线以下列形式表示:</p>

<p class="CDPAlignCenter CDPAlign"><em> Y = XW +b </em></p>

<p>在我们的示例中，为了更好地理解，让我们用一种更简单的形式来描述:</p>

<p class="CDPAlignCenter CDPAlign"><em>预测= X *权重+偏差</em></p>

<p><em> Weight </em>是直线的斜率，<em> bias </em>是截距(当<em> X = 0 </em>时<em> Y </em>的值)。构建线性模型后，我们需要识别梯度下降。<em>成本</em>函数识别均方误差，以产生梯度下降:</p>

<div><img class="fm-editor-equation" src="img/86474060-a097-4969-8acd-c91067ae4997.png" style="width:24.83em;height:3.75em;"/></div>

<p>让我们通过伪代码来表示<em> cost </em>函数，来解决我们估算房价的问题:</p>

<pre>def estimate_house_price(sqft, location):<br/> price = 0<br/> #and this<br/> price += sqft * 235.43<br/> #maybe this too<br/> price += location * 643.34<br/> #adding a little bit of salt for a perfect result <br/> price += 191.23<br/> return price</pre>

<p>值<kbd>235.43</kbd>、<kbd>643.34</kbd>和<kbd>191.23</kbd>看起来可能是随机的，但是有了这些值，我们就能够发现任何新房子的估价。我们如何得出这个值？我们应该进行一次迭代，以达到正确的值，同时减少我们在正确方向上的误差:</p>

<pre>def estimate_house_price(sqft, location):<br/> price = 0<br/> #and this<br/> price += sqft * 1.0<br/> #maybe this too<br/> price += location * 1.0<br/> #adding a little bit of salt for a perfect result<br/> price += 1.0<br/> return price</pre>

<p>因此，我们将从<kbd>1.0</kbd>开始迭代，并开始在正确的方向上最小化误差。让我们用TensorFlow把它写成代码。稍后，我们将更深入地了解使用的方法:</p>

<pre>#import all the necessary libraries<br/>import tensorflow as tf<br/>import matplotlib.pyplot as plt<br/>import numpy<br/><br/>#Random number generator<br/>randnumgen = numpy.random<br/><br/>#The values that we have plotted on the graph<br/>values_X = <br/>  numpy.asarray([1,2,3,4,5.5,6.75,7.2,8,3.5,4.65,5,1.5,4.32,1.65,6.08])<br/>values_Y = <br/> numpy.asarray([50,60,65,78,89,104,111,122,71,85,79,56,81.8,55.5,98.3])<br/><br/># Parameters<br/>learning_rate = 0.01<br/>training_steps = 1000<br/>iterations = values_X.shape[0]<br/><br/># tf float points - graph inputs<br/>X = tf.placeholder("float")<br/>Y = tf.placeholder("float")<br/><br/># Set the weight and bias<br/>W = tf.Variable(randnumgen.randn(), name="weight")<br/>b = tf.Variable(randnumgen.randn(), name="bias")<br/><br/># Linear model construction<br/># y = xw + b<br/>prediction = tf.add(tf.multiply(X, W), b)<br/><br/>#The cost method helps to minimize error for gradient descent. <br/>#This is called mean squared error.<br/>cost = tf.reduce_sum(tf.pow(prediction-Y, 2))/(2*iterations)<br/><br/># In TensorFlow, minimize() method knows how to optimize the values for # weight &amp; bias. <br/>optimizer = <br/>   tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)<br/><br/>#assigning default values <br/>init = tf.global_variables_initializer()<br/><br/>#We can start the training now<br/>with tf.Session() as sess:<br/><br/>    # Run the initializer. We will see more in detail with later       <br/>    #chapters<br/>     sess.run(init)<br/><br/>    # Fit all training data<br/>     for step in range(training_steps):<br/>         for (x, y) in zip(values_X, values_Y):<br/>             sess.run(optimizer, feed_dict={X: x, Y: y})<br/>             c = sess.run(cost, feed_dict={X: values_X, Y:values_Y})<br/>             print("Step:", '%04d' % (step+1), "cost=", "  <br/>                             {:.4f}".format(c), \<br/>                             "W=", sess.run(W), "b=", sess.run(b))<br/><br/>     print("Successfully completed!")<br/>     # with this we can identify the values of Weight &amp; bias<br/>     training_cost = sess.run(cost, feed_dict={X: values_X, Y: <br/>                                               values_Y})<br/>     print("Training cost=", training_cost, "Weight=", sess.run(W), <br/>           "bias=", sess.run(b))<br/><br/>     # Lets plot all the values on the graph<br/>     plt.plot(values_X, values_Y, 'ro', label='house price points')<br/>     plt.plot(values_X, sess.run(W) * values_X + sess.run(b), <br/>                                       label='Line Fitting')<br/>     plt.legend()<br/>     plt.show()</pre>

<p>你可以从我们的GitHub资源库(<a href="https://github.com/PacktPublishing/Machine-Learning-Projects-for-Mobile-Applications" target="_blank">https://GitHub . com/packt publishing/Machine-Learning-Projects-for-Mobile-Applications</a>)的<kbd>Chapter01</kbd>下找到相同的代码。</p>





            



            

        

    






    

        <title>TensorFlow Lite and Core ML</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">TensorFlow Lite和Core ML</h1>

                

            

            

                

<p>对于这本书来说，一个好的起点是尝试ML模型数据集和训练模型。这将有助于快速进入后续章节。我们不打算在这里处理基本的ML算法；相反，这将是一个更加基于实践的方法。你可以从我们的GitHub库(<a href="https://github.com/intrepidkarthi/MLmobileapps" target="_blank">https://github.com/intrepidkarthi/MLmobileapps</a>)下载完整的代码库。</p>

<p>在本书中，我们将讨论两个框架:TensorFlow Lite和Core ML。这两个框架与Android和iOS紧密耦合。我们将通过TensorFlow Lite在移动设备上研究ML的基础知识。假设读者知道张量流和基本ML算法的基础，因为这本书不打算涵盖那些元素。</p>

<p>如前所述，我们每个人几乎无时无刻不在口袋里拿着一部智能手机。我们有大量的数据来自这些设备上的传感器。除此之外，我们还有来自边缘设备的数据。在写这本书的时候，这个类别下有将近230亿个设备，包括智能扬声器、智能手表和智能传感器。过去只能在价格较高的设备上使用的高端技术现在也可以在价格较低的设备上使用。这些设备的指数级增长率为这些设备上的ML铺平了道路。</p>

<p>虽然在设备上运行ML有很多原因，但最主要的原因是延迟。如果您正在处理视频或音频，您不希望不断地来回ping服务器上的数据。另一个优点是，当设备处于离线模式时，您可以进行处理。重要的是，数据保留在用户本地的设备上。这在电池/功耗方面更节能。</p>

<p>虽然这看起来是一个优点，但是这种方法也有一些缺点。我们的大多数设备都使用容量有限、处理能力较低、内存限制严格的电池。TensorFlow框架不会解决所有这些问题，这就是为什么它已经转变为在所有这些条件下都能有效工作的框架。TensorFlow Lite是一个轻量级、节能和内存高效的框架，将在嵌入式小尺寸设备上运行。</p>





            



            

        

    






    

        <title>TensorFlow Lite</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">TensorFlow Lite</h1>

                

            

            

                

<p>TensorFlow Lite框架由五个高级组件组成。所有这些组件都经过优化，可在移动平台上运行，如下面的架构图所示:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/e310245b-5a13-4c46-8eef-c2b27ea9969b.png" style="width:40.67em;height:12.58em;"/></p>

<div><p>以下是TensorFlow Lite架构的核心单元:</p>

<ul>

<li>第一部分是使用TensorFlow Lite转换器将您现有的模型转换为与TensorFlow Lite兼容的模型(<kbd>.tflite</kbd>)，并将您的训练模型存储在磁盘上。您还可以在移动或嵌入式应用中使用预先训练的模型。</li>

<li>Java/c++ API——API加载<kbd>.tflite</kbd>模型并调用解释器。它在所有平台上都可用。Java API是写在C++ API之上的包装器，只在Android上可用。</li>

<li>解释器和内核——解释器模块在操作内核的帮助下运行。它有选择地加载内核；核心解释器的大小是75 KB。这与TensorFlow Mobile所需的1.1 MB相比，在TensorFlow Lite上有了显著的降低。通过所有支持的操作，它的核心解释器大小达到400 KB。开发人员可以选择性地选择他们想要包含的操作。这样，他们可以保持较小的足迹。</li>

</ul>

<ul>

<li>硬件加速委托——在选定的Android设备上，解释器将使用Android <strong>神经网络API </strong> ( <strong> NNAPI </strong>)进行硬件加速，或者如果没有可用的，则默认为CPU执行。</li>

</ul>

<p>您还可以使用解释器可以使用的C++ API来实现定制内核。</p>





            



            

        

    






    

        <title>Supported platforms</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">支持的平台</h1>

                

            

            

                

<div><img src="img/5932ca97-aa07-425a-be99-c9c50801f52d.png" style="width:36.17em;height:14.00em;"/></div>

<p>TensorFlow Lite目前支持Android/iOS平台以及Linux(例如Raspberry Pi)平台。在Raspberry Pi等嵌入式设备上，Python API有所帮助。TensorFlow Lite平台还支持核心ML模型以及iOS平台。</p>

<p>在iOS平台上，从预先训练的TensorFlow模型中，我们可以直接将格式转换为核心ML模型，应用将直接在核心ML运行时上运行:<br/></p>

<div><img src="img/c9e266c8-ec44-4c8d-8f29-da87e827e4ce.png" style="width:29.58em;height:13.92em;"/></div>

<p class="mce-root">使用单一模型，我们可以通过转换格式在Android/iOS平台上运行该模型。</p>





            



            

        

    






    

        <title>TensorFlow Lite memory usage and performance </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">TensorFlow Lite内存使用和性能</h1>

                

            

            

                

<p>TensorFlow对模型使用FlatBuffers。FlatBuffers是一个跨平台、开源的序列化库。使用FlatBuffers的主要优点是，在通过打包/解包访问数据之前，它不需要二次表示。它通常与基于对象的内存分配相结合。FlatBuffers比协议缓冲区更节省内存，因为它有助于我们保持较小的内存占用。</p>

<p>FlatBuffers最初是为游戏平台开发的。因为它对性能敏感，所以也用于其他平台。在转换时，TensorFlow Lite预融合激活和偏差，使TensorFlow Lite执行速度更快。解释器使用静态内存和执行计划，这使得它可以更快地加载。优化后的操作内核在NEON和ARM平台上运行更快。</p>

<p>TensorFlow充分利用了这些设备在硅片层面上的所有创新。TensorFlow Lite支持Android NNAPI。在撰写本文时，一些<strong> Oracle企业经理</strong>(<strong>OEM</strong>)已经开始使用NNAPI。TensorFlow Lite采用直接图形加速，在Android上使用<strong>开放图形库</strong> ( <strong> OpenGL </strong>)，在iOS上使用Metal。</p>

<p>为了提高性能，对量化进行了更改。这是一种存储数字并对其进行计算的技术。这在两个方面有所帮助。首先，只要型号更小，就更适合更小的设备。其次，许多处理器都有专门的合成指令集，处理定点操作数比处理浮点数快得多。所以，一个非常简单的量化方法是在你完成训练后简单的缩小权重和激活。然而，这导致次优精度。</p>

<p>TensorFlow Lite在MobileNet和Inception-v3上的性能是TensorFlow的三倍。虽然TensorFlow Lite仅支持推理，但它很快会被改编为包含一个培训模块。TensorFlow Lite支持大约50种常用操作。</p>

<p>它支持MobileNet、Inception-v3、ResNet50、SqueezeNet、DenseNet、Inception-v4、SmartReply等:</p>

<div><img src="img/4822afd9-7187-4fa3-9ba7-fda3de9d8532.png" style="width:42.67em;height:27.92em;"/></div>

<p>图中的<em> y </em>轴以毫秒为单位。</p>





            



            

        

    






    

        <title>Hands-on with TensorFlow Lite </title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">TensorFlow Lite实践</h1>

                

            

            

                

<p>借助TensorFlow Lite，您可以使用现有模型快速开始构建您的第一个基于TensorFlow Lite的应用程序:</p>

<div><img src="img/55e0c65f-d683-4dfa-bd69-ab50474e68db.png" style="width:35.83em;height:8.33em;"/></div>

<p>实时使用TensorFlow Lite包括四个步骤:</p>

<ol>

<li>在第一步中，我们需要要么使用一个现有的模型，要么准备我们自己的模型并训练它。</li>

<li>一旦模型准备好了，就需要使用转换器将其转换成<kbd>.tflite</kbd>格式。</li>

<li>然后，我们可以在它的基础上编写ops进行任何类型的优化。</li>

<li>你可以开始写你的hello world项目了。</li>

</ol>

<p>让我们从这里直接进入代码。</p>





            



            

        

    






    

        <title>Converting SavedModel into TensorFlow Lite format</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">将SavedModel转换为TensorFlow Lite格式</h1>

                

            

            

                

<p>通过调用转换方法，只需一行代码就可以将ML模型转换成TensorFlow Lite模型。下面是将现有模型转换为TensorFlow Lite格式的简单Python片段。您可以输入现有模型并将其转换成<kbd>.tflite</kbd>格式:</p>

<pre>import sys<br/>from tf.contrib.lite import convert_savedmodel<br/>convert_savedmodel.convert(<br/>                            saved_model_directory="/tmp/your_model",<br/>                            output_tflite_file="/tmp/my_model.tflite")</pre>

<p>这里的代码使用FlatBuffers将在其他框架中创建的现有模型转换为TensorFlow Lite格式。需要遵循一些转换策略。</p>





            



            

        

    






    

        <title>Strategies</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">战略</h1>

                

            

            

                

<p>我们实施以下策略:</p>

<ul>

<li>使用冻结的graphdef(或SavedModel)</li>

<li>避免不支持的运算符</li>

</ul>

<ul>

<li>使用可视化工具理解模型(TensorBoard和TensorFlow Lite可视化工具)</li>

<li>为任何缺失的功能编写自定义运算符</li>

<li>如果遗漏了什么，向社区提出问题</li>

</ul>

<p>在以后的章节中，当我们深入到实际应用中时，我们将会看到这些策略的细节。</p>





            



            

        

    






    

        <title>TensorFlow Lite on Android</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">Android上的TensorFlow Lite</h1>

                

            

            

                

<p>我们可以开始使用TensorFlow GitHub存储库中提供的演示应用程序。这是一个相机应用程序，使用浮点Inception-v3模型或量化的MobileNet模型对图像进行连续分类。使用Android或更早版本尝试一下。</p>

<p>试玩app可以在:<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app" target="_blank">https://github . com/tensor flow/tensor flow/tree/master/tensor flow/contrib/lite/Java/demo/app</a>找到。</p>

<p class="mce-root">这个应用程序执行帧的实时分类。它显示最有可能的分类类别。它还显示检测物体所用的时间。</p>

<p class="mce-root">有三种方法可以在您的设备上获得演示应用程序:</p>

<ul>

<li class="mce-root">你可以下载预编译的APK二进制文件</li>

<li class="mce-root">您可以在Android Studio上构建并运行应用程序</li>

<li class="mce-root">您可以使用Bazel下载TensorFlow Lite的源代码，并通过命令行运行应用程序</li>

</ul>





            



            

        

    






    

        <title>Downloading the APK binary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">下载APK二进制文件</h1>

                

            

            

                

<p class="mce-root">这是尝试应用程序的最简单的方法。</p>

<p>安装应用程序后，启动应用程序。首次打开应用程序时，它会提示您使用运行时权限访问设备摄像头。一旦启用了权限，您就可以使用该应用程序来识别实时背面摄像头视图中的对象。在结果中，您可以看到已识别对象的前三个分类，以及延迟。</p>





            



            

        

    






    

        <title>TensorFlow Lite on Android Studio</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">Android Studio上的TensorFlow Lite</h1>

                

            

            

                

<p class="mce-root">您可以通过以下步骤直接从Android Studio下载并构建TensorFlow Lite:</p>

<ol>

<li class="mce-root">下载并安装最新版本的Android Studio。</li>

<li>在您的工作室设置中，确保NDK版本大于14，SDK版本大于26。我们在本书和以后的应用中使用27。我们将在以后的项目中详细讨论如何配置它。</li>

<li class="mce-root">您可以从以下信息框中的链接下载该应用程序。</li>

<li class="mce-root">按照Android Studio的指示，你需要安装所有的Gradle依赖项。</li>

</ol>

<p>演示应用可以在:<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo" target="_blank">https://github . com/tensor flow/tensor flow/tree/master/tensor flow/contrib/lite/Java/demo/app/src/main/Java/com/example/Android/tflitecamerademo</a>找到。</p>

<p>我们需要一个模型，以便在应用程序中使用它。我们可以使用现有的模型，也可以训练我们自己的模型。让我们在这个应用程序中使用一个现有的模型。</p>

<p>您可以通过信息框中的链接下载模型。您也可以从以下链接下载压缩的模型文件:</p>

<ul>

<li>您可以下载一个Inception-v3浮点模型或者最新的MobileNet模型。将适当的<kbd>.tflite</kbd>复制到Android应用的<kbd>assets</kbd>目录中。然后，您可以在<kbd>Camera2BasicFragment.java</kbd>文件<kbd>tensorflow/contrib/lite/java/demo/app/src/main/assets/</kbd>中更改分类器。</li>

</ul>

<p>模型可以从:<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md" target="_blank">https://github . com/tensor flow/tensor flow/blob/master/tensor flow/contrib/lite/g3doc/models . MD</a>下载。</p>

<p class="mce-root">现在，您可以构建并运行演示应用程序。</p>





            



            

        

    






    

        <title>Building the TensorFlow Lite demo app from the source</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">从源代码构建TensorFlow Lite演示应用程序</h1>

                

            

            

                

<p class="mce-root">第一步，克隆TensorFlow repo。你需要巴泽尔来建造APK:</p>

<pre class="mce-root">git clone https://github.com/tensorflow/tensorflow</pre>





            



            

        

    






    

        <title>Installing Bazel</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">安装Bazel</h1>

                

            

            

                

<p class="mce-root">如果Bazel没有安装在您的系统上，您需要安装它。本书根据macOS High Sierra 10.13.2经验编写。Bazel是通过自制软件安装的。</p>





            



            

        

    






    

        <title>Installing using Homebrew</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">使用自制软件安装</h1>

                

            

            

                

<p>以下是安装Homebrew的步骤:</p>

<ol>

<li>家酿有依赖JDK，你需要先安装。从Oracle网站下载并安装最新的JDK。</li>

<li>然后，安装家酿。</li>

</ol>

<p>您可以直接从终端运行以下脚本:</p>

<pre class="mce-root"><strong>/usr/bin/ruby -e "$(curl -fsSL \</strong><br/><strong>   https://raw.githubusercontent.com/Homebrew/install/master/install)"</strong></pre>

<p>一旦安装了Homebrew，您可以使用以下命令安装Bazel:</p>

<pre class="mce-root"><strong>brew install bazel</strong></pre>

<p class="mce-root">一切都好。现在，您可以使用下面显示的命令来验证Bazel版本:</p>

<pre class="mce-root"><strong>bazel version</strong></pre>

<p class="mce-root">如果已经安装了Bazel，您可以使用以下命令升级版本:</p>

<pre class="mce-root"><strong>brew upgrade bazel</strong></pre>

<p>请注意，Bazel目前不支持Windows上的Android版本。Windows用户应该下载预构建的二进制文件。</p>





            



            

        

    






    

        <title>Installing Android NDK and SDK</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">安装安卓NDK和SDK</h1>

                

            

            

                

<p class="mce-root">您需要Android NDK来构建TensorFlow Lite代码。你可以通过以下链接从NDK档案馆下载。</p>

<p>安卓NDK档案可以从:<a href="https://developer.android.com/ndk/downloads/older_releases" target="_blank">https://developer.android.com/ndk/downloads/older_releases</a>下载。</p>

<p>Android Studio自带SDK工具。您需要访问构建工具版本23或更高版本(应用程序运行在具有API 21或更高版本的设备上)。</p>

<p>您可以使用API级别和到SDK和NDK的路径来更新目录根目录中的<kbd>WORKSPACE</kbd>文件。</p>

<p class="mce-root">更新<kbd>api_level</kbd>以及SDK和NDK在存储库根目录下的位置。如果从Studio打开SDK管理器，可以找到SDK路径。例如，请注意SDK的以下内容:</p>

<pre class="mce-root">android_sdk_repository (<br/> name = "androidsdk",<br/> api_level = 27,<br/> build_tools_version = "27.0.3",<br/> path = "/Users/coco/Library/Android/sdk",<br/>)</pre>

<p>对于安卓NDK档案:</p>

<pre class="mce-root">android_ndk_repository(<br/> name = "androidndk",<br/> path = "/home/coco/android-ndk-r14b/",<br/> api_level = 19,<br/>)</pre>

<p>在写作的时候，<kbd>android-ndk-r14b-darwin-x86_64.zip</kbd>是从NDK档案馆拿来的。您可以根据可用性调整上述参数。</p>

<p>现在，我们准备好构建源代码了。要构建演示应用程序，请运行Bazel:</p>

<pre class="mce-root"><strong>bazel build --cxxopt=--std=c++11 </strong><br/><strong> //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo</strong></pre>

<div><br/>

Caution: Due to a bug, Bazel only supports the Python 2 environment right now. </div>

<p>MobileNet是开始ML的好地方。该数据集中的模型图像由299 * 299像素的图像组成。但是，相机捕捉224 * 224像素的图像，并调整其大小以匹配模型中的大小。每个图像在磁盘中占用224 * 224 * 3字节。之后这些字节被逐行转换成ByteBuffer。这里，数字<em> 3 </em>表示像素的RGB值。</p>

<p>这里的演示应用程序使用TensorFlow Lite Java API，它将单个图像作为输入，并在输出中生成相同的图像。输出包含一个二维数组。第一个数组包含类别索引值，第二维包含分类的置信度值。根据这些值，它在前端向用户显示前三名。</p>





            



            

        

    






    

        <title>TensorFlow Lite on iOS</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">iOS上的TensorFlow Lite</h1>

                

            

            

                

<p>现在，我们将在iOS环境中构建相同的应用程序。该应用程序具有相同的功能，我们也将使用相同的量化MobileNet模型。我们需要在真实的iOS设备上运行它，才能使用相机功能；它在模拟器上不起作用。</p>





            



            

        

    






    

        <title>Prerequisites</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">先决条件</h1>

                

            

            

                

<p class="mce-root">若要开始使用Xcode，您需要在他们的门户网站上拥有有效的Apple开发者id。该应用程序还需要iPhone，因为它使用相机模块。您需要将预置描述文件分配给特定设备。只有这样，您才能在设备上构建和运行应用程序。</p>

<p>您可以克隆完整的TensorFlow存储库，但是要运行此应用程序，您可能不需要完整的源代码。如果您已经下载了它，则不需要再次下载:</p>

<pre class="mce-root"><strong>git clone https://github.com/tensorflow/tensorflow</strong></pre>

<p class="mce-root">Xcode附带了命令行工具，如下所示:</p>

<pre class="mce-root"><strong>xcode-select --install</strong></pre>





            



            

        

    






    

        <title>Building the iOS demo app</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">构建iOS演示应用程序</h1>

                

            

            

                

<p class="mce-root">如果你对iOS应用构建不是很熟悉，请看看这方面的一些基础教程。您需要安装<kbd>cocoapods</kbd>来安装所有的依赖项:</p>

<pre class="mce-root"><strong>sudo gem install cocoapods</strong></pre>

<p class="mce-root">有一个shell脚本可用于下载运行该应用程序所需的模型文件:</p>

<pre class="mce-root"><strong>sh tensorflow/contrib/lite/examples/ios/download_models.sh</strong></pre>

<p class="mce-root">您可以转到项目目录并从命令行安装<kbd>pod</kbd>:</p>

<pre class="mce-root"><strong>cd tensorflow/contrib/lite/examples/ios/camera</strong><br/><strong>pod install</strong><br/><strong>pod update</strong></pre>

<p class="mce-root">一旦更新完成，你应该能看到<kbd>tflite_camera_example.xcworkspace</kbd>。然后，您可以在Xcode中打开应用程序。您也可以使用以下命令:</p>

<pre class="mce-root"><strong>open tflite_camera_example.xcworkspace</strong></pre>

<p class="mce-root">现在是时候在您的iPhone上构建并运行该应用程序了。</p>

<p>您需要允许应用程序使用相机的用户权限。用相机指向物体，开始看到分类结果！</p>





            



            

        

    






    

        <title>Core ML</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">核心ML</h1>

                

            

            

                

<p>Core ML帮助我们构建面向iOS平台的ML学习应用。</p>

<p>核心ML使用基于新输入数据进行预测的训练模型。例如，根据一个地区的历史土地价格训练的模型可能能够在给定位置和面积的详细信息时预测土地的价格。</p>

<p>核心ML是其他特定领域框架的基础。Core ML支持的主要框架包括用于评估学习决策树的GamePlayKit，用于文本分析的<strong>自然语言处理</strong> ( <strong> NLP </strong>)，以及用于基于图像分析的视觉框架。</p>

<p class="mce-root">核心ML构建于accelerate、<strong>基本神经网络子程序</strong> ( <strong> BNNSs </strong>)和金属性能着色器之上，如核心ML文档中的架构图所示:</p>

<ul>

<li class="mce-root">有了Accelerate框架，你可以进行大规模的数学计算，也可以进行基于图像的计算。它针对高性能进行了优化，还包含用C编写的API，用于矢量和矩阵计算、<strong>数字信号处理</strong> ( <strong> DSP </strong>)和其他计算。</li>

<li>bnn有助于实现神经网络。从训练数据来看，子例程方法和其他集合对于实现和运行神经网络是有用的。</li>

</ul>

<ul>

<li>借助Metal framework，您可以使用GPU设备渲染高级三维图形并运行并行计算。它附带了金属着色语言、MetalKit框架和金属性能着色器框架。通过金属性能着色器框架，它可以与每个GPU系列的硬件功能配合工作，以获得最佳性能。</li>

</ul>

<p>核心ML应用程序建立在上述三层组件之上，如下图所示:</p>

<div><img src="img/2c94e43c-1b91-4472-93a9-a45b0b942ef9.png" style="width:18.67em;height:11.75em;"/></div>

<p>Core ML针对片上性能进行了优化，最大限度地减少了内存占用和功耗。</p>





            



            

        

    






    

        <title>Core ML model conversion</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">核心ML模型转换</h1>

                

            

            

                

<p>要在iOS上运行您的第一个应用程序，您不需要开始构建自己的模型。你可以使用任何一个现有的最好的模型。如果您有一个使用另一个第三方框架创建的模型，您可以使用核心ML工具Python包，或第三方包，如MXNet converter或TensorFlow converter。下面给出了访问这些工具的链接。如果您的模型不支持这些转换器，您也可以编写自己的转换器。</p>

<p>核心ML工具Python包可从:<a href="https://pypi.org/project/coremltools/" target="_blank">https://pypi.org/project/coremltools/</a><br/>tensor flow转换器可通过链接访问:<a href="https://github.com/tf-coreml/tf-coreml" target="_blank">https://github.com/tf-coreml/tf-coreml</a><br/>MXNet转换器可从:<a href="https://github.com/apache/incubator-mxnet/tree/master/tools/coreml" target="_blank">https://github . com/Apache/incubator-MXNet/tree/master/Tools/coreml</a></p>

<p class="mce-root"/>

<p>核心的ML Tools Python包支持从Caffe v1、Keras 1.2.2+、scikit-learn 0.18、XGBoost 0.6和LIBSVM 3.22框架的转换。这包括SVM模型、树形集成、神经网络、广义线性模型、特征工程和管道模型。</p>

<p>可以通过<kbd>pip</kbd>安装核心ML工具:</p>

<pre><strong>pip install -U coremltools</strong></pre>





            



            

        

    






    

        <title>Converting your own model into a Core ML model</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">将您自己的模型转换成核心ML模型</h1>

                

            

            

                

<p>将现有模型转换成核心ML模型可以通过<kbd>coremltools</kbd> Python包来完成。如果您想将一个简单的Caffe模型转换成一个核心ML模型，可以通过下面的例子来完成:</p>

<pre>import coremltools<br/>my_coremlmodel = <br/>  coremltools.converters.caffe.convert('faces.caffemodel')<br/>  coremltools.utils.save_spec(my_coremlmodel, 'faces.mlmodel')</pre>

<p>不同型号的转换步骤不同。您可能需要添加标签和输入名称，以及模型的结构。</p>





            



            

        

    






    

        <title>Core ML on an iOS app</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">iOS应用程序上的核心ML</h1>

                

            

            

                

<p>在iOS应用上集成Core ML非常简单。去苹果开发者页面下载预先训练好的模型。从那里下载MobileNet模型。</p>

<p>下载完<kbd>MobileNet.mlmodel</kbd>后，将其添加到项目<em>中的<kbd>Resources</kbd>组。</em>视觉框架通过将现有的图像格式转换成可接受的输入类型来缓解我们的问题。您可以看到您的模型的细节，如下图所示。在接下来的章节中，我们将开始在现有模型的基础上创建我们自己的模型。</p>

<p>让我们看看如何将模型加载到我们的应用程序中:</p>

<div><img src="img/b25d7626-d452-41b7-bf16-a30b97c9ed70.png"/></div>

<p>在您最近创建的Xcode项目中打开<kbd>ViewController.swift</kbd>，并导入Vision和核心ML框架:<br/></p>

<pre>/**<br/>Lets see the UIImage given to vision framework for the prediction.<br/>The results could be slightly different based on the UIImage conversion.<br/>**/<br/>func visionPrediction(image: UIImage) {<br/>     guard let visionModel = try? VNCoreMLModel(for: model.model) else{<br/>                fatalError("World is gonna crash!")<br/>     }<br/>    let request = VNCoreMLRequest(model: visionModel) { request, error  <br/>                                                        in<br/>     if let predictions = request.results as? [VNClassificationObservation] {<br/> //top predictions sorted based on confidence<br/> //results come in string, double tuple<br/>     let topPredictions = observations.prefix(through: 5)<br/> .map { ($0.identifier, Double($0.confidence)) }<br/>     self.show(results: topPredictions)<br/>     }<br/>   }<br/>}</pre>

<p>让我们通过用于预测的核心ML MobileNet模型加载相同的图像:</p>

<pre>/** <br/>Method that predicts objects from image using CoreML. The only downside of this method is, the mlmodel expects images in 224 * 224 pixels resolutions. So we need to manually convert UIImage<br/>into pixelBuffer.<br/>**/<br/>func coremlPrediction(image: UIImage) {<br/>     if let makeBuffer = image.pixelBuffer(width: 224, height: 224),<br/>     let prediction = try? model.prediction(data: makeBuffer) {<br/>     let topPredictions = top(5, prediction.prob)<br/>     show(results: topPredictions)<br/>    }<br/>}</pre>





            



            

        

    






    

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    



    

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p>我们现在已经熟悉了TensorFlow Lite和Core ML背后的基本思想。前面提到的所有代码都可以在我们的GitHub库中找到。由于这两个库都运行在移动设备上，它们都有局限性。我们将在后续章节中深入探讨实时应用程序。</p>

<p>在接下来的章节中，你将学习如何基于特定的用例来开发和训练特定的模型。我们还将介绍如何在此基础上构建自己的移动应用程序。准备好开始训练模型，并在您自己的移动应用程序上使用它！</p>





            



            

        

    



</div></div></div></body></html>