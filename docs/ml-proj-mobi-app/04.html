<html><head/><body><html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Deep Diving into the ML Kit with Firebase</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用Firebase深入了解ML套件</h1>

                

            

            

                

<p>在这一章中，我们将进一步探索基于Google Firebase的移动应用ML工具包平台。</p>

<p>谷歌在I/O 2018上推出Firebase ML套件。ML工具包是Firebase应用程序套件的一部分，它使开发人员能够将<strong>机器学习</strong> ( <strong> ML </strong>)功能整合到移动应用程序中。Firebase ML工具包<strong>软件开发工具包</strong> ( <strong> SDK </strong>)附带了一些移动应用程序中常见的功能，无论Android和iOS开发人员对ML的熟悉程度如何，都可以为他们提供帮助。</p>

<p>本章涵盖的概念如下:</p>

<ul>

<li>了解ML试剂盒的基础知识</li>

<li>学习将Firebase添加到我们的应用程序中</li>

<li>使用Firebase创建可用于人脸检测、条形码扫描和设备上文本识别的多个应用程序</li>

</ul>

<p>这是我们本章知识库的链接:<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4">https://g</a>T10】itT12】hub.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4和<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://github.com/PacktPublishing/Machine-Learning-Projects-for-Mobile-Applications" target="_blank">https://github . com/packt publishing/Machine-Learning-Projects-for-Mobile-Applications</a>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>ML Kit basics</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">ML试剂盒基础</h1>

                

            

            

                

<p>当然，我们总是可以在没有Firebase帮助的情况下完成所有基于ML的实现。然而，有几个原因可以解释为什么不是每个人都能做到这一点。这可能是以下原因之一:</p>

<ul>

<li>一个非常优秀的移动应用开发者可能不擅长构建ML模型。建立一个ML模型肯定需要时间。这可能因具体情况而异。</li>

<li>找到解决您的用例的正确的数据模型集将是一个非常困难的问题。假设你想检测一个亚洲人脸上的年龄和性别分类。在这种情况下，可用的现有模型对于您的用例来说可能不够准确。</li>

<li>托管您自己的模型将会更昂贵，并且在应用程序的服务器端需要额外的关注。</li>

</ul>

<p class="mce-root">ML工具包是本地设备上Google Cloud Vision API、移动视觉和TensorFlow Lite模型的组合:</p>

<p class="mce-root CDPAlignCenter CDPAlign"><img src="img/c4e94a35-93e4-4daf-8931-36595e17094e.png" style="width:27.50em;height:14.33em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Basic feature set</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">基本功能集</h1>

                

            

            

                

<p>ML Kit附带了一个现成的代码库，可用于常见情况，如从图像中检测人脸、条形码扫描、在图像中查找文本以及图像标记。通过将数据传递给API，我们可以用几行代码获得基本用例查询的答案。</p>

<p>ML工具包既提供了设备上的API，也提供了云API。根据我们的要求，我们可以使用这些服务中的任何一个。虽然设备上的API工作更快，但云API将提供更好的准确性。</p>

<p>并非所有的移动应用程序都属于ML工具包提供的默认API。我们总会有自己的案子要通过ML来解决。ML Kit支持将我们的定制TensorFlow Lite模型部署到云中，并作为一个层与您的模型进行交互。</p>

<p>在撰写本书时，ML Kit在测试模式下提供了以下功能:</p>

<ul>

<li>文本识别</li>

<li>人脸检测</li>

<li>条形码扫描</li>

<li>图像标注</li>

<li>地标检测</li>

</ul>

<p>根据使用情况，这些功能可以通过设备上和基于云的检测来实现。例如，离线时从图像中检测人脸可以通过编程在设备上实现，而不是通过将图像上传到云并获得结果。与此同时，从图像中检测地标不能离线高效地完成，因为数据会随时间而变化，并且地标上的数据量将是巨大的。</p>

<p>我们将在将要构建的Android应用程序中涵盖上述每一项功能的一个基本示例。在开发应用程序时，在云中运行ML模型会带来两个主要问题:</p>

<ul>

<li>这些应用程序需要使用互联网。我们通常在文本、图像、音频和视频之上应用ML。根据我们的使用案例，我们最终可能会在数据带宽上花费更多。</li>

<li>应用程序不会只留在您身边。当设备中的数据离开您的设备时，您可能无法控制它。</li>

</ul>

<p>记住这些问题，我们需要考虑构建一个更好的基于ML的应用程序。下表显示了每个API在本地设备和云中的可用性:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/a9b008f2-f5e9-4db1-b1cd-3326644ebd9f.png" style="width:44.42em;height:16.42em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Building the application</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">构建应用程序</h1>

                

            

            

                

<p>我们现在将使用Android Studio安装一个新的Android应用程序。为此，安装最新的Android Studio并创建一个新项目，如下所示:</p>

<div><div><img src="img/8eba5a5c-f325-4911-b687-acc6d9cbcbf9.png" style="width:60.25em;height:42.83em;"/></div>

</div>

<p class="mce-root"/>

<p class="graf graf--p graf-after--p">下一个截图描述了我们要瞄准的Android API。选择<strong> API 15 </strong>及以上版本几乎涵盖了所有现有的Android设备，因此建议使用:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/014884d8-8bd1-4dfa-9a5d-f94bd65e892d.png" style="width:53.42em;height:18.33em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Adding Firebase to our application</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">将Firebase添加到我们的应用程序中</h1>

                

            

            

                

<p>我们已经创建了Android Studio应用程序，其中包含一个空白活动。如果您使用的是Android Studio 2.2或任何更高版本，请使用Firebase助手将您的应用程序连接到Firebase。该助手将帮助您连接到现有的Firebase项目或创建一个新的项目。它还将安装所有必要的<em> Gradle依赖关系</em>。除此之外，我们还可以手动添加Firebase项目。</p>

<p>这个项目是使用Android Studio版本3.1.3构建的。</p>

<p>如果在您的<strong>工具</strong>部分没有找到Firebase助手，请转到文件|设置|构建、执行和部署|所需插件，然后添加<strong> Firebase服务</strong>。或者，您可以按照以下步骤手动添加它:</p>

<ol>

<li>前往<a href="https://console.firebase.google.com/">https://console.firebase.google.com/</a>。下面的截图有助于我们理解我们的页面是什么样子的:</li>

</ol>

<p class="CDPAlignCenter CDPAlign"><img src="img/bb187a3b-3d82-4071-80b8-42cf5e48cf4a.png" style="width:38.17em;height:42.75em;"/></p>

<p>前面的屏幕截图显示了如何添加一个新项目。另一种方法是使用Firebase的现有项目。该应用程序支持跨Android、iOS和web平台的项目。之后，在Firebase控制台上添加更多关于您的应用程序的详细信息。您可以从应用程序的<kbd>app</kbd>文件夹下的<kbd>build.gradle</kbd>文件中获得您的Android软件包名称:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-788 image-border" src="img/810cf6c9-7c19-4c33-8ff6-1b60aafc9ea2.png" style="width:127.50em;height:68.58em;"/></p>

<ol start="2">

<li>下载<kbd>google-services.json</kbd>文件。</li>

</ol>

<p style="padding-left: 60px">创建应用程序后，您可以从Firebase控件下载该文件:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/ecab47e8-98ea-4c26-91b8-8636aeeeb094.png" style="width:43.83em;height:39.67em;"/></p>

<p style="padding-left: 60px">然后你可以将<kbd>.json</kbd>文件放入你的应用程序的<kbd>app</kbd>文件夹中。</p>

<ol start="3">

<li>然后我们继续对<kbd>build.gradle</kbd>文件进行修改。您将需要在项目级和应用级<kbd>build.gradle</kbd>文件中进行更改。</li>

</ol>

<p>项目级的<kbd>build.gradle</kbd>文件放在项目的主文件夹(<kbd>&lt;project&gt;/ build.gradle</kbd>)下:</p>

<pre>buildscript {<br/>    repositories {<br/>        google()<br/>        jcenter()<br/>    }<br/>    dependencies {<br/>        classpath 'com.android.tools.build:gradle:3.1.3'<br/>        classpath 'com.google.gms:google-services:4.0.0'<br/>    }<br/>}</pre>

<p class="a12g-setup-section">app级<kbd>build.gradle</kbd>文件放在以下文件夹下:<kbd>&lt;project&gt;/&lt;app-module&gt;/build.gradle</kbd>。在其中添加以下几行:</p>

<div><div><div><pre style="padding-left: 30px">dependencies {

  // Add this line

  compile 'com.google.firebase:firebase-core:16.0.0'}

...

// Add to the bottom of the file

apply plugin: 'com.google.gms.google-services'</pre></div>

</div>

<p>默认情况下，这包括Firebase的分析服务。</p>

</div>

<p class="a12g-setup-note">现在，单击IDE右上角的“立即同步”按钮。</p>

<p class="a12g-setup-note">完成后，在连接的Android设备或模拟器上运行应用程序:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/67758a4b-efe8-48b2-989d-826392354ee6.png" style="width:36.33em;height:26.25em;"/></p>

<p>在前面的屏幕截图中看到的消息确认Firebase配置已经成功完成。现在，您可以看到控制台中的新应用程序下添加了一个用户。</p>

<p>现在，让我们开始在应用程序中构建特性。在我们开始之前，我们需要在应用程序级的<kbd>build.gradle</kbd>文件中添加ML套件依赖，如下所示:</p>

<pre class="prettyprint">dependencies {<br/>  // You should always use the latest version <br/>  implementation 'com.google.firebase:firebase-ml-vision:16.0.0'<br/>}</pre>

<p>在Firebase中，你会被默认放在星火计划中。您可以升级到Blaze计划，以便使用Cloud Vision APIs，根据免费计划，每月的请求限制为1，000次。在这一章中，我们将在一个直播摄像机上使用他们的所有设备上的模块。</p>

<p>通过设备上的培训，当我们第一次运行应用程序时，模型将自动下载。如果我们只想下载那些特定的模型，可以通过将以下元数据添加到您的应用程序<kbd>manifest</kbd>文件中来实现:</p>

<pre class="prettyprint">&lt;application ...&gt;<br/>  ...<br/>  &lt;meta-data<br/>      android:name="com.google.firebase.ml.vision.DEPENDENCIES"<br/>      android:value="ocr" /&gt;<br/>  &lt;!-- To use multiple models: android:value="ocr, barcode, face, model4, model5" --&gt;<br/>&lt;/application&gt;</pre>

<p>特定模型的使用取决于应用。如果ML模型将成为你的应用体验的核心部分，这将是有意义的。否则，应该只在需要时才下载模型。这将减少移动设备上不必要的负载。</p>

<p>在我们正在构建的应用程序中，我们将启动摄像头视图，从这里我们将切换到ML套件中的所有设备上ML配置。</p>

<p>在相机视图的底部，我们将添加一个<kbd>spinner</kbd>，此时我们将选择当前的ML功能:</p>

<pre>Spinner spinner = (Spinner) findViewById(R.id.spinner);<br/>//Adding the list of items to be detected <br/>List&lt;String&gt; options = new ArrayList&lt;&gt;();<br/>options.add(FACE_DETECTION);<br/>options.add(TEXT_DETECTION);<br/>options.add(BARCODE_DETECTION);<br/>options.add(IMAGE_LABEL_DETECTION);<br/>options.add(CLASSIFICATION);<br/>// Creating adapter for spinner<br/>ArrayAdapter&lt;String&gt; dataAdapter = new ArrayAdapter&lt;&gt;(this, <br/>                           R.layout.spinner_style, options);<br/>// Drop down layout style - list view with radio button<br/>dataAdapter.setDropDownViewResource(android.R.layout.simple_spinner_dro<br/>                                    pdown_item);<br/>// attaching data adapter to spinner<br/>spinner.setAdapter(dataAdapter);<br/>spinner.setOnItemSelectedListener(this);</pre>

<p>基于用户的选择，摄像机视图将开始在实时摄像机视图中显示结果。让我们从面部检测开始。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Face detection</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">人脸检测</h1>

                

            

            

                

<p>通过人脸检测，您可以自动检测图像或视频中的人脸。这将报告人脸在媒体中的实际位置以及大小和方向。一旦一张脸被识别出来，我们就可以进一步检测它的其他身体部位，比如鼻子、眼睛和嘴巴。人脸检测API检测以下内容:</p>

<ul>

<li>检测到的面部的包围盒</li>

<li>脸部的倾斜角度和旋转角度</li>

<li>鼻底、口腔底部、口腔左侧和口腔右侧的坐标</li>

<li>左眼睁开、右眼睁开并且此人微笑的概率</li>

</ul>

<p>有几个术语与ML工具包的人脸检测功能相关联。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Face orientation tracking</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">人脸方向跟踪</h1>

                

            

            

                

<p><strong>人脸</strong> <strong>追踪</strong>可以用来检测视频中的特定人脸。我们可以计算特定人脸出现的帧数，还可以根据人脸的位置和运动来检测两张人脸是否相似(这在视频中通常是可能的)。</p>

<p>使用欧拉角跟踪面部位置，欧拉角根据摄像机的角度确定面部的位置:</p>

<ul>

<li><strong>欧拉X </strong>:欧拉X角为正的面朝上</li>

<li><strong>欧拉Y </strong>:一个有正欧拉Y角的面转向相机的右边和左边</li>

<li><strong>欧拉Z </strong>:欧拉Z角为正的面相对于相机逆时针旋转</li>

</ul>

<p>这三个角度中，ML Kit只支持欧拉Z角的检测；不支持欧拉X角，欧拉Y角只在相机运行在<em>精确</em>模式下测量。在<em>快速</em>模式下，相机会创建快捷方式以更快地获得结果。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Landmarks</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">陆标</h1>

                

            

            

                

<p>ML套件可以检测组成面部的<strong>标志点</strong>。这些标志点包括左眼、右眼、鼻根、嘴的左侧等等。</p>

<p>它独立于地标信息检测人脸，不使用地标信息作为识别整张脸的依据，所以默认不启用。</p>

<p>根据相关的欧拉Y角，可以识别以下所有地标:</p>

<div><img src="img/9f656fa9-75d2-4e46-bd69-fab5b97fe71c.png" style="width:33.17em;height:21.25em;"/></div>

<p>每个检测到的地标包括其在图像中的相关位置。下面是它的代码编写方式:</p>

<pre style="padding-left: 30px">/** Draws the face annotations for position on the supplied canvas. */<br/>@Override<br/>public void draw(Canvas canvas) {<br/>  FirebaseVisionFace face = firebaseVisionFace;<br/>  if (face == null) {<br/>    return;<br/>  }<br/><br/>  // Draws a circle at the position of the detected face, with the   <br/>  // face's track id below.<br/>  float x = translateX(face.getBoundingBox().centerX());<br/>  float y = translateY(face.getBoundingBox().centerY());<br/><br/>  canvas.drawCircle(x, y, FACE_POSITION_RADIUS, facePositionPaint);<br/>  canvas.drawText("id: " + face.getTrackingId(), x + ID_X_OFFSET, y + <br/>                   ID_Y_OFFSET, idPaint);<br/><br/>  canvas.drawText("happiness: " + <br/>       String.format("%.2f",face.getSmilingProbability()),<br/>       x + ID_X_OFFSET * 3,<br/>       y - ID_Y_OFFSET, <br/>       idPaint);<br/><br/>  if (facing == CameraSource.CAMERA_FACING_FRONT) {<br/>        canvas.drawText(<br/>        "right eye: " + String.format("%.2f",                           <br/>                            face.getRightEyeOpenProbability()), <br/>                            x - ID_X_OFFSET, <br/>                            y, <br/>                            idPaint);<br/>        canvas.drawText("left eye: " + String.format("%.2f", <br/>                           face.getLeftEyeOpenProbability()), <br/>                           x + ID_X_OFFSET * 6, <br/>                           y,<br/>                           idPaint);<br/>  } <br/>  else <br/>  {<br/>    canvas.drawText(<br/>        "left eye: " + String.format("%.2f", <br/>                                face.getLeftEyeOpenProbability()),<br/>                                x - ID_X_OFFSET, y, idPaint);<br/>    canvas.drawText(<br/>        "right eye: " + String.format("%.2f",                                              <br/>                           face.getRightEyeOpenProbability()),<br/>                           x + ID_X_OFFSET * 6, y, idPaint);<br/>  }<br/><br/>  // Draws a bounding box around the face.<br/>  float xOffset = scaleX(face.getBoundingBox().width() / 2.0f);<br/>  float yOffset = scaleY(face.getBoundingBox().height() / 2.0f);<br/>  float left = x - xOffset;<br/>  float top = y - yOffset;<br/>  float right = x + xOffset;<br/>  float bottom = y + yOffset;<br/>  canvas.drawRect(left, top, right, bottom, boxPaint);<br/><br/>  // draw landmarks<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.BOTTOM_MOUTH);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.LEFT_CHEEK);<br/>  drawLandmarkPosition(canvas, face,  <br/>                       FirebaseVisionFaceLandmark.LEFT_EAR);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.LEFT_MOUTH);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.LEFT_EYE);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.NOSE_BASE);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.RIGHT_CHEEK);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.RIGHT_EAR);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.RIGHT_EYE);<br/>  drawLandmarkPosition(canvas, face, <br/>                       FirebaseVisionFaceLandmark.RIGHT_MOUTH);<br/>}<br/><br/></pre>

<p>使用前面的代码，我们可以在一张脸上绘制面部标志位置，并在检测到的面部周围绘制一个边框矩形。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Classification</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">分类</h1>

                

            

            

                

<p><strong>分类</strong>用于根据某些面部特征对图像进行分类，例如眼睛是睁开还是闭上，以及此人是否在微笑。</p>

<p>分类以0到1之间的值表示。例如，在微笑分类中0.7或更大的幸福值可以分类该人在微笑。同样，眼睛是否睁开的状态也可以通过分类追踪。</p>

<p>这两种分类都依赖于地标检测。<em>睁眼</em>和<em>微笑</em>分类只在正面人脸上起作用。这意味着他们需要一个更小的欧拉Y角(+/- 18度)来计算这些因子。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Implementing face detection</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">实现人脸检测</h1>

                

            

            

                

<p>人脸检测需要将这个额外的依赖项添加到应用程序级的<kbd>build.gradle</kbd>文件中:</p>

<pre>implementation 'com.google.firebase:firebase-ml-vision:16.0.0'<br/></pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Face detector configuration</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">面部检测器配置</h1>

                

            

            

                

<p class="graf graf--p graf-after--pre">为了启动面部识别，我们需要创建一个<kbd>FirebaseVisionFaceDetectorOptions</kbd>实例。让我们创建一个新实例:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionFaceDetectorOptions options = <br/>                            FirebaseVisionFaceDetectorOptions.Builder()</pre>

<p class="graf graf--p graf-after--pre">然后可以用不同属性的集合对其进行配置:</p>

<ul class="postList">

<li class="graf graf--li graf-after--p"><strong>检测模式</strong>:检测人脸时偏向速度或精度。这可以设置为<kbd>ACCURATE_MODE</kbd> <em> </em>或<kbd>FAST_MODE</kbd>。这默认为<kbd>FAST_MODE</kbd>:</li>

</ul>

<pre style="padding-left: 60px">.setModeType(FirebaseVisionFaceDetectorOptions.ACCURATE_MODE)<br/>.setModeType(FirebaseVisionFaceDetectorOptions.FAST_MODE)</pre>

<ul class="postList">

<li class="graf graf--li graf-after--pre"><strong>界标检测</strong>:确定是否尝试识别面部界标:眼睛、耳朵、鼻子、脸颊、嘴巴。默认为<kbd>NO_LANDMARKS</kbd>:</li>

</ul>

<pre style="padding-left: 60px">.setLandmarkType(FirebaseVisionFaceDetectorOptions.ALL_LANDMARKS)<br/>.setLandmarkType(FirebaseVisionFaceDetectorOptions.NO_LANDMARKS)</pre>

<ul class="postList">

<li class="graf graf--li graf-after--pre"><strong>特征分类</strong>:决定是否将人脸分类为<em>微笑</em>和<em>睁眼</em>等类别。这默认为<kbd>NO_CLASSIFICATIONS</kbd>:</li>

</ul>

<pre style="padding-left: 60px">.setClassificationType(FirebaseVisionFaceDetectorOptions.ALL_<br/>                                      CLASSIFICATIONS)<br/>.setClassificationType(FirebaseVisionFaceDetectorOptions.NO_<br/>                                     CLASSIFICATIONS)</pre>

<ul class="postList">

<li class="graf graf--li graf-after--pre"><strong>最小面部尺寸</strong>:这是相对于图像要检测的面部的最小尺寸:</li>

</ul>

<pre class="graf graf--pre graf-after--li" style="padding-left: 60px">.setMinFaceSize(0.15f)</pre>

<ul class="postList">

<li class="graf graf--li graf-after--pre"><strong>启用人脸跟踪</strong>:决定是否给人脸分配一个ID，用于跨图像跟踪人脸；</li>

</ul>

<pre class="graf graf--pre graf-after--li" style="padding-left: 60px">.setTrackingEnabled(true)<br/>.setTrackingEnabled(false)</pre>

<p class="graf graf--p graf-after--pre">将所有这些放在一起，我们只剩下以下内容:</p>

<pre>val options = FirebaseVisionFaceDetectorOptions.Builder()<br/>        .setModeType(FirebaseVisionFaceDetectorOptions.FAST_MODE)<br/>        .setLandmarkType(<br/>            FirebaseVisionFaceDetectorOptions.ALL_LANDMARKS)      <br/>        .setClassificationType(<br/>            FirebaseVisionFaceDetectorOptions.ALL_CLASSIFICATIONS)<br/>        .setMinFaceSize(0.20f)<br/>        .setTrackingEnabled(true)<br/>        .build()</pre>

<p class="graf graf--p graf-after--pre graf--trailing">如果您没有使用构建器设置任何选项，那么它们将会被设置为之前所述的默认值。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Running the face detector</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">运行面部检测器</h1>

                

            

            

                

<p>让我们看一下运行面部检测器的一步一步的过程。下图显示了面部的边界框以及所有面部标志位置标记:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-749 image-border" src="img/1d074738-3403-4a80-8c4a-f06c186d33e2.png" style="width:43.67em;height:23.50em;"/></p>

<p>边界框是由两个经度和两个纬度定义的区域，其中:纬度是介于-90.0和90.0之间的十进制数，经度是介于-180.0和180.0之间的十进制数。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Step one: creating a FirebaseVisionImage from the input</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第一步:从输入创建一个FirebaseVisionImage</h1>

                

            

            

                

<p>为了运行人脸检测，我们需要创建一个<kbd>FirebaseVisionFace</kbd>类的实例。创建一个<kbd>FirebaseVisionFace</kbd>对象有五种方法。该对象可以从位图、字节缓冲区、<kbd>media.Image</kbd>、字节数组或设备上的文件中创建。</p>

<p>然后，创建的<kbd>FirebaseVisionImage</kbd>对象将被传递给<kbd>FirebaseVisionFaceDetector</kbd>对象的<kbd>detectInImage()</kbd>方法。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Using a bitmap</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">使用位图</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">让我们使用一个位图实例来创建这个<kbd>FirebaseVisionImage</kbd>实例，其中图像中的对象应该向右切换，不需要旋转。我们可以通过将位图传递给<kbd>fromBitmap()</kbd>函数来创建实例；这将给我们一个如下的<kbd>FirebaseVisionImage</kbd>:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage myImage = FirebaseVisionImage.fromBitmap(bitmap);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From media.Image</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">来自媒体。图像</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">让我们使用一个<kbd>media.Image</kbd>实例来创建一个<kbd>FirebaseVisionImage</kbd>的实例。从设备的摄像头捕捉图像时，可能会出现这种情况。在这样做的时候，我们必须传递这个图像的实例以及它的旋转，所以这必须在调用<kbd>fromMediaImage()</kbd>函数之前进行计算。</p>

<p>旋转功能如下:</p>

<pre class="prettyprint">private static final SparseIntArray ORIENTATIONS = new <br/>                                                   SparseIntArray();<br/>static {<br/>    ORIENTATIONS.append(Surface.ROTATION_0, 90);<br/>    ORIENTATIONS.append(Surface.ROTATION_90, 0);<br/>    ORIENTATIONS.append(Surface.ROTATION_180, 270);<br/>    ORIENTATIONS.append(Surface.ROTATION_270, 180);<br/>}<br/><br/>/**<br/> * Get the angle by which an image must be rotated given the device's <br/>   current orientation.<br/> */<br/>@RequiresApi(api = Build.VERSION_CODES.LOLLIPOP)<br/>private int getRotationCompensation(String cameraId, Activity activity, <br/>                                    Context context)<br/>        throws CameraAccessException {<br/>    // Get the device's current rotation relative to its "native"    <br/>    // orientation.<br/>    // Then, from the ORIENTATIONS table, look up the angle the image      <br/>    // must be rotated to compensate for the device's rotation.<br/>    int deviceRotation = <br/>        activity.getWindowManager().getDefaultDisplay().getRotation();<br/>        int rotationCompensation = ORIENTATIONS.get(deviceRotation);<br/><br/> // On most devices, the sensor orientation is 90 degrees, but for some<br/>// devices it is 270 degrees. For devices with a sensor orientation of<br/>// 270, rotate the image an additional 180 ((270 + 270) % 360) degrees.<br/>    CameraManager cameraManager = (CameraManager) context.getSystemService(CAMERA_SERVICE);<br/>    int sensorOrientation = cameraManager<br/>            .getCameraCharacteristics(cameraId)<br/>            .get(CameraCharacteristics.SENSOR_ORIENTATION);<br/>    rotationCompensation = (rotationCompensation + sensorOrientation + <br/>                            270) % 360;<br/><br/>// Return the corresponding FirebaseVisionImageMetadata rotation value.<br/>    int result;<br/>    switch (rotationCompensation) {<br/>        case 0:<br/>            result = FirebaseVisionImageMetadata.ROTATION_0;<br/>            break;<br/>        case 90:<br/>            result = FirebaseVisionImageMetadata.ROTATION_90;<br/>            break;<br/>        case 180:<br/>            result = FirebaseVisionImageMetadata.ROTATION_180;<br/>            break;<br/>        case 270:<br/>            result = FirebaseVisionImageMetadata.ROTATION_270;<br/>            break;<br/>        default:<br/>            result = FirebaseVisionImageMetadata.ROTATION_0;<br/>            Log.e(TAG, "Bad rotation value: " + rotationCompensation);<br/>    }<br/>    return result;<br/>}</pre>

<p>然后将结果传递给方法，如下所示:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage myImage = <br/>              FirebaseVisionImage.fromMediaImage(mediaImage, rotation);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From a ByteBuffer</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">来自字节缓冲器</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">让我们使用ByteBuffer创建这个<kbd>FirebaseVisionImage</kbd>的实例。然而，要做到这一点，我们必须首先创建一个<kbd>FirebaseVisionImageMetadata</kbd>的实例。这包含构建视觉图像所需的数据，如格式、旋转和测量值(高度和宽度)，如下所示:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImageMetadata metadata = new <br/>    FirebaseVisionImageMetadata.Builder()<br/>        .setWidth(1280)<br/>        .setHeight(720)<br/>        .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21)<br/>        .setRotation(rotation)<br/>        .build();</pre>

<p class="graf graf--p graf-after--pre">然后，我们可以将其与ByteBuffer一起传递，以创建以下实例:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage myImage = <br/>                FirebaseVisionImage.fromByteBuffer(buffer, metadata);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From a ByteArray</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从字节数组</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">从ByteArray创建图像的工作方式与ByteBuffer相同，只是我们必须使用<kbd>fromByteArray()</kbd>函数:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage myImage = <br/>              FirebaseVisionImage.fromByteArray(byteArray, metadata);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From a file</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从文件中</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">通过使用上下文和所需的<strong>统一资源标识符</strong> ( <strong> URI </strong>)调用<kbd>fromFilePath()</kbd>函数，可以从文件中创建一个视觉图像实例:</p>

<pre class="graf graf--pre graf-after--p graf--trailing">val image: FirebaseVisionImage?<br/>try {<br/>    image = FirebaseVisionImage.fromFilePath(context, uri);<br/>} catch (IOException e) {<br/>    e.printStackTrace();<br/>}</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Step two: creating an instance of FirebaseVisionFaceDetector object</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第二步:创建FirebaseVisionFaceDetector对象的实例</h1>

                

            

            

                

<p><kbd>FirebaseVisionFaceDetector</kbd>检测输入图像中的<kbd>&lt;FirebaseVisionFace&gt;</kbd>个实例。运行人脸检测器后，创建一个<kbd>FirebaseVisionFaceDetector</kbd>的实例，如下所示:</p>

<pre>FirebaseVisionFaceDetector detector = FirebaseVision.getInstance()<br/>.getVisionFaceDetector(options);   </pre>

<p>前面的方法返回一个任务，该任务异步返回一个检测到的<kbd>FirebaseVisionFaces</kbd> ( <kbd>Task&lt;List&lt;FirebaseVisionFace&gt;&gt;</kbd>)列表。然后，创建的对象将被传递给图像检测方法。</p>

<p>一定要记住检查控制台中由构造函数生成的错误。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Step three: image detection</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第三步:图像检测</h1>

                

            

            

                

<p>基于映像检测，侦听器回调将进入成功或失败方法。输出将包含带有边界框的已识别面的列表。</p>

<p>最后，将图像传递给<kbd>detectInImage()</kbd>方法，如下所示:</p>

<pre>@Override<br/>protected Task&lt;List&lt;FirebaseVisionFace&gt;&gt; detectInImage(FirebaseVisionImage image) {<br/>  return detector.detectInImage(image);<br/>}<br/><br/>@Override<br/>protected void onSuccess(<br/>    @NonNull List&lt;FirebaseVisionFace&gt; faces,<br/>    @NonNull FrameMetadata frameMetadata,<br/>    @NonNull GraphicOverlay graphicOverlay) {<br/>  graphicOverlay.clear();<br/>  for (int i = 0; i &lt; faces.size(); ++i) {<br/>    FirebaseVisionFace face = faces.get(i);<br/>    FaceGraphic faceGraphic = new FaceGraphic(graphicOverlay);<br/>    graphicOverlay.add(faceGraphic);<br/>    faceGraphic.updateFace(face, frameMetadata.getCameraFacing());<br/>  }<br/>}<br/><br/>@Override<br/>protected void onFailure(@NonNull Exception e) {<br/>  Log.e(TAG, "Face detection failed " + e);<br/>}</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Retrieving information from detected faces</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从检测到的人脸中检索信息</h1>

                

            

            

                

<p>如果面部识别操作成功，一列<kbd>FirebaseVisionFace</kbd>对象将被传递给成功的监听器。每个<kbd>FirebaseVisionFace</kbd>对象代表图像中检测到的一张脸。对于每个面部，您可以获得其在输入图像中的边界坐标，以及您配置面部检测器查找的任何其他信息:</p>

<pre class="prettyprint">for (FirebaseVisionFace face : faces) {<br/>    Rect bounds = face.getBoundingBox();<br/>    float rotY = face.getHeadEulerAngleY();  // Head is rotated to the <br/>                                                right rotY degrees<br/>    float rotZ = face.getHeadEulerAngleZ();  // Head is tilted sideways <br/>                                                rotZ degrees<br/><br/>   // If landmark detection was enabled (mouth, ears, eyes, cheeks, and<br/>   // nose available):<br/>    FirebaseVisionFaceLandmark leftEar = <br/>             face.getLandmark(FirebaseVisionFaceLandmark.LEFT_EAR);<br/>    if (leftEar != null) {<br/>        FirebaseVisionPoint leftEarPos = leftEar.getPosition();<br/>    }<br/><br/>    // If classification was enabled:<br/>    if (face.getSmilingProbability() != <br/>             FirebaseVisionFace.UNCOMPUTED_PROBABILITY) {<br/>        float smileProb = face.getSmilingProbability();<br/>    }<br/>    if (face.getRightEyeOpenProbability() != <br/>          FirebaseVisionFace.UNCOMPUTED_PROBABILITY) {<br/>            float rightEyeOpenProb = face.getRightEyeOpenProbability();<br/>    }<br/><br/>    // If face tracking was enabled:<br/>    if (face.getTrackingId() != FirebaseVisionFace.INVALID_ID) {<br/>        int id = face.getTrackingId();<br/>    }<br/>}</pre>

<p>有了这个，我们就可以用ML工具包来研究人脸检测器了。现在，去我们的仓库直接从那里提取代码:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-750 image-border" src="img/d949baf4-83a5-4518-9495-8cbdf19ce493.png" style="width:19.08em;height:32.83em;"/></p>

<p>这是我们本章知识库的链接:<a href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4">https://g</a><a href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4">it</a><a href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4">hub.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4</a>。以及<a href="https://github.com/PacktPublishing/Machine-Learning-Projects-for-Mobile-Applications" target="_blank">https://github . com/packt publishing/Machine-Learning-Projects-for-Mobile-Applications</a>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Barcode scanner</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">条形码扫描仪</h1>

                

            

            

                

<p>现在让我们开始使用ML Kit实现一个基于移动设备的条形码扫描仪。条形码有多种格式。ML试剂盒支持下列所有格式:</p>

<p class="CDPAlignCenter CDPAlign"><img src="img/5b72d7db-c33e-41de-b095-75e2bd298562.png" style="width:50.67em;height:24.67em;"/></p>

<p>一旦在摄像机视图上识别了条形码，<kbd>draw</kbd>方法将边界框连同检测到的条形码的原始值放在它的上面。以下代码在提供的<kbd>canvas</kbd>上绘制位置、大小和原始值的条形码块注释:</p>

<pre>/**<br/> * Draws the barcode block annotations<br/>*/<br/>@Override<br/>public void draw(Canvas canvas) {<br/>  if (barcode == null) {<br/>    throw new IllegalStateException("Attempting to draw a null <br/>                                     barcode.");<br/>  }<br/><br/>  // Draws the bounding box around the BarcodeBlock.<br/>  RectF rect = new RectF(barcode.getBoundingBox());<br/>  rect.left = translateX(rect.left);<br/>  rect.top = translateY(rect.top);<br/>  rect.right = translateX(rect.right);<br/>  rect.bottom = translateY(rect.bottom);<br/>  canvas.drawRect(rect, rectPaint);<br/><br/>  // Renders the barcode at the bottom of the box.<br/>  canvas.drawText(barcode.getRawValue(), rect.left, rect.bottom, <br/>                  barcodePaint);<br/>}</pre>

<p>如果我们知道将要读取的条形码格式，我们可以通过仅在配置设置时检测该格式来加快过程。例如，为了检测QR码，让我们通过设置如下所示的条形码格式来创建<kbd>FirebaseVisionBarcodeDetectorOptions</kbd>实例:</p>

<pre class="prettyprint">FirebaseVisionBarcodeDetectorOptions options =<br/>        new FirebaseVisionBarcodeDetectorOptions.Builder()<br/>        .setBarcodeFormats(FirebaseVisionBarcode.FORMAT_QR_CODE)<br/>        .build();</pre>

<div><div><p class="graf graf--p graf-after--pre">现在我们已经定义了这个选项，我们可以使用我们的<kbd>FirebaseVision</kbd>实例的<kbd>get</kbd>函数，传入我们的<kbd>options</kbd>实例:</p>

<pre class="graf graf--pre graf-after--p graf--trailing">val detector =          <br/>       FirebaseVision.getInstance().getVisionBarcodeDetector(options)</pre></div>

</div>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Step one: creating a FirebaseVisionImage object</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第一步:创建FirebaseVisionImage对象</h1>

                

            

            

                

<p>在构建选项之后，我们可以继续进行识别。与我们对人脸检测所做的类似，<kbd>FirebaseVisionImage</kbd>对象可以从位图、ByteBuffer、<kbd>media.Image</kbd>、ByteArray或设备上的文件中创建。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From bitmap</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从位图</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">让我们使用位图创建这个<kbd>FirebaseVisionImage</kbd>的实例。首先，让我们将一个位图传递给<kbd>fromBitmap()</kbd>方法，这将返回一个<kbd>FirebaseVisionImage</kbd>:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage image = FirebaseVisionImage.fromBitmap(bitmap);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From media.Image</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">来自媒体。图像</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">让我们使用一个<kbd>media.Image</kbd>实例来创建这个<kbd>FirebaseVisionImage</kbd>实例。这将使用设备摄像头捕捉。在图像被捕获之后，我们也需要将它传递给<kbd>rotation</kbd>方法。必须在调用<kbd>fromMediaImage()</kbd>方法之前调用<kbd>rotation</kbd>。给定设备的当前方向，下面的方法获取图像必须旋转的角度:</p>

<pre class="prettyprint">@RequiresApi(api = Build.VERSION_CODES.LOLLIPOP)<br/>private int getRotationCompensation(String cameraId, Activity activity,  <br/>                                    Context context)<br/>        throws CameraAccessException {<br/>// Get the device's current rotation relative to its "native"     <br/>   orientation.<br/>// Then,from the ORIENTATIONS table,look up the angle the image must be<br/>// rotated to compensate for the device's rotation.<br/>    int deviceRotation =  <br/>        activity.getWindowManager().getDefaultDisplay().getRotation();<br/>    int rotationCompensation = ORIENTATIONS.get(deviceRotation);<br/><br/> // On most devices, the sensor orientation is 90 degrees, but for some<br/> // devices it is 270 degrees. For devices with a sensor orientation of<br/> // 270, rotate the image an additional 180 ((270 + 270) % 360)  <br/>    degrees.<br/>    CameraManager cameraManager = (CameraManager) context.getSystemService(CAMERA_SERVICE);<br/>    int sensorOrientation = cameraManager<br/>            .getCameraCharacteristics(cameraId)<br/>            .get(CameraCharacteristics.SENSOR_ORIENTATION);<br/>    rotationCompensation = (rotationCompensation + sensorOrientation + <br/>                            270) % 360;<br/><br/>// Return the corresponding FirebaseVisionImageMetadata rotation value.<br/>    int result;<br/>    switch (rotationCompensation) {<br/>        case 0:<br/>            result = FirebaseVisionImageMetadata.ROTATION_0;<br/>            break;<br/>        case 90:<br/>            result = FirebaseVisionImageMetadata.ROTATION_90;<br/>            break;<br/>        case 180:<br/>            result = FirebaseVisionImageMetadata.ROTATION_180;<br/>            break;<br/>        case 270:<br/>            result = FirebaseVisionImageMetadata.ROTATION_270;<br/>            break;<br/>        default:<br/>            result = FirebaseVisionImageMetadata.ROTATION_0;<br/>            Log.e(TAG, "Bad rotation value: " + rotationCompensation);<br/>    }<br/>    return result;<br/>}</pre>

<p>前面的方法与我们用于面部检测的方法相同:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage image = <br/>           FirebaseVisionImage.fromMediaImage(mediaImage, rotation);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From ByteBuffer</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">来自ByteBuffer</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">让我们使用ByteBuffer创建这个<kbd>FirebaseVisionImage</kbd>的实例。然而，为了做到这一点，我们必须首先创建一个<kbd>FirebaseVisionImageMetadata</kbd>的实例。这包含构建视觉图像所需的数据，例如旋转和测量，如下所述:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImageMetadata metadata = new <br/>    FirebaseVisionImageMetadata.Builder()<br/>        .setWidth(1280)<br/>        .setHeight(720)<br/>        .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21)<br/>        .setRotation(rotation)<br/>        .build();</pre>

<p class="graf graf--p graf-after--pre">通过前面的实例，我们得到了输入图像的宽度和高度。它可以根据您的应用需求进行配置。</p>

<p>如果想了解更多图片格式参数，可以查看:<a href="https://developer.android.com/reference/android/graphics/ImageFormat" target="_blank">https://developer . Android . com/reference/Android/graphics/image format</a>。</p>

<p class="graf graf--p graf-after--pre">然后，我们可以将其与ByteBuffer一起传递，以创建以下实例:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage image = FirebaseVisionImage.fromByteBuffer(buffer, <br/>                            metadata);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From ByteArray</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">来自ByteArray</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">ByteBuffer就像一个构建器来创建一个<kbd>byte[]</kbd>。与数组不同，它有更多的助手方法。从ByteArray创建图像的工作方式与ByteBuffer相同，只是我们必须使用如下所示的<kbd>fromByteArray()</kbd>函数:</p>

<pre class="graf graf--pre graf-after--p">FirebaseVisionImage image =  <br/>            FirebaseVisionImage.fromByteArray(byteArray, metadata);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>From file</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">从文件</h1>

                

            

            

                

<p class="graf graf--p graf-after--h4">通过使用上下文和URI调用<kbd>fromFilePath()</kbd>函数，可以从文件中创建一个视觉图像实例:</p>

<pre class="graf graf--pre graf-after--p graf--trailing">val image: FirebaseVisionImage?<br/>try {<br/>    image = FirebaseVisionImage.fromFilePath(context, uri);<br/>} catch (IOException e) {<br/>    e.printStackTrace();<br/>}</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Step two: creating a FirebaseVisionBarcodeDetector object</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第二步:创建FirebaseVisionBarcodeDetector对象</h1>

                

            

            

                

<p><kbd>FirebaseVisionBarcodeDetector</kbd>识别提供的<kbd>FirebaseVisionImage</kbd>中的条形码(各种一维和二维格式),如下所示:</p>

<pre class="prettyprint">FirebaseVisionBarcodeDetector detector = FirebaseVision.getInstance().getVisionBarcodeDetector();<br/>// Or, we can specify the formats to recognize:<br/>FirebaseVisionBarcodeDetector detector = <br/>FirebaseVision.getInstance().getVisionBarcodeDetector(options);</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Step three: barcode detection</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">第三步:条形码检测</h1>

                

            

            

                

<p>基于映像检测，侦听器回调将进入成功或失败方法。输出将包含如下已识别的<kbd>FirebaseVisionBarcode</kbd>对象列表:</p>

<pre>@Override<br/>protected Task&lt;List&lt;FirebaseVisionBarcode&gt;&gt; detectInImage(FirebaseVisionImage image) {<br/>    return detector.detectInImage(image);<br/>}<br/><br/>@Override<br/>protected void onSuccess(<br/>        @NonNull List&lt;FirebaseVisionBarcode&gt; barcodes,<br/>        @NonNull FrameMetadata frameMetadata,<br/>        @NonNull GraphicOverlay graphicOverlay) {<br/>    graphicOverlay.clear();<br/>    for (int i = 0; i &lt; barcodes.size(); ++i) {<br/>        FirebaseVisionBarcode barcode = barcodes.get(i);<br/>        BarcodeGraphic barcodeGraphic = new BarcodeGraphic(graphicOverlay, barcode);<br/>        graphicOverlay.add(barcodeGraphic);<br/>    }<br/>}<br/><br/>@Override<br/>protected void onFailure(@NonNull Exception e) {<br/>    Log.e(TAG, "Barcode detection failed " + e);<br/>}</pre>

<p>成功检测到一个或多个条形码后，需要将<kbd>FirebaseVisionBarcode</kbd>对象传递给方法，以便从检测到的条形码中获取数据。根据条形码的类型，我们得到相应的输出如下:</p>

<pre class="prettyprint">for (FirebaseVisionBarcode barcode: barcodes) {<br/>    //Returns a Rect instance that contains the bounding box for the recognized barcode<br/>    Rect bounds = barcode.getBoundingBox();<br/>    <br/>//Returns the coordinates for each corner of the barcode.<br/>    Point[] corners = barcode.getCornerPoints();<br/>    <br/>//Returns the barcode value in its raw format<br/>    String rawValue = barcode.getRawValue();<br/>    <br/>//Returns the format type of the barcode<br/>    int valueType = barcode.getValueType();<br/>    <br/>// See API reference for complete list of supported types<br/>    switch (valueType) {<br/>        case FirebaseVisionBarcode.TYPE_WIFI:<br/>            String ssid = barcode.getWifi().getSsid();<br/>            String password = barcode.getWifi().getPassword();<br/>            int type = barcode.getWifi().getEncryptionType();<br/>            break;<br/>        case FirebaseVisionBarcode.TYPE_URL:<br/>            String title = barcode.getUrl().getTitle();<br/>            String url = barcode.getUrl().getUrl();<br/>            break;<br/>    }<br/>}</pre>

<p class="CDPAlignLeft CDPAlign">这样，我们现在应该能够使用ML试剂盒进行条形码扫描。这将是任何零售商业应用的便利工具:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-751 image-border" src="img/acd47226-e919-4b93-8038-1c63cc14ff9a.png" style="width:19.42em;height:33.25em;"/></p>

<p>现在，让我们去我们的存储库，从那里提取代码，并开始您的实验。</p>

<p>这是我们本章知识库的链接:<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://github.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4">https://g</a>T8】itT10】hub.com/intrepidkarthi/MLmobileapps/tree/master/Chapter4。以及<a xmlns:epub="http://www.idpf.org/2007/ops" href="https://github.com/PacktPublishing/Machine-Learning-Projects-for-Mobile-Applications" target="_blank">https://github . com/packt publishing/Machine-Learning-Projects-for-Mobile-Applications</a>。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Text recognition</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">文本识别</h1>

                

            

            

                

<p>在面部检测和条形码扫描之后，让我们构建一个应用程序来从输入图像或相机馈送中识别文本。这种方法叫做<strong>光学字符识别</strong> ( <strong> OCR </strong>)。文本识别既支持设备上的识别，也支持基于云的识别。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>On-device text recognition</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">设备上的文本识别</h1>

                

            

            

                

<p>让我们跳过我们用于面部检测和条形码扫描的默认方法。</p>

<p>在创建了<kbd>FirebaseVisionImage</kbd>对象后，我们将创建<kbd>detector</kbd>实例，在这里我们将传递如下所示的<kbd>VisionImage</kbd>对象，类似于我们之前在人脸检测和条形码扫描中所做的:</p>

<pre class="prettyprint">FirebaseVisionTextDetector detector = FirebaseVision.getInstance()<br/>        .getVisionTextDetector();</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Detecting text on a device</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">检测设备上的文本</h1>

                

            

            

                

<p>对于文本检测，将图像对象从<kbd>detector</kbd>实例传递给<kbd>detectInImage()</kbd>方法。下面的代码块用于实现这一点:</p>

<pre class="prettyprint">Task&lt;FirebaseVisionText&gt; result =<br/>        detector.detectInImage(image)<br/>                .addOnSuccessListener(new OnSuccessListener&lt;FirebaseVisionText&gt;() {<br/>                    @Override<br/>                    public void onSuccess(FirebaseVisionText firebaseVisionText) {<br/>                        // Task completed successfully<br/>                        // ...<br/>                    }<br/>                })<br/>                .addOnFailureListener(<br/>                        new OnFailureListener() {<br/>                            @Override<br/>                            public void onFailure(@NonNull Exception e) {<br/>                                // Task failed with an exception<br/>                                // ...<br/>                            }<br/>                        });</pre>

<p>成功识别文本后，我们可以使用下面的代码块解析<kbd>FirebaseVisionText</kbd>对象以进一步处理它:</p>

<pre class="prettyprint">for (FirebaseVisionText.Block block: firebaseVisionText.getBlocks()) {<br/>    Rect boundingBox = block.getBoundingBox();<br/>    Point[] cornerPoints = block.getCornerPoints();<br/>    String text = block.getText();<br/><br/>    for (FirebaseVisionText.Line line: block.getLines()) {<br/>        // ...<br/>        for (FirebaseVisionText.Element element: line.getElements()) {<br/>            // ...<br/>        }<br/>    }<br/>}</pre>

<p>既然我们已经熟悉了在任何给定媒体中查找人脸、条形码和文本，那么让我们来看一个如何在云中进行文本识别的示例:</p>

<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-752 image-border" src="img/9e8ef4bc-a364-445e-bcff-0e2ae3b0c05c.png" style="width:18.17em;height:26.92em;"/></p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Cloud-based text recognition</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">基于云的文本识别</h1>

                

            

            

                

<p>要使用基于云的检测，我们需要在开发人员控制台上为您的项目启用Google Vision API。云API需要额外的费用，但是每月对API的前1000次调用是免费的。无论如何，您仍然需要输入您的信用卡信息以确保订阅计划的安全。根据你的需要选择这个。</p>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Configuring the detector</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">配置检测器</h1>

                

            

            

                

<p>我们需要配置<kbd>FirebaseVisionCloudDetectorOptions</kbd>对象。默认情况下，云检测器使用该模型的<kbd>STABLE</kbd>版本，并返回多达十个结果。但是，我们可以根据自己的需要进行更改，并将参数设置如下:</p>

<pre class="mce-root">FirebaseVisionCloudDetectorOptions options =<br/> new FirebaseVisionCloudDetectorOptions.Builder()<br/> .setModelType(FirebaseVisionCloudDetectorOptions.LATEST_MODEL)<br/> .setMaxResults(12)<br/> .build();</pre>

<p class="mce-root">要使用默认设置，我们可以在下一步使用<kbd>FirebaseVisionCloudDetectorOptions.DEFAULT</kbd>。</p>

<p>在这之后，我们创建了<kbd>FirebaseVisionImage</kbd>对象。由于这在前面的人脸检测器和条形码扫描仪的实现中已经讨论过，我们将在这里跳过这一部分。</p>

<p>创建一个<kbd>detector</kbd>实例来传递图像对象:</p>

<pre class="prettyprint">FirebaseVisionCloudTextDetector detector = FirebaseVision.getInstance()<br/>        .getVisionCloudTextDetector();<br/>// Or, to change the default settings:<br/>// FirebaseVisionCloudTextDetector detector = FirebaseVision.getInstance()<br/>//         .getVisionCloudTextDetector(options);</pre>

<p>现在，我们可以将图像对象传递给<kbd>detectInImage()</kbd>方法:</p>

<pre class="prettyprint">Task&lt;FirebaseVisionCloudText&gt; result = detector.detectInImage(image)<br/>        .addOnSuccessListener(new OnSuccessListener&lt;FirebaseVisionCloudText&gt;() {<br/>            @Override<br/>            public void onSuccess(FirebaseVisionCloudText firebaseVisionCloudText) {<br/>                // Task completed successfully<br/>                // ...<br/>            }<br/>        })<br/>        .addOnFailureListener(new OnFailureListener() {<br/>            @Override<br/>            public void onFailure(@NonNull Exception e) {<br/>                // Task failed with an exception<br/>                // ...<br/>            }<br/>        });</pre>

<p>成功检测到文本后，我们将得到一个文本块列表。然后，我们可以处理输出，以便进一步进行如下操作:</p>

<pre class="prettyprint">String recognizedText = firebaseVisionCloudText.getText();<br/><br/>for (FirebaseVisionCloudText.Page page: firebaseVisionCloudText.getPages()) {<br/>    List&lt;FirebaseVisionCloudText.DetectedLanguage&gt; languages =<br/>            page.getTextProperty().getDetectedLanguages();<br/>    int height = page.getHeight();<br/>    int width = page.getWidth();<br/>    float confidence = page.getConfidence();<br/><br/>    for (FirebaseVisionCloudText.Block block: page.getBlocks()) {<br/>        Rect boundingBox = block.getBoundingBox();<br/>        List&lt;FirebaseVisionCloudText.DetectedLanguage&gt; blockLanguages =<br/>                block.getTextProperty().getDetectedLanguages();<br/>        float blockConfidence = block.getConfidence();<br/>        // And so on: Paragraph, Word, Symbol<br/>    }<br/>}</pre>





            



            

        

    </body>



</html>
<html xmlns:epub="http://www.idpf.org/2007/ops">

    <head>

        <title>Summary</title>

        

        <meta charset="utf-8"/>

<meta content="urn:uuid:6d0d6080-3897-4636-acde-b4b62ec60a94" name="Adept.expected.resource"/>

    </head>



    <body>

        



                            

                    <h1 class="header-title">摘要</h1>

                

            

            

                

<p class="mce-root">在这一点上，我们很容易实现基于移动设备的ML应用程序的基本功能，包括文本内检测、人脸检测和条形码扫描。同样，我们可以通过云API实现图像标注和地标检测。我们现在应该能够看到ML Kit涵盖了基于移动的ML应用程序的基本需求。</p>

<p class="mce-root">在下一章，我们将继续构建类似于Snapchat上可用的<strong>增强现实</strong> ( <strong> AR </strong>)过滤器。</p>





            



            

        

    </body>



</html></body></html>